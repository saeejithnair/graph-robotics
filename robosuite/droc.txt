File: README.md

# Distilling and Retrieving Generalizable Knowledge for Robot Manipulation via Language Corrections
## [<a href="https://sites.google.com/stanford.edu/droc" target="_blank">Project Page</a>][<a href="https://arxiv.org/abs/2311.10678">Paper</a>]

![til](https://github.com/lihzha/visualizations/blob/main/overview(twitter).gif)

We propose a method for responding to online language corrections, distilling generalizable knowledge from them, and retrieving useful knowledge for novel tasks, as described <a href="https://sites.google.com/stanford.edu/droc" target="_blank">here</a>.

[//]: # (### Abstract)
> Today's robot policies exhibit subpar performance when faced with the challenge of generalizing to novel environments. Human corrective feedback is a crucial form of guidance to enable such generalization. However, adapting to and learning from online human corrections is a non-trivial endeavor: not only do robots need to remember human feedback over time to retrieve the right information in new settings and reduce the intervention rate, but also they would need to be able to respond to feedback that can be arbitrary corrections about high-level human preferences to low-level adjustments to skill parameters. In this work, we present Distillation and Retrieval of Online Corrections (DROC), a large language model (LLM)-based system that can respond to arbitrary forms of language feedback, distill generalizable knowledge from corrections, and retrieve relevant past experiences based on textual and visual similarity for improving performance in novel settings. DROC is able to respond to a sequence of online language corrections that address failures in both high-level task plans and low-level skill primitives. We demonstrate that DROC effectively distills the relevant information from the sequence of online corrections in a knowledge base and retrieves that knowledge in settings with new task or object instances. DROC outperforms other techniques that directly generate robot code via LLMs by using only half of the total number of corrections needed in the first round and requires little to no corrections after two iterations.

For more details, please refer to our [paper](https://arxiv.org/abs/2311.10678).


## Installation

1. Create a virtual environment with Python 3.8 and install all the dependencies.
      ```bash
      conda create -n droc python==3.8
      pip install -r requirements.txt
      pip install -e .
      ```

2. (Optional) Download <a href="https://github.com/facebookresearch/fairo/tree/main/polymetis">Polymetis</a> for real robot experiments. Note this only supports PyTorch ~= 1.12. If you are using new versions of PyTorch, please refer to the <a href="https://github.com/facebookresearch/fairo/tree/main/polymetis">monometis</a> fork from Hengyuan Hu.

3. Set your OpenAI key in `utils/LLM_utils.py`.


## Code Structure

* **scripts**: Contains the main script and all baseline scripts.

* **prompts**: Contains all LLM prompts. For the function of each prompt, please refer to [here](https://github.com/Stanford-ILIAD/droc/tree/main/prompts/prompt_overview.md).

* **utils**: Contains all utilities for running the main script, including I/O, perception, robot control, LLM, exception handling, etc.

* **cache**: Contains the knowledge base (in .pkl format), calibration results, detection results, and other caches.


## Usage

### Real robot experiments

Our real robot experiments are based on [Polymetis](https://github.com/facebookresearch/fairo/tree/main/polymetis). However, this only supports PyTorch ~= 1.12. If you are using new versions of PyTorch, please refer to the [monometis](https://github.com/hengyuan-hu/monometis) fork from Hengyuan Hu. Due to privacy issues, we are not able to release the codes for real robot control and perception. To run real robot experiments, please implement these modules on your own by following the steps below:

1. Implement the environment for your robot (e.g., Franka Panda) in `utils/robot/panda_env.py`. You may use the functions defined in `utils/transformation_utils.py`.

2. Implement the camera perception code in `utils/perception/camera.py`.

3. Define the task name and the corresponding clip candidates in `utils/perception/perception_utils.py`.

4. Run the main script.
      ```bash
      python scripts/script.py --realrobot True --task <task>
      ```
      Where `<task>` is the name of the task to run.

### Dummy testing
If you only want to test your prompts, a better choice would be running your code in a dummy mode.
```bash
python scripts/script.py --task <task>
```

### Plan-level examples

1. Define the task name and the corresponding clip candidates in `utils/perception/perception_utils.py`.

2. Put all images for retrieval in `cache/image_for_retrieval`. Images should be named in the format of '{object}_x.png', e.g., 'cup_1.png'.

3. Run the following command.
      ```bash
      python scripts/script.py --load_image True --task <task>
      ```


## Troubleshooting

Since the released implementation is primarily concered with real world robot experiments, there may be some unexpected bugs specific to individual use cases. If you meet any bug depolying this codebase, feel free to contact <lihanzha20@gmail.com>.



## Citation

```
@misc{zha2023distilling,
      title={Distilling and Retrieving Generalizable Knowledge for Robot Manipulation via Language Corrections}, 
      author={Lihan Zha and Yuchen Cui and Li-Heng Lin and Minae Kwon and Montserrat Gonzalez Arenas and Andy Zeng and Fei Xia and Dorsa Sadigh},
      year={2023},
      eprint={2311.10678},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}
```



File: prompts/change_frame.txt

There are four directions: forward, back, left and right. My frame is different from the absolute frame. "{correct_direction}" in absolute frame is "{wrong_direction}" in my frame, then "forward" in absolute frame is


File: prompts/get_constraint_feature.txt

Return the indexes of the tasks that share the same object with the query.
Task list: 1. open the fridge. 2. pick up the milk. 3. pour the milk into the bowl. 4. close the fridge
Query: the milk should be put in the glass.
Response: [2]

Task list: 1. take off the coat. 2. hang the coat on the wall. 3. turn on the light. 4. put the tissue in the coat.
Query: the tissue should be put in the black coat.
Response: [1, 4]


File: prompts/get_ini_obj_state.txt

Given the object's name, please give me the initial states of these objects.
Object name: top cabinet, middle cabinet, bottom cabinet, scissors, scissors, apple, spoon
Object state: top cabinet(closed), middle cabinet(closed), bottom cabinet(closed), scissors(on table), scissors(on table), apple(on table), spoon(on table)


File: prompts/get_obj_name_from_task.txt

Please extract the object's name from the given string.
Input: Put the scissors into the middle black cabinet
Output: middle black cabinet
Input: Put the yellow mug on the shelf
Output: shelf


File: prompts/get_pos_scale.txt

Relative to the given bounding box of an object, estimate how long does "move a little bit" and "move more" refer to. All units are in centimeters. Bounding boxes are arranged in (len_x, len_y, len_z) order. "Forward" direction vector is (1,0,0), "left" is (0,1,0) and "up" is (0,0,1).

Bounding box: (1, 8, 3)
{
"'A little bit' for 'forward' or 'backward'": 2,
"'A little bit' for 'left' or 'right'": 3,
"'A little bit' for 'up' or 'down'": 2.5,
"'More' for 'forward' or 'back'": 4
}


File: prompts/get_pose_from_str.py

import numpy as np
from perception_utils import detect, sort_from_high_to_low
from geometric_utils import calculate_centroid, calculate_major_axis, calculate_minor_axis, calculate_tail_point

# Query: the centroid and minor axis of "top black hat"
black_hats = detect('black hat')
black_hats = sort_from_high_to_low(black_hats, key='z')
top_black_hat = black_hats[0]
top_black_hat.position = calculate_centroid(top_black_hat.pcd)
top_black_hat.rotation = calculate_minor_axis(top_black_hat.pcd)
ret_val = top_black_hat

# Query: the tail point of "cup"
cups = detect('cup')
cups = sort_from_high_to_low(cups, key='probability')
cup = cups[0]
cup.position = calculate_tail_point(cup.pcd)
ret_val = cup

# Query: the major axis of "middle shelf"
shelves = detect('shelf')
shelves = sort_from_high_to_low(shelves, key='z')
middle_shelf = shelves[len(shelves)//2]
middle_shelf.rotation = calculate_major_axis(middle_shelf.pcd)
ret_val = middle_shelf


File: prompts/get_query_obj.txt

Please extract the visual feature from the given object name. Also extract the positional description if there is one.
Input: top white drawer
Output: {'visual': 'white', 'positional': 'top', 'object_name_with_visual': 'white drawer'}
Input: apple
Output: {'visual': None, 'positional': None, 'object_name_with_visual': None}
Input: big orange pie
Output: {'visual': 'big orange', 'positional': None, 'object_name_with_visual': 'big orange pie'}


File: prompts/get_task_feature.txt

The information related to the task "open the top white drawer" is: grasp_pos, grasp_ori, pull_distance.
The information related to the task "pick up the red pen" is: pick_pos, pick_ori, pickup_height
The information related to the task "{}" is:


File: prompts/get_task_pose_str.txt

The robot is performing a task to achieve a high-level goal. Please determine what is the target gripper position and orientation.

Here are some examples.

INPUT: the robot's task is to put the cup into the top brown shelf.
RESPONSE:
{
    "gripper position": "top_brown_shelf.centroid",
    "gripper orientation": "absolute_vertical"
}

INPUT: the robot's task is to pick up the black pen.
RESPONSE:
{
    "gripper position": "black_pen.centroid",
    "gripper orientation": "absolute_vertical" 
}

INPUT: the robot's task is to open the middle cabinet.
RESPONSE:
{
    "gripper position": "middle_cabinet.centroid",
    "gripper orientation": "middle_cabinet.minor_axis"
}

Rules:
1. Examples for gripper position: centroid, tail_point.
2. Examples for gripper orientation: major_axis, minor_axis, absolute_vertical, absolute_horizontal


File: prompts/hist_retrieval.txt

A human is issuing commands to a robot. These commands may depend on the robot's past experiences. Your task is to determine what does a command depend on. Label each command with one of the following: (a) Relies on last actions (b) Relies on initial actions (c) Task failure (d) No dependence.

"you are at the right position now. continue": (b)
"keep going": (a)
"move forward a little bit and then tilt up": (d)
"the pencil has dropped, pick it up again": (c)
"open the shelf now": (b)
"that's wrong. move right now": (a)
"now grasp the handle and pull it out": (b)
"move forward to the drawer": (d)
"you failed to grasp it. try again.": (a)
"grasp the handle": (b)
"now you can pick it up": (b)
"grasp the scissors and move up": (b)
"move back": (d)


File: prompts/hl_backbone.txt

Imagine you are working with a robot equipped with a two-fingered gripper for grasping. The robot's mission is to complete tasks based on a given instructions. However, it may struggle with high-level instructions. 
Your role is to understand those instructions and break them down into smaller, actionable sub-tasks. If there is ambiguity in deciding the object given existing information, just list all possible objects like Example 1.

Here are some examples:

{}

Rules:
1. In each sub-task of your response, the robot should manipulate one object and only move its gripper once.
2. If you think the instruction is low-level enough, just repeat the instruction in your response.

Object state: 


File: prompts/hl_cap_backbone.txt

Imagine you are working with a robot equipped with a two-fingered gripper for grasping. The robot's mission is to complete tasks based on a given instructions. However, it may struggle with high-level instructions. 
Your role is to understand those instructions and break them down into smaller, actionable sub-tasks.

Here are some examples:

{}

Rules:
1. In each sub-task of your response, the robot should manipulate one object and only move its gripper once.
2. If you think the instruction is low-level enough, just repeat the instruction in your response.


File: prompts/hl_cap_content.txt

Instruction: put the apple in the fridge.
Response:
{
  "1": "Open the fridge.",
  "2": "Pick up the apple.",
  "3": "Put apple into the fridge.",
  "4": "Close the fridge."
}

Instruction: open the top white cabinet
Response:
{
  "1": "Open the top white cabinet."
}


File: prompts/hl_content.txt

Object state: fridge(closed), green apple(on table), red apple(on table)
Instruction: put the apple in the fridge.
Response:
{
  "1": "Open the fridge.",
  "2": ["Pick up apple_x.", {"apple": ["green apple", "red apple"]}], 
  "3": "Put apple_x into the fridge.",
  "4": "Close the fridge."
}

Object state: cabinet(closed)
Instruction: open the top white cabinet
Response:
{
  "1": ["Open the cabinet.", {"top white cabinet: ["cabinet"]}]
}

Object state: N/A
Instruction: put the spoon into the bottom black shelf
Response:
{
  "1": "Open the bottom black shelf.",
  "2": "Pick up the spoon.",
  "3": "Put the spoon into the bottom black shelf.",
  "4": "Close the bottom black shelf."
}


File: prompts/hl_corr.txt

Below is a robot's plan for executing a high-level task. The plan is wrong, and human has issued corrections.
Please first reason what are the task constraints and robot constraints that the robot fails to obey, and then modify the original plan. Modified original plan should start from beginning and replan should start from current step

Here are some examples:
Task: put the orange into the fridge.
Plan:
{
  "1": "Pick up the orange.",
  "2": "Open the fridge.",
  "3": "Put the orange into the fridge.",
  "4": "Close the fridge"
}
Outcome: Interrupted by human at the step "open the fridge".
Correction: you can only pick up one thing at a time.
Object state: fridge(closed, not full), orange(on table)
Task constraint: None
Robot constraint: The robot only has one gripper and can only grasp one thing in hand at a time.
Updated object state: fridge(closed, not full), orange(in gripper)
Modified original plan:
{
  "1": "Open the fridge.",
  "2": "Pick up the orange.",
  "3": "Put the orange into the fridge.",
  "4": "Close the fridge"
}
Replan:
{
  "1": "Put the orange down.",
  "2": "Open the fridge.",
  "3": "Pick up the orange.",
  "4": "Put the orange into the fridge.",
  "5": "Close the fridge"
}

Task: sort the forks into the shelf
Plan:
{
  "1": "Pick up fork1",
  "2": "Put fork1 in shelf1",
  "3": "Pick up fork2",
  "4": "Put fork2 in shelf1"
}
Outcome: Interrupted by human at the step "put fork1 in shelf1".
Correction: that's the wrong shelf to put
Object state: shelf1(empty), shelf2(empty), shelf3(empty), fork1(on table), fork2(on table)
Task constraint: Forks should be put in shelf2.
Robot constraint: None
Updated object state: shelf1(empty), shelf2(empty), shelf3(empty), fork1(in gripper), fork2(on table)
Modified original plan:
{
  "1": "Pick up fork1",
  "2": "Put fork1 in shelf2",
  "3": "Pick up fork2",
  "4": "Put fork2 in shelf2"
}
Replan:
{
  "1": "Put fork1 in shelf2",
  "2": "Pick up fork2",
  "3": "Put fork2 in shelf2"
}


File: prompts/hl_retrieval.txt

Given the instruction, determine what are the constraints that is relevant to this instruction. Answer with indexes in list format. Below is an example.
Instruction: Cook the meal.
Constraints:
1. The scissors should be put in top drawer.
2. The carrot is in the fridge.
3. The right stove is broken.
Response: [2, 3]

Instruction: {instruction}
Constraints:
{constraints}
Response: 


File: prompts/is_planning_error.txt

Please determine whether the given correction indicates there's an error beyond the task's scope.

Task: open the top shelf
Correction: move right a little bit
Output: no

Task: pick up the apple.
Correction: that's wrong. move left please
Output: no

Task: put the apple into the fridge
Correction: you should open the fridge first
Output: yes

Task: open the fridge
Correction: wrong detection
Output: no

Task: pick up the pen
Correction: no, pick the red pen
Output: yes


File: prompts/ll_backbone.txt

Imagine you are programming a robot equipped with a two-fingered gripper for grasping objects to complete certain tasks. The robot has several functions at its disposal:

1. move_gripper_to_pose(pos, ori)
2. get_task_pose(task_name)
3. parse_pos(pos_description)
4. open_gripper(width=1)
5. close_gripper(width=0)
6. get_current_state()
7. reset_to_default_pose()

Here are some examples to use above functions.

{}


File: prompts/ll_content.txt

# Task: Place the green pen into the top shelf.
# Task-related knowledge: None
put_pos, put_ori = get_task_pose("put the green pen into the top shelf")
preput_pos = parse_pos(f"a point 5cm up and back to {put_pos}")
move_gripper_to_pose(preput_pos, put_ori)
current_pos, current_ori = get_current_state()
put_pos = parse_pos(f"a point a bit down to {current_pos}")
move_gripper_to_pose(put_pos, current_ori)
open_gripper()

# Task: Open the top brown cabinet
# Task-related knowledge: gripper_width
grasp_pos, grasp_ori = get_task_pose("open the top brown cabinet")
open_gripper(gripper_width)
move_gripper_to_pose(grasp_pos, grasp_ori)
close_gripper()
current_pos, current_ori = get_current_state()
pull_pos = parse_pos(f"a point 20cm back to {current_pos}.")
move_gripper_to_pose(pull_pos, current_ori)
open_gripper()

# Task: Pick up the cup.
# Task-related knowledge: pickup_height
pick_pos, pick_ori = get_task_pose("pick up the cup")
open_gripper()
move_gripper_to_pose(cup_pos, cup_ori)
close_gripper()
current_pos, current_ori = get_current_state()
pickup_pos = parse_pos(f"a point {pickup_height} above to {current_pos}.")
move_gripper_to_pose(pickup_pos, current_ori)

# Task: Close the white drawer
# Task-related knowledge: None
close_pos, close_ori = get_task_pose("close the top cabinet")
close_gripper()
preclose_pos = parse_pos(f"a point 20cm backward to {close_pos}.")
move_gripper_to_pose(preclose_pos, close_ori)
current_pos, current_ori = get_current_state()
push_pos = parse_pos(f"a point 20cm forward to {current_pos}.")
move_gripper_to_pose(push_pos, current_ori)

# Task: Place the fork on the top table.
# Task-related knowledge: place_depth
put_pos, put_ori = get_task_pose("place the fork on the top table")
preput_pos = parse_pos(f"a point a bit up to {put_pos}")
move_gripper_to_pose(preput_pos, put_ori)
current_pos, current_ori = get_current_state()
put_pos = parse_pos(f"a point {place_depth} to {current_pos}")
move_gripper_to_pose(put_pos, current_ori)
open_gripper()


File: prompts/ll_corr.txt

Imagine you are programming a robot equipped with a two-fingered gripper for grasping objects to complete certain tasks. The robot has several functions at its disposal:

1. move_gripper_to_pose(pos, ori)
2. get_task_pose(task_name)
3. parse_pos(pos_description)
4. parse_ori(ori_description)
5. open_gripper(width=1)
6. close_gripper(width=0)
7. get_current_state()
8. reset_to_default_pose()
9. change_reference_frame(wrong_direction, correct_direction)

Here are some examples to use above functions.

# Task: tilt up a little bit and rotate clockwise 45 degrees
current_pos, current_ori = get_current_state()
target_ori = parse_ori(f"tilt up a little bit relative to {current_ori}.")
move_gripper_to_pose(current_pos, target_ori)
target_ori_2 = parse_ori(f"rotate clockwise 45 degrees relative to {target_ori}.")
move_gripper_to_pose(current_pos, target_ori_2)

# Task: move forward a little bit
current_pos, current_ori = get_current_state()
target_pos = parse_pos(f"a point a little bit forward to {current_pos}")
move_gripper_to_pose(target_pos, current_ori)

# Task: move down and put down the pen.
current_pos, current_ori = get_current_state()
target_pos = parse_pos(f"a point a bit down to {current_pos}")
move_gripper_to_pose(target_pos, current_ori)
open_gripper()

# Task: move towards the white drawer a little bit
current_pos, current_ori = get_current_state()
white_drawer_pos, white_drawer_ori = get_task_pose("move towards the white drawer a little bit")
target_pos = parse_pos(f"a point a little bit towards {white_drawer_pos}")
move_gripper_to_pose(target_pos, current_ori)

# Task: move along the red block 3cm
current_pos, current_ori = get_current_state()
block_pos, block_ori = get_task_pose("move along the red block 3cm")
target_pos = parse_pos(f"a point a little bit towards {block_pos}")
move_gripper_to_pose(target_pos, current_ori)

# Task: that's forward. you should move right.
change_reference_frame('forward', 'right')
current_pos, current_ori = get_current_state()
target_pos = parse_pos(f"a point right to {current_pos}")
move_gripper_to_pose(target_pos, current_ori)
open_gripper()

# Task: flip the cup
# Assume the cup is already in the hand.
current_pos, current_ori = get_current_state()
target_ori = parse_ori(f"rotate clockwise 180 degrees relative to {current_ori}")
move_gripper_to_pose(current_pos, target_ori)

# Task: the gripper should be oriented parallel to the table
current_pos, current_ori = get_current_state()
target_ori = parse_ori(f"tilt to horizontal")
move_gripper_to_pose(current_pos, target_ori)

You are provided with a program, its outcome and human's corrective feedback. The program and the outcome are only meant to provide you the context to implement human's feedback. You don't need to continue the given program.
You have access to the variables defined in the previous code.

The robot's current task is: "[]".

Respond in the following format:
Response:
'''
...
'''


File: prompts/ll_corr_nohist.txt

Imagine you are programming a robot equipped with a two-fingered gripper for grasping objects to complete certain tasks. The robot has several functions at its disposal:

1. move_gripper_to_pose(pos, ori)
2. get_task_pose(task_name)
3. parse_pos(pos_description)
4. parse_ori(ori_description)
5. open_gripper(width=1)
6. close_gripper(width=0)
7. get_current_state()
8. reset_to_default_pose()

Here are some examples to use above functions.

# Task: tilt up a little bit and rotate clockwise 45 degrees
current_pos, current_ori = get_current_state()
target_ori = parse_ori(f"tilt up a little bit relative to {current_ori}.")
move_gripper_to_pose(current_pos, target_ori)
target_ori_2 = parse_ori(f"rotate clockwise 45 degrees relative to {target_ori}.")
move_gripper_to_pose(current_pos, target_ori_2)

# Task: move forward a little bit
current_pos, current_ori = get_current_state()
target_pos = parse_pos(f"a point a little bit forward to {current_pos}")
move_gripper_to_pose(target_pos, current_ori)

# Task: move down and put down the pen.
current_pos, current_ori = get_current_state()
target_pos = parse_pos(f"a point a bit down to {current_pos}")
move_gripper_to_pose(target_pos, current_ori)
open_gripper()

# Task: move towards the white drawer a little bit
current_pos, current_ori = get_current_state()
white_drawer_pos, white_drawer_ori = get_task_pose("move towards the white drawer a little bit")
target_pos = parse_pos(f"a point a little bit towards {white_drawer_pos}")
move_gripper_to_pose(target_pos, current_ori)

# Task: move along the red block
current_pos, current_ori = get_current_state()
target_pos = parse_pos(f"a point forward to {current_pos}")
move_gripper_to_pose(target_pos, current_ori)

# Task: flip the cup
# Assume the cup is already in the hand.
current_pos, current_ori = get_current_state()
target_ori = parse_ori(f"rotate clockwise 180 degrees relative to {current_ori}")
move_gripper_to_pose(current_pos, target_ori)

# Task: the gripper should be oriented parallel to the table
current_pos, current_ori = get_current_state()
target_ori = parse_ori(f"tilt to horizontal")
move_gripper_to_pose(current_pos, target_ori)


File: prompts/ll_distill.txt

A robot is doing a task, and a human is giving it feedback to help it complete the task correctly. 
Your first task is to use the command 'save_information(info_name, info_value, response_round: int)' to store the correct value of "task-related knowledge".
Your second task is to determine whether there are additional waypoints needed. To do this, you need to distinguish between positional adjustments and waypoints based on analyzing human's feedback.

Below are some examples:
Interaction history:
'''
Task: Insert the pen to the holder, with the pen already in hand.
# Task-related knowledge: insertion_pos, insertion_ori, insertion_depth

Response 1:
'''
insert_pos, insert_ori = get_task_pose("insert the pen to the holder")
move_gripper_to_pose(insert_pos, insert_ori)
current_pos, current_ori = get_current_state()
movedown_pos = parse_pos(f"a point 5cm to {current_pos}.")
move_gripper_to_pose(movedown_pos, current_ori)
open_gripper()
'''
Outcome: Interrupted at codeline "move_gripper_to_pose(insert_pos, insert_ori)".
Human feedback: move forward a bit

Response to feedback 2:
'''
current_pos, current_ori = get_current_state()
target_pos = parse_pos(f"a point a bit backward to {current_pos}")
move_gripper_to_pose(target_pos, current_ori)

'''
Outcome: Done.
Human feedback: move left and tilt up a little bit

Response to feedback 3:
'''
current_pos, current_ori = get_current_state()
target_pos = parse_pos(f"a point a little bit left to {current_pos}.")
target_ori = parse_ori(f"tilt up a little bit relative to {current ori}.")
move_gripper_to_pose(target_pos, target_ori)
'''
Outcome: Done.
Human feedback: Tilt left a bit.

Response to feedback 4:
'''
current_pos, current_ori = get_current_state()
target_ori = parse_ori(f"tilt left a bit relative to {current ori}.")
move_gripper_to_pose(current_pos, target_ori)
'''
Outcome: Done.
Human feedback: Now put it down.

Response to feedback 5:
'''
movedown_pos = parse_pos(f"a point 5cm to {current_pos}.")
move_gripper_to_pose(movedown_pos, current_ori)
open_gripper()
'''
Outcome: Done.
Human feedback: Done.
'''

Your response:
save_information("insertion_pos", current_pos, 4)
save_information("insertion_ori", target_ori, 4)
save_information("insertion_depth", "5cm", 5)
Additional waypoint needed: None


Interaction history:
'''
Task: Pick up the pen
# Task-related knowledge: pickup_pos, pickup_ori, pickup_height

Response 1:
'''
pickup_pos, pickup_ori = get_task_pose("pick up the pen")
move_gripper_to_pose(pickup_pos, pickup_ori)
close_gripper()
current_pos, current_ori = get_current_state()
moveup_pos = parse_pos(f"a point 5cm to {current_pos}.")
move_gripper_to_pose(moveup_pos, current_ori)
'''
Outcome: Interrupted at codeline "move_gripper_to_pose(pickup_pos, pickup_ori)".
Human feedback: move left and forward a bit

Response to feedback 2:
'''
current_pos, current_ori = get_current_state()
target_pos = parse_pos(f"a point a bit left and forward to {current_pos}")
move_gripper_to_pose(target_pos, current_ori)
'''
Outcome: Done.
Human feedback: Now pick it up

Response to feedback 3:
'''
close_gripper()
moveup_pos = parse_pos(f"a point 5cm to {current_pos}.")
move_gripper_to_pose(moveup_pos, current_ori)
'''
Outcome: Done.
Human feedback: flip the pen.
'''

Response to feedback 4:
'''
current_pos, current_ori = get_current_state()
target_ori = parse_ori(f"rotate clockwise 180 degrees relative to {current_ori}")
move_gripper_to_pose(current_pos, target_ori)
'''
Outcome: Done.
Human feedback: Done.
'''

Your response:
save_information("pickup_pos", target_pos, 2)
save_information("pickup_ori", current_ori, 2)
save_information("pickup_height", "5cm", 3)
Additional waypoint needed: Yes. "Flip the pen"
New code:
'''
pickup_pos, pickup_ori = get_task_pose("pick up the pen")
move_gripper_to_pose(pickup_pos, pickup_ori)
close_gripper()
current_pos, current_ori = get_current_state()
moveup_pos = parse_pos(f"a point 5cm to {current_pos}.")
move_gripper_to_pose(moveup_pos, current_ori)
target_ori = parse_ori(f"rotate clockwise 180 degrees relative to {current_ori}")
move_gripper_to_pose(moveup_pos, target_ori)
'''


File: prompts/ll_retrieval.txt

Imagine you're designing a robot with specific tasks. I'll give you a list of tasks the robot has previously performed and a new task to address. Your goal is to determine the following:
1. Does the new task fall into the same category with the any previous task (e.g., "open", "place" and "pick up" are different categories of tasks)?
2. If both are "Yes", which specific previous tasks are they that meet both requirements?

Answer format:
{
    "1": "Yes" or "No",
    "2": {TASK_INDEX} (If applicable. Answer with the index of the task (start from 1). Form your answers in list format.)
}

Previous tasks: []
New task: []


File: prompts/parse_name.txt

Please determine which category does the given object in the sentence fall into: [["coffee maker", "coffee machine", "keurig coffee maker"], ["red cup", "red mug"], ["coffee capsule", "cone"], ["pink cup", "pink mug"], ["white spoon"]]
Always answer one number. If you think there is no answer, just answer one number with your best guess.
Sentence: Put the coffee capsule into the coffee machine.
Category (index of the list, start from 0): 0

Please determine which category does the given object in the sentence fall into: {object_class}
Always answer one number. If you think there is no answer, just answer one number with your best guess.
Sentence: {object_name}
Category (index of the list, start from 0): 


File: prompts/parse_ori.py

from perception_utils import get_ori, get_horizontal_ori, get_vertical_ori

# Query: tilt up a little bit
ret_val = get_ori(degrees=10, axis='x')

# Query: rotate clockwise 45 degrees
ret_val = get_ori(45, 'z')

# Query: tilt right more
ret_val = get_ori(-30, 'y')

# Query: vertical
ret_val = get_vertical_ori()

# Query: rotate to horizontal
ret_val = get_horizontal_ori()


File: prompts/parse_plan.txt

Plan: {'1': 'Pick up apple_1.', '2': 'Open fridge_1.', '3': 'Put apple_x into fridge_x.', '4': 'Close fridge_x.'}
Object state: fridge_1(open), fridge_2(closed), apple_1(on table), apple_2(on table)
Output: 
{
    "1": "Pick up apple_1.",
    "2": "Put apple_1 into fridge_1",
    "3": "Close fridge_1"
}


File: prompts/parse_pos.py

from perception_utils import get_directional_vec, get_current_pos
import numpy as np

# Choose direction from "left", "right", "forward", "backward", "up", "down"
{mapping_dict}

# Query: a point a little bit left to current_pos.
current_pos = get_current_pos()
direction = get_directional_vec("left")
ret_val = current_pos + {little_left} * direction

# Query: a point a little bit up to adjusted_pos.
current_pos = get_current_pos()
direction = get_directional_vec("right")
ret_val = current_pos + {little_up} * direction

# Query: a point a little bit forward from [x y z].
current_pos = np.array([x, y, z])
direction = get_directional_vec("forward")
ret_val = current_pos + {little_forward} * direction

# Query: a point more back from current_pos.
current_pos = get_current_pos()
direction = get_directional_vec("back")
ret_val = current_pos + {more_back} * direction


File: prompts/prompt_overview.md

# Prompt Overview

## Plan-level (High-level)
1. **hl_backbone.txt**: Prompt backbone for generating high-level plans. Please refer to `utils/modulable_prompt.py` for details. 
2. **hl_cap_backbone.txt**: Prompt backbone for generating high-level plans for the Code-as-Policy baseline.
3. **hl_cap_content.txt**: Prompt content for generating high-level plans for the Code-as-Policy baseline.
4. **hl_content.txt**: Prompt content for generating high-level plans.
5. **hl_corr.txt**: Interpret high-level corrections.
6. **hl_retrieval.txt**: Retireve high-level knowledge from the knowledge base based on semantics similarity.
7. **get_constraint_feature.txt**: Determine which previous tasks interact with the same object with the current task.


## Skill-level (Low-level)
1. **ll_backbone.txt**: Prompt backbone for generating low-level skills.
2. **ll_content.txt**: Prompt content for generating low-level skills.
3. **ll_corr_nohist.txt**: Interpret low-level corrections for the no-history baseline.
4. **ll_corr.txt**: Interpret low-level corrections.
5. **ll_distill.txt**: Distill low-level knowledge from the interaction history.
6. **ll_retrieval.txt**: Retrieve low-level knowledge from the knowledge base.
7. **get_task_feature.txt**: Get the task-related knowledge type for a given task.
8. **hist_retrieval.txt**: Retrieve related interaction history to interpret a correction.


## Functional
1. **change_frame.txt**: Determine whether current reference frame need to be changed based on the corrections.
2. **get_ini_obj_state.txt**: Ask LLM to infer the initial state of the objetcs.
3. **update_obj_state.txt**: Update the objects' states after a skill is finished.
4. **get_pos_scale.txt**: Get the scale for vague distance expressions (e.g., “a little bit”) in the correction.
5. **get_pose_from_str.py**: Get the reference frame (pose) given an object query.
6. **get_task_pose_str.txt**: Determine the grasp pose for grasping a given object represented by the geometric properties of the object.
7. **is_planning_error.txt**: Determine whether the error is plan-level (high-level) or skill-level given the correction.


## Parsing
1. **parse_name.txt**: Parse the open-world object names generated by LLMs to the object names defined in CLIP candidates.
2. **parse_ori.py**: Parse a text description of orientation to a numpy array.
3. **parse_plan.txt**: Parse the textual plan to a given format.
4. **parse_pos.py**: Parse a text description of position to a numpy array.
5. **replace_des_with_val.txt**: Replace the text description of positions in the interaction history with values. 
6. **replace_true_name.txt**: Replace the vague reference in the object name with the object's true name.
7. **get_obj_name_from_task.txt**: Extract the object's name from a sentence description of the task.
8. **get_query_obj.txt**: Extract the visual feature description from the full object name.



File: prompts/replace_des_with_val.txt

Given a string, replace the vague distance expression with the given exact value.
"a point a little bit forward to current position", "1.5cm": "a point 1.5cm forward to current position"
"a point a bit more right to [0.54 0.12 0.41]", "5cm": "a point 5cm right to [0.54 0.12 0.41]"


File: prompts/replace_true_name.txt

Step: pick up the pen_x
Object name: pen1
Output: pick up the pen1


File: prompts/update_obj_state.txt

After a task has been fulfilled, the object's state shall change.
Task name: put the scissors into the top cabinet
Initial object state: top cabinet(open, empty), middle cabinet(closed, empty), bottom cabinet(closed, empty), scissors(on table), scissors(on table), apple(on table), spoon(on table)
Updated object state: top cabinet(open, not full), middle cabinet(closed, empty), bottom cabinet(closed, empty), scissors(in top cabinet), scissors(on table), apple(on table), spoon(on table)

Task name: pick up the lemon
Initial object state: lemon(on table)
Updated object state: lemon(in gripper)

Task name: open the top cabinet
Initial object state: bottom cabinet(closed)
Updated object state: bottom cabinet(closed), top cabinet(open)

Task name: {task_name}
Initial object state: {initial_state}
Updated object state: 


File: requirements.txt

openai==0.28
git+https://github.com/facebookresearch/segment-anything.git
open_clip_torch
mediapipe
pyrealsense2
imageio
open3d
gym


File: scripts/baselines/all_history.py

import numpy as np
import os, pickle, atexit, argparse, json, re
import warnings
from collections import defaultdict
from utils.modulable_prompt import modulable_prompt
from utils.string_utils import extract_array_from_str, replace_description_with_value, replace_code_with_no_description, replace_strarray_with_str, \
                            format_plan, format_code, get_lines_starting_with, format_dictionary, replace_brackets_in_file
from utils.perception.perception_utils import _parse_pos, _correct_past_detection, _get_task_pose, _get_task_detection, _change_reference_frame, get_detected_feature, compare_feature, \
                                    get_initial_state, initialize_detection, get_considered_classes, get_objs, _update_object_state, compare_text_image_sim, test_image, clear_saved_detected_obj, set_policy_and_task, set_saved_detected_obj
from utils.io.io_utils import read_py, HISTORY_TMP_PATH, add_to_log, load_file, USER_LOG, delete_file, get_previous_tasks, save_information_perm, save_information, save_plan_info
from utils.LLM_utils import query_LLM
from utils.transformation_utils import get_real_pose, calculate_relative_pose
from utils.exception_utils import InterruptedByHuman, GraspError, RobotError, PlanningError, WrongDetection, interruption_handler, robot_error_handler, grasp_error_handler, detection_error_handler, no_exception_handler, other_exception_handler

prompt_plan_instance = modulable_prompt('prompts/hl_backbone.txt', 'prompts/hl_content.txt')
prompt_codepolicy_instance = modulable_prompt('prompts/ll_backbone.txt', 'prompts/ll_content.txt')
prompt_hl_retrieval = 'prompts/hl_retrieval.txt'
prompt_parse_ori = read_py('prompts/parse_ori.py')
prompt_parse_plan = read_py('prompts/parse_plan.txt')
prompt_replace_true_name = read_py('prompts/replace_true_name.txt')
prompt_is_planning_error = read_py('prompts/is_planning_error.txt')
prompt_correction_file = 'prompts/ll_corr.txt'
prompt_correction_no_history = read_py('prompts/ll_corr_nohist.txt')
prompt_retrieve = read_py('prompts/hist_retrieval.txt')
prompt_plan_correction = read_py('prompts/hl_corr.txt')
prompt_saveinfo = read_py('prompts/ll_distill.txt')
prompt_get_task_feature_file = 'prompts/get_task_feature.txt'
prompt_get_constraint_feature = read_py('prompts/get_constraint_feature.txt')
rel_pos, rel_ori, gripper_opened = None, None, False

# ------------------------------------------ Primitives ------------------------------------------

def get_current_state():
    if realrobot:
        pose = policy.robot_env.robot.get_ee_pose()
        ee_pos = pose[0].numpy()
        ee_ori = pose[1].numpy()
        return (ee_pos, ee_ori)
    else:
        return (np.array((1.,0.,0.)), np.array((1.,0.,0.,0.)))

def get_horizontal_ori(): 
    return policy.get_horizontal_ori()

def get_vertical_ori():
    return policy.get_vertical_ori()

def open_gripper(width=1):
    global gripper_opened
    if type(width) is not int and type(width) is not float:
        width = 1
    gripper_opened = True
    policy.open_gripper(width)

def close_gripper(width=None):
    global gripper_opened
    if gripper_opened:
        check_grasp = True
    else:
        check_grasp = False
    if width is None:
        policy.close_gripper(check_grasp=check_grasp)
    else:
        if type(width) is not int and type(width) is not float:
            width = 1
        policy.open_gripper(width)

def get_ori(degrees, axis):
    return policy.rotate_gripper(degrees, axis)

def move_gripper_to_pose(pos, rot):
    return policy.move_to_pos(pos, rot)

def parse_pos(pos_description, reference_frame='object'):
    global corr_rounds
    reference_frame = str(reference_frame)
    if reference_frame != "object" and reference_frame != "absolute":
        reference_frame = 'absolute'
    numpy_array = extract_array_from_str(pos_description)
    if numpy_array is None:
        ret_val, code_as_policies = _parse_pos(pos_description, reference_frame)
        if not any(char.isdigit() for char in pos_description):
            new_pos_description = replace_description_with_value(code_as_policies, pos_description)
            replace_code_with_no_description(new_pos_description, corr_rounds)
        return ret_val
    else:
        current_pos, _ = get_current_state()
        replace = False
        if np.linalg.norm(current_pos-numpy_array) < 0.008:
            pos_description = replace_strarray_with_str(pos_description, "current position")
            replace = True
        ret_val, code_as_policies = _parse_pos(pos_description, reference_frame)
        if replace:
            if not any(char.isdigit() for char in pos_description):
                new_pos_description = replace_description_with_value(code_as_policies, pos_description)
                replace_code_with_no_description(new_pos_description, corr_rounds)
        else:
            if not any(char.isdigit() for char in pos_description.split('[')[0]):
                new_pos_description = replace_description_with_value(code_as_policies, pos_description)
                replace_code_with_no_description(new_pos_description, corr_rounds)
        return ret_val

def parse_ori(ori_description):
    ori_description = ori_description.split(' relative')[0]
    whole_prompt = prompt_parse_ori + '\n' + '\n' + f"# Query: {ori_description}" + "\n"
    response = query_LLM(whole_prompt, ["# Query:"], "cache/llm_response_parse_ori.pkl")
    code_as_policies = response.text
    add_to_log('-'*80)
    add_to_log('*at parse_ori*')
    add_to_log(code_as_policies)
    add_to_log('-'*80)
    localss = {}
    exec(code_as_policies, globals(), localss)
    return localss['ret_val']

def reset_to_default_pose():
    return policy.reset()

def get_task_pose(task):
    global rel_pos, rel_ori
    if rel_pos is None:
        pos, ori =  _get_task_pose(task, visualize=True)
        return pos, ori
    else:
        pos, ori =  _get_task_pose(task, visualize=False)
        assert len(ori) == 4
        real_pos, real_ori = get_real_pose(pos, ori, rel_pos, rel_ori)
        rel_pos, rel_ori = None, None
        return real_pos, real_ori

def get_task_detection(task):
    return _get_task_detection(task)

def change_reference_frame(wrong_direction, correct_direction):
    _change_reference_frame(wrong_direction, correct_direction)

def execute_post_action(step_name):
    if "open" in step_name.lower() or "close" in step_name.lower():
        current_pos, current_ori = get_current_state()
        target_pos = parse_pos(f"a point 8cm back to {current_pos}.", reference_frame='object')
        move_gripper_to_pose(target_pos, current_ori)
        reset_to_default_pose()
    if "put" in step_name.lower():
        current_pos, current_ori = get_current_state()
        target_pos = parse_pos(f"a point 60cm above {current_pos}.", reference_frame='absolute')
        move_gripper_to_pose(target_pos, current_ori)
        reset_to_default_pose()
    if "pick" in step_name.lower():
        current_pos, current_ori = get_current_state()
        target_pos = parse_pos(f"a point 60cm above {current_pos}.", reference_frame='absolute')
        move_gripper_to_pose(target_pos, current_ori)

# ------------------------------------------ Main function ------------------------------------------

def main():
    global corr_rounds, use_interrupted_code, gripper_opened

    # Infinite outmost loop
    while True:

        li = input('\n\n\n' + "I'm ready to take instruction." + '\n' + 'Input your instruction:')

        # Get initial objects and their states
        initialize_detection(first=True)
        obj_state = get_initial_state()
        obj_dict = get_objs()
        print(obj_state)

        # Retrieve plan-related knowledge and add it to the prompt
        plan_related_info, image_features = retrieve_plan_info(li)
        prompt_plan_instance.set_object_state(obj_state)
        prompt_plan_instance.add_constraints(plan_related_info)

        # Generate initial plans (which could only be wrong in not grounding objects)
        prompt_plan = prompt_plan_instance.get_prompt()
        whole_prompt = prompt_plan + '\n' + '\n' + f"Instruction: {li}" + "\n"
        response = query_LLM(whole_prompt, ["Instruction:"], "cache/llm_response_planning_high.pkl")
        raw_code_step = format_plan(response)
        print('***raw plan***')
        print(raw_code_step)

        # Ground the plan to true objects, as well as determine the which one is the true object
        code_step, task_features = ground_plan(raw_code_step, plan_related_info, obj_dict, obj_state, image_features)
        plan_success = False

        # Loop for handling plan failures
        while not plan_success:
            try:
                print('***grounded plan***')
                print(code_step)
                add_to_log(prompt_plan_instance.get_prompt(), also_print=False)
                add_to_log(code_step, also_print=False)
                tmp = load_file(HISTORY_TMP_PATH)
                tmp[li] = {}
                clear_saved_detected_obj()
                plan_features = {}

                # Loop for executing each sub-step
                for num, step_name in code_step.items():
                    parsed_step_name = step_name.lower()[:-1] if step_name[-1] == '.' else step_name.lower()
                    add_to_log("****Step name: " + parsed_step_name + '****', file_path=USER_LOG)
                    add_to_log(f"I am performing the task: {parsed_step_name}.", also_print=True)
                    use_interrupted_code, gripper_opened, corr_rounds = False, False, 0
                    locals = defaultdict(dict)
                    initialize_detection()

                    # Retrieve relavant task info and generate code
                    task_feature = task_features[int(num)-1]
                    if task_feature is not None:
                        set_saved_detected_obj(step_name, task_feature)
                    task_related_knowledge, localss = retrieve_task_info(step_name)
                    prompt_codepolicy = prompt_codepolicy_instance.get_prompt()
                    whole_prompt = prompt_codepolicy + '\n' + '\n' + f"# Task: {step_name}" + "\n" + task_related_knowledge + "\n"
                    response = query_LLM(whole_prompt, ["# Task:", "# Outcome:"], "cache/llm_response_planning_low.pkl")
                    _, code_as_policies = format_code(response)

                    # Add info to history temporary file for later interaction history retrieval
                    tmp[li][step_name] = {}
                    tmp[li][step_name]['code response 0'] = code_as_policies
                    add_to_log('-'*80 + '\n' + '*whole prompt*' + '\n' + whole_prompt)
                    add_to_log(f"# Task: {step_name}" + "\n" + task_related_knowledge + "\n" + '-'*80)
                    add_to_log('-'*80 + '\n' + '*original code*' + '\n' + code_as_policies + '\n' + '-'*80)
                    pickle.dump(tmp, open(HISTORY_TMP_PATH, "wb"))

                    # Main loop for code execution, correction and code regeneration
                    while True:
                        try:
                            add_to_log('-'*80 + '\n' + code_as_policies + '\n' + '-'*80, also_print=True)
                            exec(code_as_policies, globals(), localss)
                            _break, corr_rounds, code_as_policies, use_interrupted_code = no_exception_handler(localss, locals, corr_rounds, li, step_name, failure_reasoning, use_interrupted_code, code_as_policies)
                            if _break:
                                break
                        except InterruptedByHuman:
                            _break = interruption_handler(localss, locals, corr_rounds, li, step_name, code_as_policies)
                            if _break:
                                break
                            code_as_policies, corr_rounds = failure_reasoning(step_name, li, corr_rounds)
                        except RobotError:
                            _break = robot_error_handler(localss, locals, corr_rounds, li, step_name, code_as_policies)
                            if _break:
                                break
                            code_as_policies, corr_rounds = failure_reasoning(step_name, li, corr_rounds)
                        except GraspError:
                            open_gripper(width=1)
                            _break = grasp_error_handler(localss, locals, corr_rounds, li, step_name, code_as_policies)
                            if _break:
                                break
                            code_as_policies, corr_rounds = failure_reasoning(step_name, li, corr_rounds)
                        except WrongDetection:
                            code_as_policies = detection_error_handler(localss, locals, corr_rounds, li, step_name, code_as_policies)
                        except PlanningError as pe:
                            raise PlanningError(pe)
                        except Exception as e:
                            print(e)
                            _break = other_exception_handler(localss, locals, corr_rounds, li, step_name)
                            if _break:
                                break
                            code_as_policies, corr_rounds = failure_reasoning(step_name, li, corr_rounds)
                    add_to_log(f'********************Success! "{parsed_step_name}" is fulfilled !!!!!********************', also_print=True)
                    print('# of corrections: ',corr_rounds)
                    save_task_info(locals, step_name, corr_rounds, li)
                    _, task_feature, _ = get_detected_feature()
                    plan_features[parsed_step_name] = task_feature
                    obj_state = update_object_state(obj_state, parsed_step_name)
                    print(obj_state)
                plan_success = True
                delete_file(HISTORY_TMP_PATH)
            except PlanningError:
                _, task_feature, _ = get_detected_feature()
                plan_features[parsed_step_name] = task_feature
                code_step, obj_state = replan(corr_rounds, li, step_name, code_step, obj_state, plan_features)
        add_to_log("---------------------------Ready to move to next instruction...---------------------------", also_print=True)

# ------------------------------------------ LLM reasoning functions ------------------------------------------

def failure_reasoning(step_name, li, corr_rounds):
    tmp = load_file(HISTORY_TMP_PATH)
    _is_plan_error = is_plan_error(step_name, li, corr_rounds)
    if _is_plan_error:
        add_to_log("**Error Type: Planning error", also_print=True)
        raise PlanningError('')
    prompt = retrieve_interaction_history(step_name, li, corr_rounds)
    response = query_LLM(prompt, ["Outcome:", "# Outcome:", "# Task:"], 'cache/llm_response_correction.pkl')
    _, correction_code = format_code(response)
    error_type = "Don't know"
    add_to_log("**Error Type: " + error_type, also_print=True)
    add_to_log("**Response Code: " + correction_code)
    tmp = load_file(HISTORY_TMP_PATH)
    tmp[li][step_name][f'error type {corr_rounds}'] = error_type
    corr_rounds += 1
    tmp[li][step_name][f'code response {corr_rounds}'] = correction_code
    pickle.dump(tmp, open(HISTORY_TMP_PATH, "wb"))
    return correction_code, corr_rounds

def retrieve_plan_info(li):
    if not os.path.exists('cache/task_history'):
        os.makedirs('cache/task_history')
    if not os.path.exists('cache/task_history/constraints.pkl'):
        with open('cache/task_history/constraints.pkl', 'wb') as file:
            pickle.dump({}, file)
    all_constraints_dict = pickle.load(open('cache/task_history/constraints.pkl',"rb"))
    if all_constraints_dict != {}:
        v = all_constraints_dict[list(all_constraints_dict.keys())[0]]
        if type(v) == str:
            has_image=False
        else:
            has_image=True
        if has_image:
            all_constraints_list = [v[0] for _,v in all_constraints_dict.items()]
            all_image_features_list = [v[1] for _,v in all_constraints_dict.items()]
            all_constraints_str = ''
            for k,v in all_constraints_dict.items():
                all_constraints_str = all_constraints_str + f'{k}. {v[0]}'
                if k != list(all_constraints_dict.keys())[-1]:
                    all_constraints_str += '\n'
            with open(prompt_hl_retrieval, "r") as template_file:
                template_content = template_file.read()
            values = {"instruction": li, "constraints": all_constraints_str}
            prompt = template_content.format(**values)
            response = query_LLM(prompt, [], 'cache/llm_retrieve_plan_info.pkl')
            constraint_index_list = eval(response.text)
            print(f'Planning-level retrieval: Retrieving these constraints: {[all_constraints_list[i-1] for i in constraint_index_list]}')
            return [all_constraints_list[i-1] for i in constraint_index_list], [all_image_features_list[i-1] for i in constraint_index_list]
        else:
            all_constraints_list = [v for _,v in all_constraints_dict.items()]
            all_constraints_str = ''
            for k,v in all_constraints_dict.items():
                all_constraints_str = all_constraints_str + f'{k}. {v}'
                if k != list(all_constraints_dict.keys())[-1]:
                    all_constraints_str += '\n'
            with open(prompt_hl_retrieval, "r") as template_file:
                template_content = template_file.read()
            values = {"instruction": li, "constraints": all_constraints_str}
            prompt = template_content.format(**values)
            print(prompt)
            response = query_LLM(prompt, [], 'cache/llm_retrieve_plan_info.pkl')
            print(response.text)
            constraint_index_list = eval(response.text)
            print(f'Planning-level retrieval: Retrieving these constraints: {[all_constraints_list[i-1] for i in constraint_index_list]}')
            return ([all_constraints_list[i-1] for i in constraint_index_list], None)
    else:
        print('Planning-level retrieval: not retrieving anything...')
        return ([], None)

def ground_plan(raw_plan_step, plan_related_info, obj_dict, obj_state, image_features):
    grounded_plan = {}
    task_features = []
    for num, step in raw_plan_step.items():
        if isinstance(step, str):
            grounded_plan[num] = step
            task_features.append(None)
        else:
            if len(plan_related_info) == 1:
                raw_step, _ = step
                grounded_plan[num] = replace_step_with_true_obj_name(raw_step,plan_related_info[0])
                task_features.append(None)
                print(f"Replacing '{raw_step}' with '{grounded_plan[num]}'.")
            else:
                raw_step, _ = step
                task_feature = get_task_detection(raw_step)
                sims = []
                for feature in image_features:
                    sims.append(((feature.squeeze()-task_feature.squeeze())**2).sum())
                prob_order = np.argsort(np.array(sims))
                retrieved_idx = prob_order[0]
                grounded_plan[num] = replace_step_with_true_obj_name(raw_step,plan_related_info[retrieved_idx])
                task_features.append(task_feature)
                print(f"Replacing '{raw_step}' with '{grounded_plan[num]}'.")
    prompt = prompt_parse_plan + '\n' + f'Plan: {grounded_plan}' + '\n' + f'Object state: {obj_state}' + '\n' + 'Output: '
    response = query_LLM(prompt, ["Plan: "], "cache/llm_parse_plan.pkl")
    grounded_plan = json.loads(response.text)
    return grounded_plan, task_features

def replace_step_with_true_obj_name(step, true_object_name):
    prompt = prompt_replace_true_name + '\n' + f'Step name: {step}' + '\n' + f'Object name: {true_object_name}' + '\n' + 'Output: '
    response = query_LLM(prompt, [], 'cache/llm_replace_true_name.pkl')
    grounded_step = response.text
    return grounded_step 

def is_plan_error(step_name, li, corr_rounds):
    tmp = load_file(HISTORY_TMP_PATH)
    correction = tmp[li][step_name][f'Correction {corr_rounds}']
    prompt = prompt_is_planning_error + '\n\n' + f'Task: {step_name}' + '\n' +f'Correction: {correction}' + '\n' + 'Output: '
    response = query_LLM(prompt, [], 'cache/llm_is_planning_error.pkl')
    add_to_log(prompt)
    add_to_log(response.text)
    if 'yes' in response.text.lower():
        is_planning_error = True
    elif 'no' in response.text.lower():
        is_planning_error = False
    return is_planning_error

def retrieve_interaction_history(step_name, li, corr_rounds):
    global use_interrupted_code
    tmp = load_file(HISTORY_TMP_PATH)
    correction = tmp[li][step_name][f'Correction {corr_rounds}']
    prompt_correction = replace_brackets_in_file(prompt_correction_file, step_name)
    history_prompt = ''
    history_prompt = history_prompt + 'Response 1:' + '\n' + "'''" + '\n' + tmp[li][step_name]['code response 0'] + '\n' + "'''"
    history_prompt = history_prompt + '\n' + 'Outcome: ' + tmp[li][step_name]['Outcome 0']
    history_prompt = history_prompt + '\n' + 'Human feedback: ' + tmp[li][step_name]['Correction 0']
    for i in range(1, corr_rounds+1):
        history_prompt = history_prompt + '\n' + '\n'
        history_prompt = history_prompt + f'Response to feedback {i+1}:' + '\n' + "'''" + '\n' + tmp[li][step_name][f'code response {i}'] + '\n' + "'''"
        history_prompt = history_prompt + '\n' + 'Outcome: ' + tmp[li][step_name][f'Outcome {i}']
        history_prompt = history_prompt + '\n' + 'Human feedback: ' + tmp[li][step_name][f'Correction {i}']
    whole_prompt = prompt_correction + '\n' + '\n' + history_prompt
    return whole_prompt

def replan(corr_rounds, li, step_name, original_plan, object_state, plan_features):
    parsed_step_name = step_name.lower()[:-1] if step_name[-1] == '.' else step_name.lower()
    tmp = load_file(HISTORY_TMP_PATH)
    correction = tmp[li][step_name][f'Correction {corr_rounds}']
    whole_prompt = prompt_plan_correction + '\n' + '\n' + f"Task: {li}" + "\n" + f"Plan:\n{format_dictionary(original_plan)}" + '\n' + f'Outcome: Interrupted by human at the step "{parsed_step_name}.'
    whole_prompt = whole_prompt + '\n' + 'Correction: ' + correction + '.' + '\n' + f'Object state: {object_state}'
    response = query_LLM(whole_prompt, ["Task:"], "cache/llm_planning_correction.pkl")
    task_constraint = get_lines_starting_with(response.text, 'Task constraint:', first=True).split('Task constraint: ')[1]
    robot_constraint = get_lines_starting_with(response.text, 'Robot constraint:', first=True).split('Robot constraint: ')[1]
    updated_object_state = get_lines_starting_with(response.text, 'Updated object state:', first=True).split('Updated object state: ')[1]
    code_step = format_plan(response, "Modified original plan:")
    add_to_log(code_step, also_print=False)
    print(response.text)
    if 'none' in task_constraint.lower():
        pass
    else:
        task_feature = get_constraint_related_feature(task_constraint, plan_features)
        prompt_plan_instance.add_constraints(task_constraint)
        save_plan_info(task_constraint, task_feature)
    if 'none' in robot_constraint.lower():
        pass
    else:
        prompt_plan_instance.add_constraints(robot_constraint)
        save_plan_info(robot_constraint)
    add_to_log(task_constraint, also_print=False)
    add_to_log(robot_constraint, also_print=False)
    update_plan = f'Instruction: {li}' + '\n' + 'Response:' + '\n' + format_dictionary(code_step)
    prompt_plan_instance.update_content(update_plan)
    prompt_plan_instance.set_object_state(updated_object_state)
    re_plan = format_plan(response, "Replan:")
    add_to_log(re_plan, also_print=False)
    delete_file(HISTORY_TMP_PATH)
    return re_plan, updated_object_state

def update_object_state(obj_state, task_name):
    new_obj_state = _update_object_state(obj_state, task_name)
    prompt_plan_instance.set_object_state(new_obj_state)
    return new_obj_state

def retrieve_task_info(step_name):
    global rel_pos, rel_ori

    previous_tasks = get_previous_tasks()

    if previous_tasks == '':
        print('Skill-level: not retrieving anything...')
        return "# Task-related knowledge: None.", {}
    else:

        with open("prompts/ll_retrieval.txt", "r") as template_file:
            template_content = template_file.read()
        filled_content = template_content.replace("[]", previous_tasks, 1)
        prompt = filled_content.replace("[]", step_name, 1)
        response = query_LLM(prompt, [], 'cache/llm_check_done.pkl')
        add_to_log(prompt,also_print=False)
        add_to_log(response.text, also_print=False)
        response_dict = json.loads(response.text)

        if response_dict["1"] == 'Yes':
            task_visual_feature = get_task_detection(step_name)
            max_diff = 10000
            for task_index in response_dict["2"]:
                assert type(task_index) == int
                info_dict : dict = pickle.load(open(f'cache/task_history/{task_index}.pkl', "rb"))
                vis_feature = info_dict['dino_image_feature']
                diff = np.linalg.norm(vis_feature-task_visual_feature)
                if diff<=max_diff:
                    max_diff = diff
                    ind = task_index
            info_dict : dict = pickle.load(open(f'cache/task_history/{ind}.pkl', "rb"))
            rel_pos = info_dict['relative_pose'][0]
            rel_ori = info_dict['relative_pose'][1]
            other_keys = [key for key in info_dict.keys() if key != 'relative_pose' and key != 'image_feature' and key != 'code' and key !='dino_image_feature']
            ret_str = "# Task-related knowledge: "
            locals = {}
            for i in range(len(other_keys)):
                k = other_keys[i].replace(" ", "_")
                ret_str += k
                locals[k] = info_dict[other_keys[i]]
                if i != len(other_keys)-1:
                    ret_str += ', '
            if ret_str == "# Task-related knowledge: ":
                ret_str += 'None.'
            if 'code' in info_dict.keys():
                update_code = '# Task: ' + step_name + '\n' + ret_str + '\n' + info_dict['code']
                prompt_codepolicy_instance.update_content(update_code)
            print(f'Skill-level: retrieving the knowledge these knowledge "{ret_str}".')
            return ret_str, locals
        else:
            return "# Task-related knowledge: None.", {}

def save_task_info(locals, step_name, corr_rounds, li):
    parsed_step_name = step_name.lower()[:-1] if step_name[-1] == '.' else step_name.lower()
    with open(prompt_get_task_feature_file, "r") as template_file:
        template_content = template_file.read()
    prompt_get_task = template_content.replace("{}", parsed_step_name, 1)
    response = query_LLM(prompt_get_task, ['The information'], "cache/llm_get_task_feature.pkl")
    _, task_related_info = format_code(response)
    tmp = load_file(HISTORY_TMP_PATH)
    history_prompt = "Interaction history:" + '\n' + "'''" + '\n'
    history_prompt = history_prompt + f'Task: {step_name}' + '\n'
    history_prompt = history_prompt + f'Task-related knowledge: {task_related_info}' + '\n' + '\n'
    history_prompt = history_prompt + 'Response 1:' + '\n' + "'''" + '\n' + tmp[li][step_name]['code response 0'] + '\n' + "'''"
    history_prompt = history_prompt + '\n' + 'Outcome: ' + tmp[li][step_name]['Outcome 0']
    history_prompt = history_prompt + '\n' + 'Human feedback: ' + tmp[li][step_name]['Correction 0']
    for i in range(1, corr_rounds+1):
        history_prompt = history_prompt + '\n' + '\n'
        history_prompt = history_prompt + f'Response to feedback {i+1}:' + '\n' + "'''" + '\n' + tmp[li][step_name][f'code response {i}'] + '\n' + "'''"
        history_prompt = history_prompt + '\n' + 'Outcome: ' + tmp[li][step_name][f'Outcome {i}']
        history_prompt = history_prompt + '\n' + 'Human feedback: ' + tmp[li][step_name][f'Correction {i}']
    history_prompt = prompt_saveinfo + '\n' + '\n' + history_prompt + '\n' + "'''" + '\n' + '\n' + 'Your response:' + '\n'
    response = query_LLM(history_prompt, ['Your response:', 'Interaction'], "cache/llm_response_save_info.pkl")
    codes = get_lines_starting_with(response.text, 'save_information', first=False)
    _, code_as_policies = format_code(response)
    if code_as_policies == response.text:
        has_code = False
    else:
        has_code = True
        add_to_log(code_as_policies)
    add_to_log('-'*80)
    add_to_log('*at save_info*')
    add_to_log(history_prompt, also_print=False)
    add_to_log(response.text, also_print=True)
    add_to_log(locals)
    add_to_log('-'*80)
    localss = locals.copy()
    try:
        for code in codes:
            try:
                int(code[-3])
                locals = localss[int(code[-3:-1])-1]
            except:
                locals = localss[int(code[-2])-1]
            exec(code, globals(), locals)
        info_dict : dict = pickle.load(open('cache/tmp_info.pkl', "rb"))
        if has_code:
            info_dict['code'] = code_as_policies
        keys_with_ori = [key for key in info_dict.keys() if 'ori' in key]
        real_ori = info_dict[keys_with_ori[0]]
        keys_with_pos = [key for key in info_dict.keys() if 'pos' in key]
        real_pos = info_dict[keys_with_pos[0]]
        image_feature, dino_image_feature, detected_pose = get_detected_feature()
        d_pos, d_ori = detected_pose
        assert len(d_ori) == 4
        assert len(real_ori) == 4
        relative_pose = calculate_relative_pose((real_pos,real_ori), (d_pos, d_ori), is_quat=True)
        info_to_save = ({'relative_pose': relative_pose,'image_feature': image_feature, 'dino_image_feature': dino_image_feature})
        other_keys = [key for key in info_dict.keys() if 'ori' not in key and 'pos' not in key]
        for key in other_keys:
            info_to_save[key] = info_dict[key]
        save_information_perm(info_to_save)
        task_related_info = re.sub(r'\b\w+(_pos|_ori)\b', '', task_related_info)
        task_related_info = re.sub(r',+', ',', task_related_info)
        task_related_info = task_related_info.strip(', ')
    except:
        pass
    execute_post_action(step_name)
    delete_file('cache/tmp_info.pkl')

def get_constraint_related_feature(constraint, plan_features):
    task_list = ''
    idx = 1
    for k, _ in plan_features.items():
        task_list += str(idx) + '. ' + k + '.'
        idx += 1
    prompt = prompt_get_constraint_feature + '\n\n' + f'Task list: {task_list}' + '\n' + f'Query: {constraint}' + '\n' + 'Response: '
    response = query_LLM(prompt, ["Task:"], "cache/llm_get_constraint_feature.pkl")
    print('****')
    print(prompt)
    print(response.text)
    vis_idx = eval(response.text)[0]
    return plan_features[list(plan_features.keys())[vis_idx-1]]

if __name__ == '__main__':
    global policy, realrobot
    def delete_tmp():
        delete_file(HISTORY_TMP_PATH)
        # with open('new_codeprompt.txt', 'w') as file:
        #     file.write(prompt_codepolicy_instance.get_prompt())
        # with open('new_planprompt.txt', 'w') as file:
        #     file.write(prompt_plan_instance.get_prompt())
    atexit.register(delete_tmp)
    parser = argparse.ArgumentParser()
    parser.add_argument("--task", type=str, default='drawer')
    parser.add_argument("--realrobot", type=bool, default=False)
    args = parser.parse_args()
    task = args.task
    realrobot = args.realrobot
    set_policy_and_task(realrobot, task)
    if realrobot:
        from utils.robot.robot_policy import KptPrimitivePolicy
        policy = KptPrimitivePolicy()
    else:
        from utils.robot.dummy_policy import DummyPolicy
        policy = DummyPolicy()
    if os.path.exists(f"log/{task}/"):
        pass
    else:
        os.makedirs(f"log/{task}/")
    main()


File: scripts/baselines/cap.py

import numpy as np
import os, atexit, argparse
import utils.perception.perception_utils
from utils.modulable_prompt import modulable_prompt
from utils.LLM_utils import query_LLM
from utils.io.io_utils import read_py, USER_LOG, add_to_log
from utils.perception.perception_utils import _parse_pos, _correct_past_detection, _get_task_pose, _get_task_detection, _change_reference_frame, initialize_detection, set_policy_and_task, clear_saved_detected_obj
from utils.string_utils import format_code, format_plan, extract_array_from_str, replace_strarray_with_str
from utils.exception_utils import InterruptedByHuman, GraspError, RobotError, PlanningError, WrongDetection

prompt_plan_instance = modulable_prompt('prompts/hl_cap_backbone.txt', 'prompts/hl_cap_content.txt')
prompt_codepolicy_instance = modulable_prompt('prompts/ll_backbone.txt', 'prompts/ll_content.txt')
prompt_correction_no_history = read_py('prompts/ll_corr_nohist.txt')
prompt_parse_ori = read_py('prompts/parse_ori.py')
gripper_opened = False

# ------------------------------------------ Primitives ------------------------------------------

def get_current_state():
    if realrobot:
        pose = policy.robot_env.robot.get_ee_pose()
        ee_pos = pose[0].numpy()
        ee_ori = pose[1].numpy()
        return (ee_pos, ee_ori)
    else:
        return (np.array((1.,0.,0.)), np.array((1.,0.,0.,0.)))

def get_horizontal_ori():
    return policy.get_horizontal_ori()

def get_vertical_ori():
    return policy.get_vertical_ori()

def open_gripper(width=1):
    global gripper_opened
    if type(width) is not int and type(width) is not float:
        width = 1
    gripper_opened = True
    policy.open_gripper(width)

def close_gripper(width=None):
    global gripper_opened
    if gripper_opened:
        check_grasp = True
    else:
        check_grasp = False
    if width is None:
        policy.close_gripper(check_grasp=check_grasp)
    else:
        if type(width) is not int and type(width) is not float:
            width = 1
        policy.open_gripper(width)

def get_ori(degrees, axis):
    return policy.rotate_gripper(degrees, axis)

def move_gripper_to_pose(pos, rot):
    return policy.move_to_pos(pos, rot)

def parse_pos(pos_description, reference_frame='object'):
    reference_frame = str(reference_frame)
    if reference_frame != "object" and reference_frame != "absolute":
        reference_frame = 'absolute'
    numpy_array = extract_array_from_str(pos_description)
    if numpy_array is None:
        ret_val, _ = _parse_pos(pos_description, reference_frame)
        return ret_val
    else:
        current_pos, _ = get_current_state()
        if np.linalg.norm(current_pos-numpy_array) < 0.008:
            pos_description = replace_strarray_with_str(pos_description, "current position")
        ret_val, _ = _parse_pos(pos_description, reference_frame)
        return ret_val

def parse_ori(ori_description):
    ori_description = ori_description.split(' relative')[0]
    whole_prompt = prompt_parse_ori + '\n' + '\n' + f"# Query: {ori_description}" + "\n"
    response = query_LLM(whole_prompt, ["# Query:"], "cache/llm_response_parse_ori.pkl")
    code_as_policies = response.text
    add_to_log('-'*80)
    add_to_log('*at parse_ori*')
    add_to_log(code_as_policies)
    add_to_log('-'*80)
    localss = {}
    exec(code_as_policies, globals(), localss)
    return localss['ret_val']

def reset_to_default_pose():
    return policy.reset()

def delete_last_detection(obj_name, corrected_pos=None):
    _correct_past_detection(obj_name, corrected_pos)

def get_task_pose(task):
    pos, ori =  _get_task_pose(task, visualize=True)
    return pos, ori

def get_task_detection(task):
    return _get_task_detection(task)

def change_reference_frame(wrong_direction, correct_direction):
    _change_reference_frame(wrong_direction, correct_direction)

def execute_post_action(step_name):
    if "open" in step_name.lower() or "close" in step_name.lower():
        current_pos, current_ori = get_current_state()
        target_pos = parse_pos(f"a point 8cm back to {current_pos}.", reference_frame='object')
        move_gripper_to_pose(target_pos, current_ori)
        reset_to_default_pose()
    if "put" in step_name.lower():
        current_pos, current_ori = get_current_state()
        target_pos = parse_pos(f"a point 60cm above {current_pos}.", reference_frame='absolute')
        move_gripper_to_pose(target_pos, current_ori)
        reset_to_default_pose()
    if "pick" in step_name.lower():
        current_pos, current_ori = get_current_state()
        target_pos = parse_pos(f"a point 60cm above {current_pos}.", reference_frame='absolute')
        move_gripper_to_pose(target_pos, current_ori)

# ------------------------------------------ Main function ------------------------------------------

def main():
    global gripper_opened
    # Infinite outmost loop
    while True:

        # Receive instructions and generate the initial plan
        li = input('\n\n\n' + "I'm ready to take instruction." + '\n' + 'Input your instruction:')
        initialize_detection(first=True)
        prompt_plan = prompt_plan_instance.get_prompt()
        whole_prompt = prompt_plan + '\n' + '\n' + f"Instruction: {li}" + "\n"
        response = query_LLM(whole_prompt, ["Instruction:"], "cache/llm_response_planning_high.pkl")
        code_step = format_plan(response)
        plan_success = False

        # Loop for handling plan failures
        while not plan_success:
            try:
                add_to_log(prompt_plan, also_print=False)
                add_to_log(code_step, also_print=True)
                clear_saved_detected_obj()

                # Loop for executing each sub-step
                for _, step_name in code_step.items():
                    parsed_step_name = step_name.lower()[:-1] if step_name[-1] == '.' else step_name.lower()
                    add_to_log("****Step name: " + parsed_step_name + '****', file_path=USER_LOG)
                    add_to_log(f"I am performing the task: {parsed_step_name}.", also_print=True)
                    gripper_opened = False
                    initialize_detection()

                    # Retrieve relavant task info and generate code
                    prompt_codepolicy = prompt_codepolicy_instance.get_prompt()
                    whole_prompt = prompt_codepolicy + '\n' + '\n' + f"# Task: {step_name}" + "\n" + 'Task-related knowledge: None' + "\n"
                    response = query_LLM(whole_prompt, ["# Task:", "# Outcome:"], "cache/llm_response_planning_low.pkl")
                    _, code_as_policies = format_code(response)

                    # Add info to history temporary file for later interaction history retrieval
                    add_to_log('-'*80 + '\n' + '*whole prompt*' + '\n' + whole_prompt)
                    add_to_log(f"# Task: {step_name}" + "\n" + 'Task-related knowledge: None' + "\n" + '-'*80)

                    # Main loop for code execution, correction and code regeneration
                    while True:
                        try:
                            add_to_log('-'*80 + '\n' + '*original code*' + '\n' + code_as_policies + '\n' + '-'*80, also_print=True)
                            exec(code_as_policies, globals())
                            correction = input(f'Please issue further corrections, or is the task "{parsed_step_name}" done: ')
                            if 'done' in correction.lower() or 'yes' in correction.lower() or correction.lower() == 'y':
                                break
                            else:
                                whole_prompt = prompt_correction_no_history + '\n' + '\n' + f"# Task: {correction}."
                                response = query_LLM(whole_prompt, ["# Task:", "# Outcome:"], "cache/llm_voice_teleop.pkl")
                                _, code_as_policies = format_code(response)
                        except InterruptedByHuman:
                            correction = input(f'Please issue corrections: ')
                            if 'done' in correction.lower() or 'yes' in correction.lower() or correction.lower() == 'y':
                                break
                            else:
                                whole_prompt = prompt_correction_no_history + '\n' + '\n' + f"# Task: {correction}."
                                response = query_LLM(whole_prompt, ["# Task:", "# Outcome:"], "cache/llm_voice_teleop.pkl")
                                _, code_as_policies = format_code(response)
                        except RobotError:
                            correction = input(f'Please issue corrections: ')
                            if 'done' in correction.lower() or 'yes' in correction.lower() or correction.lower() == 'y':
                                break
                            else:
                                whole_prompt = prompt_correction_no_history + '\n' + '\n' + f"# Task: {correction}."
                                response = query_LLM(whole_prompt, ["# Task:", "# Outcome:"], "cache/llm_voice_teleop.pkl")
                                _, code_as_policies = format_code(response)
                        except GraspError:
                            open_gripper(width=1)
                            correction = input(f'Please issue corrections: ')
                            if 'done' in correction.lower() or 'yes' in correction.lower() or correction.lower() == 'y':
                                break
                            else:
                                whole_prompt = prompt_correction_no_history + '\n' + '\n' + f"# Task: {correction}."
                                response = query_LLM(whole_prompt, ["# Task:", "# Outcome:"], "cache/llm_voice_teleop.pkl")
                                _, code_as_policies = format_code(response)
                        except WrongDetection:
                            correction = 'Wrong detection. Please delete past detections.'
                            whole_prompt = prompt_correction_no_history + '\n' + '\n' + f"# Task: {correction}."
                            response = query_LLM(whole_prompt, ["# Task:", "# Outcome:"], "cache/llm_voice_teleop.pkl")
                            _, code_as_policies = format_code(response)
                        except PlanningError as pe:
                            raise PlanningError(pe)
                    add_to_log(f'********************Success! "{parsed_step_name}" is fulfilled !!!!!********************', also_print=True)
                plan_success = True
            except PlanningError:
                prompt_plan = prompt_plan_instance.get_prompt()
                whole_prompt = prompt_plan + '\n' + '\n' + f"Instruction: {li}" + "\n"
                response = query_LLM(whole_prompt, ["Instruction:"], "cache/llm_response_planning_high.pkl")
                code_step = format_plan(response)
        add_to_log("---------------------------Ready to move to next instruction...---------------------------", also_print=True)
        reset_to_default_pose()


if __name__ == '__main__':
    global policy, realrobot
    def delete_tmp():
        pass
    atexit.register(delete_tmp)
    parser = argparse.ArgumentParser()
    parser.add_argument("--task", type=str, default='drawer')
    parser.add_argument("--realrobot", type=bool, default=False)
    args = parser.parse_args()
    task = args.task
    realrobot = args.realrobot
    set_policy_and_task(realrobot, task)
    if realrobot:
        from utils.robot.robot_policy import KptPrimitivePolicy
        policy = KptPrimitivePolicy()
    else:
        from utils.robot.dummy_policy import DummyPolicy
        policy = DummyPolicy()
    if os.path.exists(f"log/{task}/"):
        pass
    else:
        os.makedirs(f"log/{task}/")
    main()


File: scripts/baselines/original_code.py

import numpy as np
import os, pickle, atexit, argparse, json, re
import warnings
from collections import defaultdict
from utils.modulable_prompt import modulable_prompt
from utils.string_utils import extract_array_from_str, replace_description_with_value, replace_code_with_no_description, replace_strarray_with_str, \
                            format_plan, format_code, get_lines_starting_with, format_dictionary, replace_brackets_in_file
from utils.perception.perception_utils import _parse_pos, _correct_past_detection, _get_task_pose, _get_task_detection, _change_reference_frame, get_detected_feature, compare_feature, \
                                    get_initial_state, initialize_detection, get_considered_classes, get_objs, _update_object_state, compare_text_image_sim, test_image, clear_saved_detected_obj, set_policy_and_task, set_saved_detected_obj
from utils.io.io_utils import read_py, HISTORY_TMP_PATH, add_to_log, load_file, USER_LOG, delete_file, get_previous_tasks, save_information_perm, save_information, save_plan_info
from utils.LLM_utils import query_LLM
from utils.transformation_utils import get_real_pose, calculate_relative_pose
from utils.exception_utils import InterruptedByHuman, GraspError, RobotError, PlanningError, WrongDetection, interruption_handler, robot_error_handler, grasp_error_handler, detection_error_handler, no_exception_handler, other_exception_handler

prompt_plan_instance = modulable_prompt('prompts/hl_backbone.txt', 'prompts/hl_content.txt')
prompt_codepolicy_instance = modulable_prompt('prompts/ll_backbone.txt', 'prompts/ll_content.txt')
prompt_hl_retrieval = 'prompts/hl_retrieval.txt'
prompt_parse_ori = read_py('prompts/parse_ori.py')
prompt_parse_plan = read_py('prompts/parse_plan.txt')
prompt_replace_true_name = read_py('prompts/replace_true_name.txt')
prompt_is_planning_error = read_py('prompts/is_planning_error.txt')
prompt_correction_file = 'prompts/ll_corr.txt'
prompt_correction_no_history = read_py('prompts/ll_corr_nohist.txt')
prompt_retrieve = read_py('prompts/hist_retrieval.txt')
prompt_plan_correction = read_py('prompts/hl_corr.txt')
prompt_saveinfo = read_py('prompts/ll_distill.txt')
prompt_get_task_feature_file = 'prompts/get_task_feature.txt'
prompt_get_constraint_feature = read_py('prompts/get_constraint_feature.txt')
rel_pos, rel_ori, gripper_opened = None, None, False

# ------------------------------------------ Primitives ------------------------------------------

def get_current_state():
    if realrobot:
        pose = policy.robot_env.robot.get_ee_pose()
        ee_pos = pose[0].numpy()
        ee_ori = pose[1].numpy()
        return (ee_pos, ee_ori)
    else:
        return (np.array((1.,0.,0.)), np.array((1.,0.,0.,0.)))

def get_horizontal_ori(): 
    return policy.get_horizontal_ori()

def get_vertical_ori():
    return policy.get_vertical_ori()

def open_gripper(width=1):
    global gripper_opened
    if type(width) is not int and type(width) is not float:
        width = 1
    gripper_opened = True
    policy.open_gripper(width)

def close_gripper(width=None):
    global gripper_opened
    if gripper_opened:
        check_grasp = True
    else:
        check_grasp = False
    if width is None:
        policy.close_gripper(check_grasp=check_grasp)
    else:
        if type(width) is not int and type(width) is not float:
            width = 1
        policy.open_gripper(width)

def get_ori(degrees, axis):
    return policy.rotate_gripper(degrees, axis)

def move_gripper_to_pose(pos, rot):
    return policy.move_to_pos(pos, rot)

def parse_pos(pos_description, reference_frame='object'):
    global corr_rounds
    reference_frame = str(reference_frame)
    if reference_frame != "object" and reference_frame != "absolute":
        reference_frame = 'absolute'
    numpy_array = extract_array_from_str(pos_description)
    if numpy_array is None:
        ret_val, code_as_policies = _parse_pos(pos_description, reference_frame)
        if not any(char.isdigit() for char in pos_description):
            new_pos_description = replace_description_with_value(code_as_policies, pos_description)
            replace_code_with_no_description(new_pos_description, corr_rounds)
        return ret_val
    else:
        current_pos, _ = get_current_state()
        replace = False
        if np.linalg.norm(current_pos-numpy_array) < 0.008:
            pos_description = replace_strarray_with_str(pos_description, "current position")
            replace = True
        ret_val, code_as_policies = _parse_pos(pos_description, reference_frame)
        if replace:
            if not any(char.isdigit() for char in pos_description):
                new_pos_description = replace_description_with_value(code_as_policies, pos_description)
                replace_code_with_no_description(new_pos_description, corr_rounds)
        else:
            if not any(char.isdigit() for char in pos_description.split('[')[0]):
                new_pos_description = replace_description_with_value(code_as_policies, pos_description)
                replace_code_with_no_description(new_pos_description, corr_rounds)
        return ret_val

def parse_ori(ori_description):
    ori_description = ori_description.split(' relative')[0]
    whole_prompt = prompt_parse_ori + '\n' + '\n' + f"# Query: {ori_description}" + "\n"
    response = query_LLM(whole_prompt, ["# Query:"], "cache/llm_response_parse_ori.pkl")
    code_as_policies = response.text
    add_to_log('-'*80)
    add_to_log('*at parse_ori*')
    add_to_log(code_as_policies)
    add_to_log('-'*80)
    localss = {}
    exec(code_as_policies, globals(), localss)
    return localss['ret_val']

def reset_to_default_pose():
    return policy.reset()

def get_task_pose(task):
    global rel_pos, rel_ori
    if rel_pos is None:
        pos, ori =  _get_task_pose(task, visualize=True)
        return pos, ori
    else:
        pos, ori =  _get_task_pose(task, visualize=False)
        assert len(ori) == 4
        real_pos, real_ori = get_real_pose(pos, ori, rel_pos, rel_ori)
        rel_pos, rel_ori = None, None
        return real_pos, real_ori

def get_task_detection(task):
    return _get_task_detection(task)

def change_reference_frame(wrong_direction, correct_direction):
    _change_reference_frame(wrong_direction, correct_direction)

def execute_post_action(step_name):
    if "open" in step_name.lower() or "close" in step_name.lower():
        current_pos, current_ori = get_current_state()
        target_pos = parse_pos(f"a point 8cm back to {current_pos}.", reference_frame='object')
        move_gripper_to_pose(target_pos, current_ori)
        reset_to_default_pose()
    if "put" in step_name.lower():
        current_pos, current_ori = get_current_state()
        target_pos = parse_pos(f"a point 60cm above {current_pos}.", reference_frame='absolute')
        move_gripper_to_pose(target_pos, current_ori)
        reset_to_default_pose()
    if "pick" in step_name.lower():
        current_pos, current_ori = get_current_state()
        target_pos = parse_pos(f"a point 60cm above {current_pos}.", reference_frame='absolute')
        move_gripper_to_pose(target_pos, current_ori)

# ------------------------------------------ Main function ------------------------------------------

def main():
    global corr_rounds, use_interrupted_code, gripper_opened

    # Infinite outmost loop
    while True:

        li = input('\n\n\n' + "I'm ready to take instruction." + '\n' + 'Input your instruction:')

        # Get initial objects and their states
        initialize_detection(first=True)
        obj_state = get_initial_state()
        obj_dict = get_objs()
        print(obj_state)

        # Retrieve plan-related knowledge and add it to the prompt
        plan_related_info, image_features = retrieve_plan_info(li)
        prompt_plan_instance.set_object_state(obj_state)
        prompt_plan_instance.add_constraints(plan_related_info)

        # Generate initial plans (which could only be wrong in not grounding objects)
        prompt_plan = prompt_plan_instance.get_prompt()
        whole_prompt = prompt_plan + '\n' + '\n' + f"Instruction: {li}" + "\n"
        response = query_LLM(whole_prompt, ["Instruction:"], "cache/llm_response_planning_high.pkl")
        raw_code_step = format_plan(response)
        print('***raw plan***')
        print(raw_code_step)

        # Ground the plan to true objects, as well as determine the which one is the true object
        code_step, task_features = ground_plan(raw_code_step, plan_related_info, obj_dict, obj_state, image_features)
        plan_success = False

        # Loop for handling plan failures
        while not plan_success:
            try:
                print('***grounded plan***')
                print(code_step)
                add_to_log(prompt_plan_instance.get_prompt(), also_print=False)
                add_to_log(code_step, also_print=False)
                tmp = load_file(HISTORY_TMP_PATH)
                tmp[li] = {}
                clear_saved_detected_obj()
                plan_features = {}

                # Loop for executing each sub-step
                for num, step_name in code_step.items():
                    parsed_step_name = step_name.lower()[:-1] if step_name[-1] == '.' else step_name.lower()
                    add_to_log("****Step name: " + parsed_step_name + '****', file_path=USER_LOG)
                    add_to_log(f"I am performing the task: {parsed_step_name}.", also_print=True)
                    use_interrupted_code, gripper_opened, corr_rounds = False, False, 0
                    locals = defaultdict(dict)
                    initialize_detection()

                    # Retrieve relavant task info and generate code
                    task_feature = task_features[int(num)-1]
                    if task_feature is not None:
                        set_saved_detected_obj(step_name, task_feature)
                    task_related_knowledge, localss = retrieve_task_info(step_name)
                    prompt_codepolicy = prompt_codepolicy_instance.get_prompt()
                    whole_prompt = prompt_codepolicy + '\n' + '\n' + f"# Task: {step_name}" + "\n" + task_related_knowledge + "\n"
                    response = query_LLM(whole_prompt, ["# Task:", "# Outcome:"], "cache/llm_response_planning_low.pkl")
                    _, code_as_policies = format_code(response)

                    # Add info to history temporary file for later interaction history retrieval
                    tmp[li][step_name] = {}
                    tmp[li][step_name]['code response 0'] = code_as_policies
                    add_to_log('-'*80 + '\n' + '*whole prompt*' + '\n' + whole_prompt)
                    add_to_log(f"# Task: {step_name}" + "\n" + task_related_knowledge + "\n" + '-'*80)
                    add_to_log('-'*80 + '\n' + '*original code*' + '\n' + code_as_policies + '\n' + '-'*80)
                    pickle.dump(tmp, open(HISTORY_TMP_PATH, "wb"))

                    # Main loop for code execution, correction and code regeneration
                    while True:
                        try:
                            add_to_log('-'*80 + '\n' + code_as_policies + '\n' + '-'*80, also_print=True)
                            exec(code_as_policies, globals(), localss)
                            _break, corr_rounds, code_as_policies, use_interrupted_code = no_exception_handler(localss, locals, corr_rounds, li, step_name, failure_reasoning, use_interrupted_code, code_as_policies)
                            if _break:
                                break
                        except InterruptedByHuman:
                            _break = interruption_handler(localss, locals, corr_rounds, li, step_name, code_as_policies)
                            if _break:
                                break
                            code_as_policies, corr_rounds = failure_reasoning(step_name, li, corr_rounds)
                        except RobotError:
                            _break = robot_error_handler(localss, locals, corr_rounds, li, step_name, code_as_policies)
                            if _break:
                                break
                            code_as_policies, corr_rounds = failure_reasoning(step_name, li, corr_rounds)
                        except GraspError:
                            open_gripper(width=1)
                            _break = grasp_error_handler(localss, locals, corr_rounds, li, step_name, code_as_policies)
                            if _break:
                                break
                            code_as_policies, corr_rounds = failure_reasoning(step_name, li, corr_rounds)
                        except WrongDetection:
                            code_as_policies = detection_error_handler(localss, locals, corr_rounds, li, step_name, code_as_policies)
                        except PlanningError as pe:
                            raise PlanningError(pe)
                        except Exception as e:
                            print(e)
                            _break = other_exception_handler(localss, locals, corr_rounds, li, step_name)
                            if _break:
                                break
                            code_as_policies, corr_rounds = failure_reasoning(step_name, li, corr_rounds)
                    add_to_log(f'********************Success! "{parsed_step_name}" is fulfilled !!!!!********************', also_print=True)
                    print('# of corrections: ',corr_rounds)
                    save_task_info(locals, step_name, corr_rounds, li)
                    _, task_feature, _ = get_detected_feature()
                    plan_features[parsed_step_name] = task_feature
                    obj_state = update_object_state(obj_state, parsed_step_name)
                    print(obj_state)
                plan_success = True
                delete_file(HISTORY_TMP_PATH)
            except PlanningError:
                _, task_feature, _ = get_detected_feature()
                plan_features[parsed_step_name] = task_feature
                code_step, obj_state = replan(corr_rounds, li, step_name, code_step, obj_state, plan_features)
        add_to_log("---------------------------Ready to move to next instruction...---------------------------", also_print=True)

# ------------------------------------------ LLM reasoning functions ------------------------------------------

def retrieve_plan_info(li):
    if not os.path.exists('cache/task_history'):
        os.makedirs('cache/task_history')
    if not os.path.exists('cache/task_history/constraints.pkl'):
        with open('cache/task_history/constraints.pkl', 'wb') as file:
            pickle.dump({}, file)
    all_constraints_dict = pickle.load(open('cache/task_history/constraints.pkl',"rb"))
    if all_constraints_dict != {}:
        v = all_constraints_dict[list(all_constraints_dict.keys())[0]]
        if type(v) == str:
            has_image=False
        else:
            has_image=True
        if has_image:
            all_constraints_list = [v[0] for _,v in all_constraints_dict.items()]
            all_image_features_list = [v[1] for _,v in all_constraints_dict.items()]
            all_constraints_str = ''
            for k,v in all_constraints_dict.items():
                all_constraints_str = all_constraints_str + f'{k}. {v[0]}'
                if k != list(all_constraints_dict.keys())[-1]:
                    all_constraints_str += '\n'
            with open(prompt_hl_retrieval, "r") as template_file:
                template_content = template_file.read()
            values = {"instruction": li, "constraints": all_constraints_str}
            prompt = template_content.format(**values)
            response = query_LLM(prompt, [], 'cache/llm_retrieve_plan_info.pkl')
            constraint_index_list = eval(response.text)
            print(f'Planning-level retrieval: Retrieving these constraints: {[all_constraints_list[i-1] for i in constraint_index_list]}')
            return [all_constraints_list[i-1] for i in constraint_index_list], [all_image_features_list[i-1] for i in constraint_index_list]
        else:
            all_constraints_list = [v for _,v in all_constraints_dict.items()]
            all_constraints_str = ''
            for k,v in all_constraints_dict.items():
                all_constraints_str = all_constraints_str + f'{k}. {v}'
                if k != list(all_constraints_dict.keys())[-1]:
                    all_constraints_str += '\n'
            with open(prompt_hl_retrieval, "r") as template_file:
                template_content = template_file.read()
            values = {"instruction": li, "constraints": all_constraints_str}
            prompt = template_content.format(**values)
            print(prompt)
            response = query_LLM(prompt, [], 'cache/llm_retrieve_plan_info.pkl')
            print(response.text)
            constraint_index_list = eval(response.text)
            print(f'Planning-level retrieval: Retrieving these constraints: {[all_constraints_list[i-1] for i in constraint_index_list]}')
            return ([all_constraints_list[i-1] for i in constraint_index_list], None)
    else:
        print('Planning-level retrieval: not retrieving anything...')
        return ([], None)

def ground_plan(raw_plan_step, plan_related_info, obj_dict, obj_state, image_features):
    grounded_plan = {}
    task_features = []
    for num, step in raw_plan_step.items():
        if isinstance(step, str):
            grounded_plan[num] = step
            task_features.append(None)
        else:
            if len(plan_related_info) == 1:
                raw_step, _ = step
                grounded_plan[num] = replace_step_with_true_obj_name(raw_step,plan_related_info[0])
                task_features.append(None)
                print(f"Replacing '{raw_step}' with '{grounded_plan[num]}'.")
            else:
                raw_step, _ = step
                task_feature = get_task_detection(raw_step)
                sims = []
                for feature in image_features:
                    sims.append(((feature.squeeze()-task_feature.squeeze())**2).sum())
                prob_order = np.argsort(np.array(sims))
                retrieved_idx = prob_order[0]
                grounded_plan[num] = replace_step_with_true_obj_name(raw_step,plan_related_info[retrieved_idx])
                task_features.append(task_feature)
                print(f"Replacing '{raw_step}' with '{grounded_plan[num]}'.")
    prompt = prompt_parse_plan + '\n' + f'Plan: {grounded_plan}' + '\n' + f'Object state: {obj_state}' + '\n' + 'Output: '
    response = query_LLM(prompt, ["Plan: "], "cache/llm_parse_plan.pkl")
    grounded_plan = json.loads(response.text)
    return grounded_plan, task_features

def replace_step_with_true_obj_name(step, true_object_name):
    prompt = prompt_replace_true_name + '\n' + f'Step name: {step}' + '\n' + f'Object name: {true_object_name}' + '\n' + 'Output: '
    response = query_LLM(prompt, [], 'cache/llm_replace_true_name.pkl')
    grounded_step = response.text
    return grounded_step 

def is_plan_error(step_name, li, corr_rounds):
    tmp = load_file(HISTORY_TMP_PATH)
    correction = tmp[li][step_name][f'Correction {corr_rounds}']
    prompt = prompt_is_planning_error + '\n\n' + f'Task: {step_name}' + '\n' +f'Correction: {correction}' + '\n' + 'Output: '
    response = query_LLM(prompt, [], 'cache/llm_is_planning_error.pkl')
    add_to_log(prompt)
    add_to_log(response.text)
    if 'yes' in response.text.lower():
        is_planning_error = True
    elif 'no' in response.text.lower():
        is_planning_error = False
    return is_planning_error

def failure_reasoning(step_name, li, corr_rounds):
    tmp = load_file(HISTORY_TMP_PATH)
    _is_plan_error = is_plan_error(step_name, li, corr_rounds)
    if _is_plan_error:
        add_to_log("**Error Type: Planning error", also_print=True)
        raise PlanningError('')
    prompt = retrieve_interaction_history(step_name, li, corr_rounds)
    
    response = query_LLM(prompt, ["Outcome:", "# Outcome:", "# Task:"], 'cache/llm_response_correction.pkl')
    _, correction_code = format_code(response)
    error_type = "Don't know"

    add_to_log("**Error Type: " + error_type, also_print=True)
    add_to_log("**Response Code: " + correction_code)
    tmp = load_file(HISTORY_TMP_PATH)
    tmp[li][step_name][f'error type {corr_rounds}'] = error_type
    corr_rounds += 1
    tmp[li][step_name][f'code response {corr_rounds}'] = correction_code
    pickle.dump(tmp, open(HISTORY_TMP_PATH, "wb"))
    return correction_code, corr_rounds

def retrieve_interaction_history(step_name, li, corr_rounds):
    global use_interrupted_code
    tmp = load_file(HISTORY_TMP_PATH)
    correction = tmp[li][step_name][f'Correction {corr_rounds}']
    prompt_correction = replace_brackets_in_file(prompt_correction_file, step_name)
    history_prompt = ''
    history_prompt = history_prompt + 'Response 1:' + '\n' + "'''" + '\n' + tmp[li][step_name]['code response 0'] + '\n' + "'''"
    history_prompt = history_prompt + '\n' + 'Outcome: ' + tmp[li][step_name]['Outcome 0']
    history_prompt = history_prompt + '\n' + 'Human feedback: ' + correction
    whole_prompt = prompt_correction + '\n' + '\n' + history_prompt
    print(whole_prompt)
    return whole_prompt

def replan(corr_rounds, li, step_name, original_plan, object_state, plan_features):
    parsed_step_name = step_name.lower()[:-1] if step_name[-1] == '.' else step_name.lower()
    tmp = load_file(HISTORY_TMP_PATH)
    correction = tmp[li][step_name][f'Correction {corr_rounds}']
    whole_prompt = prompt_plan_correction + '\n' + '\n' + f"Task: {li}" + "\n" + f"Plan:\n{format_dictionary(original_plan)}" + '\n' + f'Outcome: Interrupted by human at the step "{parsed_step_name}.'
    whole_prompt = whole_prompt + '\n' + 'Correction: ' + correction + '.' + '\n' + f'Object state: {object_state}'
    response = query_LLM(whole_prompt, ["Task:"], "cache/llm_planning_correction.pkl")
    task_constraint = get_lines_starting_with(response.text, 'Task constraint:', first=True).split('Task constraint: ')[1]
    robot_constraint = get_lines_starting_with(response.text, 'Robot constraint:', first=True).split('Robot constraint: ')[1]
    updated_object_state = get_lines_starting_with(response.text, 'Updated object state:', first=True).split('Updated object state: ')[1]
    code_step = format_plan(response, "Modified original plan:")
    add_to_log(code_step, also_print=False)
    print(response.text)
    if 'none' in task_constraint.lower():
        pass
    else:
        task_feature = get_constraint_related_feature(task_constraint, plan_features)
        prompt_plan_instance.add_constraints(task_constraint)
        save_plan_info(task_constraint, task_feature)
    if 'none' in robot_constraint.lower():
        pass
    else:
        prompt_plan_instance.add_constraints(robot_constraint)
        save_plan_info(robot_constraint)
    add_to_log(task_constraint, also_print=False)
    add_to_log(robot_constraint, also_print=False)
    update_plan = f'Instruction: {li}' + '\n' + 'Response:' + '\n' + format_dictionary(code_step)
    prompt_plan_instance.update_content(update_plan)
    prompt_plan_instance.set_object_state(updated_object_state)
    re_plan = format_plan(response, "Replan:")
    add_to_log(re_plan, also_print=False)
    delete_file(HISTORY_TMP_PATH)
    return re_plan, updated_object_state

def update_object_state(obj_state, task_name):
    new_obj_state = _update_object_state(obj_state, task_name)
    prompt_plan_instance.set_object_state(new_obj_state)
    return new_obj_state

def retrieve_task_info(step_name):
    global rel_pos, rel_ori

    previous_tasks = get_previous_tasks()

    if previous_tasks == '':
        print('Skill-level: not retrieving anything...')
        return "# Task-related knowledge: None.", {}
    else:

        with open("prompts/ll_retrieval.txt", "r") as template_file:
            template_content = template_file.read()
        filled_content = template_content.replace("[]", previous_tasks, 1)
        prompt = filled_content.replace("[]", step_name, 1)
        response = query_LLM(prompt, [], 'cache/llm_check_done.pkl')
        add_to_log(prompt,also_print=False)
        add_to_log(response.text, also_print=False)
        response_dict = json.loads(response.text)

        if response_dict["1"] == 'Yes':
            task_visual_feature = get_task_detection(step_name)
            max_diff = 10000
            for task_index in response_dict["2"]:
                assert type(task_index) == int
                info_dict : dict = pickle.load(open(f'cache/task_history/{task_index}.pkl', "rb"))
                vis_feature = info_dict['dino_image_feature']
                diff = np.linalg.norm(vis_feature-task_visual_feature)
                if diff<=max_diff:
                    max_diff = diff
                    ind = task_index
            info_dict : dict = pickle.load(open(f'cache/task_history/{ind}.pkl', "rb"))
            rel_pos = info_dict['relative_pose'][0]
            rel_ori = info_dict['relative_pose'][1]
            other_keys = [key for key in info_dict.keys() if key != 'relative_pose' and key != 'image_feature' and key != 'code' and key !='dino_image_feature']
            ret_str = "# Task-related knowledge: "
            locals = {}
            for i in range(len(other_keys)):
                k = other_keys[i].replace(" ", "_")
                ret_str += k
                locals[k] = info_dict[other_keys[i]]
                if i != len(other_keys)-1:
                    ret_str += ', '
            if ret_str == "# Task-related knowledge: ":
                ret_str += 'None.'
            if 'code' in info_dict.keys():
                update_code = '# Task: ' + step_name + '\n' + ret_str + '\n' + info_dict['code']
                prompt_codepolicy_instance.update_content(update_code)
            print(f'Skill-level: retrieving the knowledge these knowledge "{ret_str}".')
            return ret_str, locals
        else:
            return "# Task-related knowledge: None.", {}

def save_task_info(locals, step_name, corr_rounds, li):
    parsed_step_name = step_name.lower()[:-1] if step_name[-1] == '.' else step_name.lower()
    with open(prompt_get_task_feature_file, "r") as template_file:
        template_content = template_file.read()
    prompt_get_task = template_content.replace("{}", parsed_step_name, 1)
    response = query_LLM(prompt_get_task, ['The information'], "cache/llm_get_task_feature.pkl")
    _, task_related_info = format_code(response)
    tmp = load_file(HISTORY_TMP_PATH)
    history_prompt = "Interaction history:" + '\n' + "'''" + '\n'
    history_prompt = history_prompt + f'Task: {step_name}' + '\n'
    history_prompt = history_prompt + f'Task-related knowledge: {task_related_info}' + '\n' + '\n'
    history_prompt = history_prompt + 'Response 1:' + '\n' + "'''" + '\n' + tmp[li][step_name]['code response 0'] + '\n' + "'''"
    history_prompt = history_prompt + '\n' + 'Outcome: ' + tmp[li][step_name]['Outcome 0']
    history_prompt = history_prompt + '\n' + 'Human feedback: ' + tmp[li][step_name]['Correction 0']
    for i in range(1, corr_rounds+1):
        history_prompt = history_prompt + '\n' + '\n'
        history_prompt = history_prompt + f'Response to feedback {i+1}:' + '\n' + "'''" + '\n' + tmp[li][step_name][f'code response {i}'] + '\n' + "'''"
        history_prompt = history_prompt + '\n' + 'Outcome: ' + tmp[li][step_name][f'Outcome {i}']
        history_prompt = history_prompt + '\n' + 'Human feedback: ' + tmp[li][step_name][f'Correction {i}']
    history_prompt = prompt_saveinfo + '\n' + '\n' + history_prompt + '\n' + "'''" + '\n' + '\n' + 'Your response:' + '\n'
    response = query_LLM(history_prompt, ['Your response:', 'Interaction'], "cache/llm_response_save_info.pkl")
    codes = get_lines_starting_with(response.text, 'save_information', first=False)
    _, code_as_policies = format_code(response)
    if code_as_policies == response.text:
        has_code = False
    else:
        has_code = True
        add_to_log(code_as_policies)
    add_to_log('-'*80)
    add_to_log('*at save_info*')
    add_to_log(history_prompt, also_print=False)
    add_to_log(response.text, also_print=True)
    add_to_log(locals)
    add_to_log('-'*80)
    localss = locals.copy()
    try:
        for code in codes:
            try:
                int(code[-3])
                locals = localss[int(code[-3:-1])-1]
            except:
                locals = localss[int(code[-2])-1]
            exec(code, globals(), locals)
        info_dict : dict = pickle.load(open('cache/tmp_info.pkl', "rb"))
        if has_code:
            info_dict['code'] = code_as_policies
        keys_with_ori = [key for key in info_dict.keys() if 'ori' in key]
        real_ori = info_dict[keys_with_ori[0]]
        keys_with_pos = [key for key in info_dict.keys() if 'pos' in key]
        real_pos = info_dict[keys_with_pos[0]]
        image_feature, dino_image_feature, detected_pose = get_detected_feature()
        d_pos, d_ori = detected_pose
        assert len(d_ori) == 4
        assert len(real_ori) == 4
        relative_pose = calculate_relative_pose((real_pos,real_ori), (d_pos, d_ori), is_quat=True)
        info_to_save = ({'relative_pose': relative_pose,'image_feature': image_feature, 'dino_image_feature': dino_image_feature})
        other_keys = [key for key in info_dict.keys() if 'ori' not in key and 'pos' not in key]
        for key in other_keys:
            info_to_save[key] = info_dict[key]
        save_information_perm(info_to_save)
        task_related_info = re.sub(r'\b\w+(_pos|_ori)\b', '', task_related_info)
        task_related_info = re.sub(r',+', ',', task_related_info)
        task_related_info = task_related_info.strip(', ')
    except:
        pass
    execute_post_action(step_name)
    delete_file('cache/tmp_info.pkl')

def get_constraint_related_feature(constraint, plan_features):
    task_list = ''
    idx = 1
    for k, _ in plan_features.items():
        task_list += str(idx) + '. ' + k + '.'
        idx += 1
    prompt = prompt_get_constraint_feature + '\n\n' + f'Task list: {task_list}' + '\n' + f'Query: {constraint}' + '\n' + 'Response: '
    response = query_LLM(prompt, ["Task:"], "cache/llm_get_constraint_feature.pkl")
    print('****')
    print(prompt)
    print(response.text)
    vis_idx = eval(response.text)[0]
    return plan_features[list(plan_features.keys())[vis_idx-1]]

if __name__ == '__main__':
    global policy, realrobot
    def delete_tmp():
        delete_file(HISTORY_TMP_PATH)
        # with open('new_codeprompt.txt', 'w') as file:
        #     file.write(prompt_codepolicy_instance.get_prompt())
        # with open('new_planprompt.txt', 'w') as file:
        #     file.write(prompt_plan_instance.get_prompt())
    atexit.register(delete_tmp)
    parser = argparse.ArgumentParser()
    parser.add_argument("--task", type=str, default='drawer')
    parser.add_argument("--realrobot", type=bool, default=False)
    args = parser.parse_args()
    task = args.task
    realrobot = args.realrobot
    set_policy_and_task(realrobot, task)
    if realrobot:
        from utils.robot.robot_policy import KptPrimitivePolicy
        policy = KptPrimitivePolicy()
    else:
        from utils.robot.dummy_policy import DummyPolicy
        policy = DummyPolicy()
    if os.path.exists(f"log/{task}/"):
        pass
    else:
        os.makedirs(f"log/{task}/")
    main()


File: scripts/script.py

import numpy as np
import os, pickle, atexit, argparse, json, re
import warnings
from collections import defaultdict
from utils.modulable_prompt import modulable_prompt
from utils.string_utils import extract_array_from_str, replace_description_with_value, replace_code_with_no_description, replace_strarray_with_str, \
                            format_plan, format_code, get_lines_starting_with, format_dictionary, replace_brackets_in_file
from utils.perception.perception_utils import _parse_pos, _correct_past_detection, _get_task_pose, _get_task_detection, _change_reference_frame, get_detected_feature, compare_feature, \
                                    get_initial_state, initialize_detection, get_considered_classes, get_objs, _update_object_state, compare_text_image_sim, test_image, clear_saved_detected_obj, set_policy_and_task, set_saved_detected_obj
from utils.io.io_utils import read_py, HISTORY_TMP_PATH, add_to_log, load_file, USER_LOG, delete_file, get_previous_tasks, save_information_perm, save_information, save_plan_info
from utils.LLM_utils import query_LLM
from utils.transformation_utils import get_real_pose, calculate_relative_pose
from utils.exception_utils import InterruptedByHuman, GraspError, RobotError, PlanningError, WrongDetection, interruption_handler, robot_error_handler, grasp_error_handler, detection_error_handler, no_exception_handler, other_exception_handler

prompt_plan_instance = modulable_prompt('prompts/hl_backbone.txt', 'prompts/hl_content.txt')
prompt_codepolicy_instance = modulable_prompt('prompts/ll_backbone.txt', 'prompts/ll_content.txt')
prompt_hl_retrieval = 'prompts/hl_retrieval.txt'
prompt_parse_ori = read_py('prompts/parse_ori.py')
prompt_parse_plan = read_py('prompts/parse_plan.txt')
prompt_replace_true_name = read_py('prompts/replace_true_name.txt')
prompt_is_planning_error = read_py('prompts/is_planning_error.txt')
prompt_correction_file = 'prompts/ll_corr.txt'
prompt_correction_no_history = read_py('prompts/ll_corr_nohist.txt')
prompt_retrieve = read_py('prompts/hist_retrieval.txt')
prompt_plan_correction = read_py('prompts/hl_corr.txt')
prompt_saveinfo = read_py('prompts/ll_distill.txt')
prompt_get_task_feature_file = 'prompts/get_task_feature.txt'
prompt_get_constraint_feature = read_py('prompts/get_constraint_feature.txt')
rel_pos, rel_ori, gripper_opened = None, None, False

# ------------------------------------------ Primitives ------------------------------------------

def get_current_state():
    if realrobot:
        pose = policy.robot_env.robot.get_ee_pose()
        ee_pos = pose[0].numpy()
        ee_ori = pose[1].numpy()
        return (ee_pos, ee_ori)
    else:
        return (np.array((1.,0.,0.)), np.array((1.,0.,0.,0.)))

def get_horizontal_ori(): 
    return policy.get_horizontal_ori()

def get_vertical_ori():
    return policy.get_vertical_ori()

def open_gripper(width=1):
    global gripper_opened
    if type(width) is not int and type(width) is not float:
        width = 1
    gripper_opened = True
    policy.open_gripper(width)

def close_gripper(width=None):
    global gripper_opened
    if gripper_opened:
        check_grasp = True
    else:
        check_grasp = False
    if width is None:
        policy.close_gripper(check_grasp=check_grasp)
    else:
        if type(width) is not int and type(width) is not float:
            width = 1
        policy.open_gripper(width)

def get_ori(degrees, axis):
    return policy.rotate_gripper(degrees, axis)

def move_gripper_to_pose(pos, rot):
    return policy.move_to_pos(pos, rot)

def parse_pos(pos_description, reference_frame='object'):
    global corr_rounds
    reference_frame = str(reference_frame)
    if reference_frame != "object" and reference_frame != "absolute":
        reference_frame = 'absolute'
    numpy_array = extract_array_from_str(pos_description)
    if numpy_array is None:
        ret_val, code_as_policies = _parse_pos(pos_description, reference_frame)
        if not any(char.isdigit() for char in pos_description):
            new_pos_description = replace_description_with_value(code_as_policies, pos_description)
            replace_code_with_no_description(new_pos_description, corr_rounds)
        return ret_val
    else:
        current_pos, _ = get_current_state()
        replace = False
        if np.linalg.norm(current_pos-numpy_array) < 0.008:
            pos_description = replace_strarray_with_str(pos_description, "current position")
            replace = True
        ret_val, code_as_policies = _parse_pos(pos_description, reference_frame)
        if replace:
            if not any(char.isdigit() for char in pos_description):
                new_pos_description = replace_description_with_value(code_as_policies, pos_description)
                replace_code_with_no_description(new_pos_description, corr_rounds)
        else:
            if not any(char.isdigit() for char in pos_description.split('[')[0]):
                new_pos_description = replace_description_with_value(code_as_policies, pos_description)
                replace_code_with_no_description(new_pos_description, corr_rounds)
        return ret_val

def parse_ori(ori_description):
    ori_description = ori_description.split(' relative')[0]
    whole_prompt = prompt_parse_ori + '\n' + '\n' + f"# Query: {ori_description}" + "\n"
    response = query_LLM(whole_prompt, ["# Query:"], "cache/llm_response_parse_ori.pkl")
    code_as_policies = response.text
    add_to_log('-'*80)
    add_to_log('*at parse_ori*')
    add_to_log(code_as_policies)
    add_to_log('-'*80)
    localss = {}
    exec(code_as_policies, globals(), localss)
    return localss['ret_val']

def reset_to_default_pose():
    return policy.reset()

def get_task_pose(task):
    global rel_pos, rel_ori
    if rel_pos is None:
        pos, ori =  _get_task_pose(task, visualize=True)
        return pos, ori
    else:
        pos, ori =  _get_task_pose(task, visualize=False)
        assert len(ori) == 4
        real_pos, real_ori = get_real_pose(pos, ori, rel_pos, rel_ori)
        rel_pos, rel_ori = None, None
        return real_pos, real_ori

def get_task_detection(task):
    return _get_task_detection(task)

def change_reference_frame(wrong_direction, correct_direction):
    _change_reference_frame(wrong_direction, correct_direction)

def execute_post_action(step_name):
    if "open" in step_name.lower() or "close" in step_name.lower():
        current_pos, current_ori = get_current_state()
        target_pos = parse_pos(f"a point 8cm back to {current_pos}.", reference_frame='object')
        move_gripper_to_pose(target_pos, current_ori)
        reset_to_default_pose()
    if "put" in step_name.lower():
        current_pos, current_ori = get_current_state()
        target_pos = parse_pos(f"a point 60cm above {current_pos}.", reference_frame='absolute')
        move_gripper_to_pose(target_pos, current_ori)
        reset_to_default_pose()
    if "pick" in step_name.lower():
        current_pos, current_ori = get_current_state()
        target_pos = parse_pos(f"a point 60cm above {current_pos}.", reference_frame='absolute')
        move_gripper_to_pose(target_pos, current_ori)

# ------------------------------------------ Main function ------------------------------------------

def main(args):
    global corr_rounds, use_interrupted_code, gripper_opened

    # Infinite outmost loop
    while True:

        li = input('\n\n\n' + "I'm ready to take instruction." + '\n' + 'Input your instruction:')

        # Get initial objects and their states
        initialize_detection(first=True, load_image=args.load_image)
        obj_state = get_initial_state()
        obj_dict = get_objs()
        print(obj_state)

        # Retrieve plan-related knowledge and add it to the prompt
        plan_related_info, image_features = retrieve_plan_info(li)
        prompt_plan_instance.set_object_state(obj_state)
        prompt_plan_instance.add_constraints(plan_related_info)

        # Generate initial plans (which could only be wrong in not grounding objects)
        prompt_plan = prompt_plan_instance.get_prompt()
        whole_prompt = prompt_plan + '\n' + '\n' + f"Instruction: {li}" + "\n"
        response = query_LLM(whole_prompt, ["Instruction:"], "cache/llm_response_planning_high.pkl")
        raw_code_step = format_plan(response)
        print('***raw plan***')
        print(raw_code_step)

        # Ground the plan to true objects, as well as determine the which one is the true object
        code_step, task_features = ground_plan(raw_code_step, plan_related_info, obj_dict, obj_state, image_features)
        plan_success = False

        # Loop for handling plan failures
        while not plan_success:
            try:
                print('***grounded plan***')
                print(code_step)
                add_to_log(prompt_plan_instance.get_prompt(), also_print=False)
                add_to_log(code_step, also_print=False)
                tmp = load_file(HISTORY_TMP_PATH)
                tmp[li] = {}
                clear_saved_detected_obj()
                plan_features = {}

                # Loop for executing each sub-step
                for num, step_name in code_step.items():
                    parsed_step_name = step_name.lower()[:-1] if step_name[-1] == '.' else step_name.lower()
                    add_to_log("****Step name: " + parsed_step_name + '****', file_path=USER_LOG)
                    add_to_log(f"I am performing the task: {parsed_step_name}.", also_print=True)
                    use_interrupted_code, gripper_opened, corr_rounds = False, False, 0
                    locals = defaultdict(dict)
                    initialize_detection()

                    # Retrieve relavant task info and generate code
                    task_feature = task_features[int(num)-1]
                    if task_feature is not None:
                        set_saved_detected_obj(step_name, task_feature)
                    task_related_knowledge, localss = retrieve_task_info(step_name)
                    prompt_codepolicy = prompt_codepolicy_instance.get_prompt()
                    whole_prompt = prompt_codepolicy + '\n' + '\n' + f"# Task: {step_name}" + "\n" + task_related_knowledge + "\n"
                    response = query_LLM(whole_prompt, ["# Task:", "# Outcome:"], "cache/llm_response_planning_low.pkl")
                    _, code_as_policies = format_code(response)

                    # Add info to history temporary file for later interaction history retrieval
                    tmp[li][step_name] = {}
                    tmp[li][step_name]['code response 0'] = code_as_policies
                    add_to_log('-'*80 + '\n' + '*whole prompt*' + '\n' + whole_prompt)
                    add_to_log(f"# Task: {step_name}" + "\n" + task_related_knowledge + "\n" + '-'*80)
                    add_to_log('-'*80 + '\n' + '*original code*' + '\n' + code_as_policies + '\n' + '-'*80)
                    pickle.dump(tmp, open(HISTORY_TMP_PATH, "wb"))

                    # Main loop for code execution, correction and code regeneration
                    while True:
                        try:
                            add_to_log('-'*80 + '\n' + code_as_policies + '\n' + '-'*80, also_print=True)
                            exec(code_as_policies, globals(), localss)
                            _break, corr_rounds, code_as_policies, use_interrupted_code = no_exception_handler(localss, locals, corr_rounds, li, step_name, failure_reasoning, use_interrupted_code, code_as_policies)
                            if _break:
                                break
                        except InterruptedByHuman:
                            _break = interruption_handler(localss, locals, corr_rounds, li, step_name, code_as_policies)
                            if _break:
                                break
                            code_as_policies, corr_rounds = failure_reasoning(step_name, li, corr_rounds)
                        except RobotError:
                            _break = robot_error_handler(localss, locals, corr_rounds, li, step_name, code_as_policies)
                            if _break:
                                break
                            code_as_policies, corr_rounds = failure_reasoning(step_name, li, corr_rounds)
                        except GraspError:
                            open_gripper(width=1)
                            _break = grasp_error_handler(localss, locals, corr_rounds, li, step_name, code_as_policies)
                            if _break:
                                break
                            code_as_policies, corr_rounds = failure_reasoning(step_name, li, corr_rounds)
                        except WrongDetection:
                            code_as_policies = detection_error_handler(localss, locals, corr_rounds, li, step_name, code_as_policies)
                        except PlanningError as pe:
                            raise PlanningError(pe)
                        except Exception as e:
                            print(e)
                            _break = other_exception_handler(localss, locals, corr_rounds, li, step_name)
                            if _break:
                                break
                            code_as_policies, corr_rounds = failure_reasoning(step_name, li, corr_rounds)
                    add_to_log(f'********************Success! "{parsed_step_name}" is fulfilled !!!!!********************', also_print=True)
                    print('# of corrections: ',corr_rounds)
                    save_task_info(locals, step_name, corr_rounds, li)
                    _, task_feature, _ = get_detected_feature()
                    plan_features[parsed_step_name] = task_feature
                    obj_state = update_object_state(obj_state, parsed_step_name)
                    print(obj_state)
                plan_success = True
                delete_file(HISTORY_TMP_PATH)
            except PlanningError:
                _, task_feature, _ = get_detected_feature()
                plan_features[parsed_step_name] = task_feature
                code_step, obj_state = replan(corr_rounds, li, step_name, code_step, obj_state, plan_features)
        add_to_log("---------------------------Ready to move to next instruction...---------------------------", also_print=True)

# ------------------------------------------ LLM reasoning functions ------------------------------------------

def failure_reasoning(step_name, li, corr_rounds):
    tmp = load_file(HISTORY_TMP_PATH)
    _is_plan_error = is_plan_error(step_name, li, corr_rounds)
    if _is_plan_error:
        add_to_log("**Error Type: Planning error", also_print=True)
        raise PlanningError('')
    history_type, prompt_or_code = retrieve_interaction_history(step_name, li, corr_rounds)
    if history_type == 1:
        response = query_LLM(prompt_or_code, ["Outcome:", "# Outcome:", "# Task:"], 'cache/llm_response_correction.pkl')
        _, correction_code = format_code(response)
        error_type = "Position Inaccuracy"
    elif history_type == 2:
        correction_code = prompt_or_code
        error_type = 'None'
    elif history_type == 3:
        response = query_LLM(prompt_or_code, ["Outcome:", "# Outcome:", "# Task:"], 'cache/llm_response_correction.pkl')
        _, correction_code = format_code(response)
        error_type = "Position Inaccuracy"
    elif history_type == 4:
        response = query_LLM(prompt_or_code, ["Outcome:", "# Outcome:", "# Task:"], 'cache/llm_response_correction.pkl')
        _, correction_code = format_code(response)
        error_type = "Position Inaccuracy"
    add_to_log("**Error Type: " + error_type, also_print=True)
    add_to_log("**Response Code: " + correction_code)
    tmp = load_file(HISTORY_TMP_PATH)
    tmp[li][step_name][f'error type {corr_rounds}'] = error_type
    corr_rounds += 1
    tmp[li][step_name][f'code response {corr_rounds}'] = correction_code
    pickle.dump(tmp, open(HISTORY_TMP_PATH, "wb"))
    return correction_code, corr_rounds

def retrieve_plan_info(li):
    if not os.path.exists('cache/task_history'):
        os.makedirs('cache/task_history')
    if not os.path.exists('cache/task_history/constraints.pkl'):
        with open('cache/task_history/constraints.pkl', 'wb') as file:
            pickle.dump({}, file)
    all_constraints_dict = pickle.load(open('cache/task_history/constraints.pkl',"rb"))
    if all_constraints_dict != {}:
        v = all_constraints_dict[list(all_constraints_dict.keys())[0]]
        if type(v) == str:
            has_image=False
        else:
            has_image=True
        if has_image:
            all_constraints_list = [v[0] for _,v in all_constraints_dict.items()]
            all_image_features_list = [v[1] for _,v in all_constraints_dict.items()]
            all_constraints_str = ''
            for k,v in all_constraints_dict.items():
                all_constraints_str = all_constraints_str + f'{k}. {v[0]}'
                if k != list(all_constraints_dict.keys())[-1]:
                    all_constraints_str += '\n'
            with open(prompt_hl_retrieval, "r") as template_file:
                template_content = template_file.read()
            values = {"instruction": li, "constraints": all_constraints_str}
            prompt = template_content.format(**values)
            response = query_LLM(prompt, [], 'cache/llm_retrieve_plan_info.pkl')
            constraint_index_list = eval(response.text)
            print(f'Planning-level retrieval: Retrieving these constraints: {[all_constraints_list[i-1] for i in constraint_index_list]}')
            return [all_constraints_list[i-1] for i in constraint_index_list], [all_image_features_list[i-1] for i in constraint_index_list]
        else:
            all_constraints_list = [v for _,v in all_constraints_dict.items()]
            all_constraints_str = ''
            for k,v in all_constraints_dict.items():
                all_constraints_str = all_constraints_str + f'{k}. {v}'
                if k != list(all_constraints_dict.keys())[-1]:
                    all_constraints_str += '\n'
            with open(prompt_hl_retrieval, "r") as template_file:
                template_content = template_file.read()
            values = {"instruction": li, "constraints": all_constraints_str}
            prompt = template_content.format(**values)
            print(prompt)
            response = query_LLM(prompt, [], 'cache/llm_retrieve_plan_info.pkl')
            print(response.text)
            constraint_index_list = eval(response.text)
            print(f'Planning-level retrieval: Retrieving these constraints: {[all_constraints_list[i-1] for i in constraint_index_list]}')
            return ([all_constraints_list[i-1] for i in constraint_index_list], None)
    else:
        print('Planning-level retrieval: not retrieving anything...')
        return ([], None)

def ground_plan(raw_plan_step, plan_related_info, obj_dict, obj_state, image_features, threshold=3):
    grounded_plan = {}
    task_features = []
    for num, step in raw_plan_step.items():
        if isinstance(step, str):
            grounded_plan[num] = step
            task_features.append(None)
        else:
            if len(plan_related_info) == 1:
                raw_step, _ = step
                grounded_plan[num] = replace_step_with_true_obj_name(raw_step,plan_related_info[0])
                task_features.append(None)
                print(f"Replacing '{raw_step}' with '{grounded_plan[num]}'.")
            else:
                raw_step, obj_names_dict = step
                # Visual-semantic retrieval
                query_text = list(obj_names_dict.keys())[0]
                query_obj_name = get_query_obj(query_text)
                if query_obj_name is not None:
                    sims = []
                    for feature in image_features:
                        sim = compare_text_image_sim(query_obj_name, feature)
                        sims.append(sim)
                    prob_order = np.argsort(np.array(sims))[::-1]
                    retrieved_idx = prob_order[:threshold]
                    plan_related_info = plan_related_info[retrieved_idx]
                    image_features = image_features[retrieved_idx]
                # Visual-visual retrieval
                task_feature = get_task_detection(raw_step)
                sims = []
                for feature in image_features:
                    sims.append(((feature.squeeze()-task_feature.squeeze())**2).sum())
                prob_order = np.argsort(np.array(sims))
                retrieved_idx = prob_order[0]
                grounded_plan[num] = replace_step_with_true_obj_name(raw_step,plan_related_info[retrieved_idx])
                task_features.append(task_feature)
                print(f"Replacing '{raw_step}' with '{grounded_plan[num]}'.")
    prompt = prompt_parse_plan + '\n' + f'Plan: {grounded_plan}' + '\n' + f'Object state: {obj_state}' + '\n' + 'Output: '
    response = query_LLM(prompt, ["Plan: "], "cache/llm_parse_plan.pkl")
    grounded_plan = json.loads(response.text)
    return grounded_plan, task_features

def replace_step_with_true_obj_name(step, true_object_name):
    prompt = prompt_replace_true_name + '\n' + f'Step name: {step}' + '\n' + f'Object name: {true_object_name}' + '\n' + 'Output: '
    response = query_LLM(prompt, [], 'cache/llm_replace_true_name.pkl')
    grounded_step = response.text
    return grounded_step 

def is_plan_error(step_name, li, corr_rounds):
    tmp = load_file(HISTORY_TMP_PATH)
    correction = tmp[li][step_name][f'Correction {corr_rounds}']
    prompt = prompt_is_planning_error + '\n\n' + f'Task: {step_name}' + '\n' +f'Correction: {correction}' + '\n' + 'Output: '
    response = query_LLM(prompt, [], 'cache/llm_is_planning_error.pkl')
    add_to_log(prompt)
    add_to_log(response.text)
    if 'yes' in response.text.lower():
        is_planning_error = True
    elif 'no' in response.text.lower():
        is_planning_error = False
    return is_planning_error

def retrieve_interaction_history(step_name, li, corr_rounds):
    global use_interrupted_code
    tmp = load_file(HISTORY_TMP_PATH)
    correction = tmp[li][step_name][f'Correction {corr_rounds}']
    retrieve_prompt = prompt_retrieve + '\n' + f'"{correction}":'
    response = query_LLM(retrieve_prompt, ['"'], 'cache/llm_retrieve.pkl')
    if len(response.text) >= 4:
        answer = response.text[:4]
    else:
        answer = response.text
    if 'a' in answer:
        history_type = 1
    elif 'b' in answer:
        history_type = 2
    elif 'c' in answer:
        history_type = 3
    elif 'd' in answer:
        history_type = 4
    add_to_log(history_type)
    assert history_type in [1,2,3,4]
    prompt_correction = replace_brackets_in_file(prompt_correction_file, step_name)
    if history_type == 1:
        add_to_log("*** retrieve answer: Last Round Code ***")
        print('History retrival: Last Round Code')
        response_code = tmp[li][step_name][f'code response {corr_rounds}']
        add_to_log(response_code)
        correction_prompt = prompt_correction + '\n' + '\n' + "Last round code:" + '\n' + "'''" + '\n' + response_code + '\n' + "'''"
        correction_prompt = correction_prompt + '\n' + 'Outcome: ' + tmp[li][step_name][f'Outcome {corr_rounds}']
        whole_prompt = correction_prompt + '\n' + 'Human feedback: ' + correction
    elif history_type == 3:
        a = input('Only restarting from beginning can solve this error. Should I restart? (y/n)')
        if 'y' in a.lower():
            raise KeyboardInterrupt
        else:
            add_to_log("*** retrieve answer: Last Round Code ***")
            print('History retrival: Initial Code')
            response_code = tmp[li][step_name][f'code response {corr_rounds}']
            correction_prompt = prompt_correction + '\n' + '\n' + "Last round code:" + '\n' + "'''" + '\n' + response_code + '\n' + "'''"
            correction_prompt = correction_prompt + '\n' + 'Outcome: ' + tmp[li][step_name][f'Outcome {corr_rounds}']
            whole_prompt = correction_prompt + '\n' + 'Human feedback: ' + correction
    elif history_type == 2:
        add_to_log("*** retrieve answer: Initial code ***")
        print('History retrival: Initial code')
        whole_prompt = tmp[li][step_name]['code when interrupted']
        use_interrupted_code = True
    elif history_type == 4:
        add_to_log("*** retrieve answer: No ***")
        print('History retrival: No dependence')
        whole_prompt = prompt_correction_no_history + '\n' + '\n' + f"# Task: {correction}" + "\n"       
    else:
        raise NotImplementedError
    return history_type, whole_prompt

def replan(corr_rounds, li, step_name, original_plan, object_state, plan_features):
    parsed_step_name = step_name.lower()[:-1] if step_name[-1] == '.' else step_name.lower()
    tmp = load_file(HISTORY_TMP_PATH)
    correction = tmp[li][step_name][f'Correction {corr_rounds}']
    whole_prompt = prompt_plan_correction + '\n' + '\n' + f"Task: {li}" + "\n" + f"Plan:\n{format_dictionary(original_plan)}" + '\n' + f'Outcome: Interrupted by human at the step "{parsed_step_name}.'
    whole_prompt = whole_prompt + '\n' + 'Correction: ' + correction + '.' + '\n' + f'Object state: {object_state}'
    response = query_LLM(whole_prompt, ["Task:"], "cache/llm_planning_correction.pkl")
    task_constraint = get_lines_starting_with(response.text, 'Task constraint:', first=True).split('Task constraint: ')[1]
    robot_constraint = get_lines_starting_with(response.text, 'Robot constraint:', first=True).split('Robot constraint: ')[1]
    updated_object_state = get_lines_starting_with(response.text, 'Updated object state:', first=True).split('Updated object state: ')[1]
    code_step = format_plan(response, "Modified original plan:")
    add_to_log(code_step, also_print=False)
    print(response.text)
    if 'none' in task_constraint.lower():
        pass
    else:
        task_feature = get_constraint_related_feature(task_constraint, plan_features)
        prompt_plan_instance.add_constraints(task_constraint)
        save_plan_info(task_constraint, task_feature)
    if 'none' in robot_constraint.lower():
        pass
    else:
        prompt_plan_instance.add_constraints(robot_constraint)
        save_plan_info(robot_constraint)
    add_to_log(task_constraint, also_print=False)
    add_to_log(robot_constraint, also_print=False)
    update_plan = f'Instruction: {li}' + '\n' + 'Response:' + '\n' + format_dictionary(code_step)
    prompt_plan_instance.update_content(update_plan)
    prompt_plan_instance.set_object_state(updated_object_state)
    re_plan = format_plan(response, "Replan:")
    add_to_log(re_plan, also_print=False)
    delete_file(HISTORY_TMP_PATH)
    return re_plan, updated_object_state

def retrieve_task_info(step_name):
    global rel_pos, rel_ori

    previous_tasks = get_previous_tasks()

    if previous_tasks == '':
        print('Skill-level: not retrieving anything...')
        return "# Task-related knowledge: None.", {}
    else:

        with open("prompts/ll_retrieval.txt", "r") as template_file:
            template_content = template_file.read()
        filled_content = template_content.replace("[]", previous_tasks, 1)
        prompt = filled_content.replace("[]", step_name, 1)
        response = query_LLM(prompt, [], 'cache/llm_check_done.pkl')
        add_to_log(prompt,also_print=False)
        add_to_log(response.text, also_print=False)
        response_dict = json.loads(response.text)

        if response_dict["1"] == 'Yes':
            task_visual_feature = get_task_detection(step_name)
            max_diff = 10000
            for task_index in response_dict["2"]:
                assert type(task_index) == int
                info_dict : dict = pickle.load(open(f'cache/task_history/{task_index}.pkl', "rb"))
                vis_feature = info_dict['dino_image_feature']
                diff = np.linalg.norm(vis_feature-task_visual_feature)
                if diff<=max_diff:
                    max_diff = diff
                    ind = task_index
            info_dict : dict = pickle.load(open(f'cache/task_history/{ind}.pkl', "rb"))
            rel_pos = info_dict['relative_pose'][0]
            rel_ori = info_dict['relative_pose'][1]
            other_keys = [key for key in info_dict.keys() if key != 'relative_pose' and key != 'image_feature' and key != 'code' and key !='dino_image_feature']
            ret_str = "# Task-related knowledge: "
            locals = {}
            for i in range(len(other_keys)):
                k = other_keys[i].replace(" ", "_")
                ret_str += k
                locals[k] = info_dict[other_keys[i]]
                if i != len(other_keys)-1:
                    ret_str += ', '
            if ret_str == "# Task-related knowledge: ":
                ret_str += 'None.'
            if 'code' in info_dict.keys():
                update_code = '# Task: ' + step_name + '\n' + ret_str + '\n' + info_dict['code']
                prompt_codepolicy_instance.update_content(update_code)
            print(f'Skill-level: retrieving the knowledge these knowledge "{ret_str}".')
            return ret_str, locals
        else:
            return "# Task-related knowledge: None.", {}

def save_task_info(locals, step_name, corr_rounds, li):
    parsed_step_name = step_name.lower()[:-1] if step_name[-1] == '.' else step_name.lower()
    with open(prompt_get_task_feature_file, "r") as template_file:
        template_content = template_file.read()
    prompt_get_task = template_content.replace("{}", parsed_step_name, 1)
    response = query_LLM(prompt_get_task, ['The information'], "cache/llm_get_task_feature.pkl")
    _, task_related_info = format_code(response)
    tmp = load_file(HISTORY_TMP_PATH)
    history_prompt = "Interaction history:" + '\n' + "'''" + '\n'
    history_prompt = history_prompt + f'Task: {step_name}' + '\n'
    history_prompt = history_prompt + f'Task-related knowledge: {task_related_info}' + '\n' + '\n'
    history_prompt = history_prompt + 'Response 1:' + '\n' + "'''" + '\n' + tmp[li][step_name]['code response 0'] + '\n' + "'''"
    history_prompt = history_prompt + '\n' + 'Outcome: ' + tmp[li][step_name]['Outcome 0']
    history_prompt = history_prompt + '\n' + 'Human feedback: ' + tmp[li][step_name]['Correction 0']
    for i in range(1, corr_rounds+1):
        history_prompt = history_prompt + '\n' + '\n'
        history_prompt = history_prompt + f'Response to feedback {i+1}:' + '\n' + "'''" + '\n' + tmp[li][step_name][f'code response {i}'] + '\n' + "'''"
        history_prompt = history_prompt + '\n' + 'Outcome: ' + tmp[li][step_name][f'Outcome {i}']
        history_prompt = history_prompt + '\n' + 'Human feedback: ' + tmp[li][step_name][f'Correction {i}']
    history_prompt = prompt_saveinfo + '\n' + '\n' + history_prompt + '\n' + "'''" + '\n' + '\n' + 'Your response:' + '\n'
    response = query_LLM(history_prompt, ['Your response:', 'Interaction'], "cache/llm_response_save_info.pkl")
    codes = get_lines_starting_with(response.text, 'save_information', first=False)
    _, code_as_policies = format_code(response)
    if code_as_policies == response.text:
        has_code = False
    else:
        has_code = True
        add_to_log(code_as_policies)
    add_to_log('-'*80)
    add_to_log('*at save_info*')
    add_to_log(history_prompt, also_print=False)
    add_to_log(response.text, also_print=True)
    add_to_log(locals)
    add_to_log('-'*80)
    localss = locals.copy()
    try:
        for code in codes:
            try:
                int(code[-3])
                locals = localss[int(code[-3:-1])-1]
            except:
                locals = localss[int(code[-2])-1]
            exec(code, globals(), locals)
        info_dict : dict = pickle.load(open('cache/tmp_info.pkl', "rb"))
        if has_code:
            info_dict['code'] = code_as_policies
        keys_with_ori = [key for key in info_dict.keys() if 'ori' in key]
        real_ori = info_dict[keys_with_ori[0]]
        keys_with_pos = [key for key in info_dict.keys() if 'pos' in key]
        real_pos = info_dict[keys_with_pos[0]]
        image_feature, dino_image_feature, detected_pose = get_detected_feature()
        d_pos, d_ori = detected_pose
        assert len(d_ori) == 4
        assert len(real_ori) == 4
        relative_pose = calculate_relative_pose((real_pos,real_ori), (d_pos, d_ori), is_quat=True)
        info_to_save = ({'relative_pose': relative_pose,'image_feature': image_feature, 'dino_image_feature': dino_image_feature})
        other_keys = [key for key in info_dict.keys() if 'ori' not in key and 'pos' not in key]
        for key in other_keys:
            info_to_save[key] = info_dict[key]
        save_information_perm(info_to_save)
        task_related_info = re.sub(r'\b\w+(_pos|_ori)\b', '', task_related_info)
        task_related_info = re.sub(r',+', ',', task_related_info)
        task_related_info = task_related_info.strip(', ')
    except:
        pass
    execute_post_action(step_name)
    delete_file('cache/tmp_info.pkl')

def get_constraint_related_feature(constraint, plan_features):
    task_list = ''
    idx = 1
    for k, _ in plan_features.items():
        task_list += str(idx) + '. ' + k + '.'
        idx += 1
    prompt = prompt_get_constraint_feature + '\n\n' + f'Task list: {task_list}' + '\n' + f'Query: {constraint}' + '\n' + 'Response: '
    response = query_LLM(prompt, ["Task:"], "cache/llm_get_constraint_feature.pkl")
    print('****')
    print(prompt)
    print(response.text)
    vis_idx = eval(response.text)[0]
    return plan_features[list(plan_features.keys())[vis_idx-1]]

def get_query_obj(query_text):
    prompt_get_query_obj = read_py('prompts/get_query_obj.txt')
    prompt = prompt_get_query_obj + '\n' + f'Input: {query_text}' + '\n' + 'Output: '
    response = query_LLM(prompt, [], 'cache/llm_get_query_obj.pkl')
    ret_dict = eval(response.text)
    query_obj = ret_dict['object_name_with_visual']
    return query_obj

def update_object_state(obj_state, task_name):
    new_obj_state = _update_object_state(obj_state, task_name)
    prompt_plan_instance.set_object_state(new_obj_state)
    return new_obj_state

if __name__ == '__main__':
    global policy, realrobot
    def delete_tmp():
        delete_file(HISTORY_TMP_PATH)
        # with open('new_codeprompt.txt', 'w') as file:
        #     file.write(prompt_codepolicy_instance.get_prompt())
        # with open('new_planprompt.txt', 'w') as file:
        #     file.write(prompt_plan_instance.get_prompt())
    atexit.register(delete_tmp)
    parser = argparse.ArgumentParser()
    parser.add_argument("--task", type=str, default='drawer')
    parser.add_argument("--realrobot", type=bool, default=False)
    parser.add_argument("--load_image", type=bool, default=False)
    args = parser.parse_args()
    task = args.task
    realrobot = args.realrobot
    set_policy_and_task(realrobot, task)
    if realrobot:
        from utils.robot.robot_policy import KptPrimitivePolicy
        policy = KptPrimitivePolicy()
    else:
        from utils.robot.dummy_policy import DummyPolicy
        policy = DummyPolicy()
    if os.path.exists(f"log/{task}/"):
        pass
    else:
        os.makedirs(f"log/{task}/")
    main(args)


File: setup.py

from setuptools import setup, find_packages


setup(
  name = 'droc',
  packages = find_packages(),
)


File: utils/LLM_utils.py

import os
import openai
import pickle
from collections import defaultdict
from utils.io.io_utils import add_to_log

openai.api_key = 'YOUR_KEY_HERE'


def create_or_load_cache(cache_file):
    if not os.path.exists(os.path.dirname(cache_file)):
        # add_to_log(f"Creating directory for {cache_file}")
        os.makedirs(os.path.dirname(cache_file))

    cache: defaultdict[str, dict] = defaultdict(dict)
    if os.path.exists(cache_file):
        # add_to_log(f"loading cache from {cache_file}")
        cache = pickle.load(open(cache_file, "rb"))
    return cache


def query_LLM(prompt, stop_sequences, cache_file):
    cache = create_or_load_cache(cache_file)
    response = cache[prompt].get("gpt-4", None)
    if response is None:
        success = False
        max_retry = 3
        while not success and max_retry > 0:
            try:
                response = openai.ChatCompletion.create(
                    model='gpt-4',
                    messages=[{'role':'system', 'content':prompt}],
                    temperature=1,
                    stop=stop_sequences,
                    max_tokens=3000
                )
                success = True
            except Exception as e:
                print("Error encountered")
                max_retry -= 1
                if max_retry == 0:
                    raise e
                else:
                    print(e)
        cache[prompt]["gpt-4"] = response
    pickle.dump(cache, open(cache_file, "wb"))
    response.text = response.choices[0].message['content']
    return response



if __name__ == "__main__":
    prompt = 'say hi'
    stop_sequences = []
    cache_file = 'cache/llm_test_correction.pkl'
    response = query_LLM(prompt, stop_sequences, cache_file)
    print(response['usage'])
    print(response.choices[0].message['content'])
    print(response.text)


File: utils/__init__.py




File: utils/exception_utils.py

from utils.io.io_utils import HISTORY_TMP_PATH, USER_LOG, load_file, add_to_log
from utils.string_utils import get_lines_starting_with
import utils.perception.perception_utils as putils
import pickle

class PlanningError(Exception):
    def __init__(self, message):
        super().__init__(message)
        self.message = message


class CodeError(Exception):
    def __init__(self, message):
        super().__init__(message)
        self.message = message


class InterruptedByHuman(Exception):

    def __init__(self, message):
        super().__init__(message)
        self.message = message


class WrongDetection(Exception):
    def __init__(self, message):
        super().__init__(message)
        self.message = message


class GraspError(Exception):
    def __init__(self, message):
        super().__init__(message)
        self.message = message


class RobotError(Exception):
    def __init__(self, message):
        super().__init__(message)
        self.message = message


class NotFinished(Exception):
    def __init__(self, message):
        super().__init__(message)
        self.message = message


def interruption_handler(localss, locals, corr_rounds, li, step_name, code_as_policies):
    for k, v in localss.items():
        locals[corr_rounds][k] = v
    putils.save_current_image('cache/image_for_training', visualize=False)
    tmp = load_file(HISTORY_TMP_PATH)
    correction = input('Please tell me how to correct: ')
    add_to_log(correction, file_path=USER_LOG)
    if 'done' in correction.lower() or 'yes' in correction.lower() or correction.lower() == 'y':
        tmp[li][step_name][f'Outcome {corr_rounds}'] = 'Done.'
        tmp[li][step_name][f'Correction {corr_rounds}'] = 'Done'
        pickle.dump(tmp, open(HISTORY_TMP_PATH, "wb"))
        return True
    current_codeline = get_lines_starting_with(code_as_policies, "move_gripper")
    if 'code when interrupted' not in tmp[li][step_name].keys():
        tmp[li][step_name]['code when interrupted'] = code_as_policies.split(current_codeline)[1]
    add_to_log('current_codeline:' + current_codeline)
    tmp[li][step_name][f'Outcome {corr_rounds}'] = 'Interrupted by human at codeline "' + current_codeline + '".'
    tmp[li][step_name][f'Correction {corr_rounds}'] = correction
    pickle.dump(tmp, open(HISTORY_TMP_PATH, "wb"))
    return False


def robot_error_handler(localss, locals, corr_rounds, li, step_name, code_as_policies):
    for k, v in localss.items():
        locals[corr_rounds][k] = v
    tmp = load_file(HISTORY_TMP_PATH)
    current_codeline = get_lines_starting_with(code_as_policies, "move_gripper")
    add_to_log('current_codeline:' + current_codeline)
    while True:
        correction = input('I cannot reach that tatget. The solution is: 1. Modify your correction; 2. Delete that detection.')
        if '1' in correction:
            correction = input('Please tell me how to correct: ')
            add_to_log(correction, file_path=USER_LOG)
            try:
                tmp[li][step_name][f'Outcome {corr_rounds}'] = tmp[li][step_name][f'Outcome {corr_rounds-1}']
            except:
                tmp[li][step_name][f'Outcome {corr_rounds}'] = 'Interrupted'
            tmp[li][step_name][f'Correction {corr_rounds}'] = correction
            tmp[li][step_name]['code when interrupted'] = code_as_policies.split(current_codeline)[1]
            break
        elif '2' in correction:
            tmp[li][step_name]['code when interrupted'] = code_as_policies.split(current_codeline)[1]
            tmp[li][step_name][f'Outcome {corr_rounds}'] = 'Wrong detection. Interrupted at codeline "' + current_codeline + '".'
            tmp[li][step_name][f'Correction {corr_rounds}'] = "Please correct past detection."
        break
    if 'done' in correction.lower() or 'yes' in correction.lower() or correction.lower() == 'y':
        tmp[li][step_name][f'Outcome {corr_rounds}'] = 'Done.'
        tmp[li][step_name][f'Correction {corr_rounds}'] = 'Done'
        pickle.dump(tmp, open(HISTORY_TMP_PATH, "wb"))
        return True
    pickle.dump(tmp, open(HISTORY_TMP_PATH, "wb"))
    return False


def grasp_error_handler(localss, locals, corr_rounds, li, step_name, code_as_policies):
    for k, v in localss.items():
        locals[corr_rounds][k] = v
    tmp = load_file(HISTORY_TMP_PATH)
    try:
        current_codeline = get_lines_starting_with(code_as_policies, "move_gripper")
        if 'code when interrupted' not in tmp[li][step_name].keys():
            tmp[li][step_name]['code when interrupted'] = code_as_policies.split(current_codeline)[1]
        tmp[li][step_name][f'Outcome {corr_rounds}'] = 'Grasp failure at codeline "' + current_codeline + '".'
    except:
        tmp[li][step_name][f'Outcome {corr_rounds}'] = 'Grasp failure.'
    correction = input('Please tell me how to correct: ')
    add_to_log(correction, file_path=USER_LOG)
    if 'done' in correction.lower() or 'yes' in correction.lower() or correction.lower() == 'y':
        tmp[li][step_name][f'Outcome {corr_rounds}'] = 'Done.'
        tmp[li][step_name][f'Correction {corr_rounds}'] = 'Done'
        pickle.dump(tmp, open(HISTORY_TMP_PATH, "wb"))
        return True
    tmp[li][step_name][f'Correction {corr_rounds}'] = correction
    pickle.dump(tmp, open(HISTORY_TMP_PATH, "wb"))
    return False


def detection_error_handler(localss, locals, corr_rounds, li, step_name, code_as_policies):
    for k, v in localss.items():
        locals[corr_rounds][k] = v
    tmp = load_file(HISTORY_TMP_PATH)
    current_codeline = get_lines_starting_with(code_as_policies, "move_gripper")
    add_to_log('current_codeline:' + current_codeline)
    if "target" in current_codeline or "current" in current_codeline:
        lines = tmp[li][step_name][f'Correction {corr_rounds-1}'].splitlines()
    else:
        tmp[li][step_name]['code when interrupted'] = code_as_policies.split(current_codeline)[1]
        lines = tmp[li][step_name][f'code response 0'].splitlines()                
    obj = putils.parse_obj_name(lines[0])
    if "target" in current_codeline or "current" in current_codeline:
        code_as_policies = f'delete_last_detection("{obj}")'
    else:
        code_as_policies = f'delete_last_detection("{obj}")' + '\n' + tmp[li][step_name]['code response 0']
    print("**Error Type: ", "WrongDetection")
    add_to_log("**Response Code: " + code_as_policies)
    pickle.dump(tmp, open(HISTORY_TMP_PATH, "wb"))
    return code_as_policies


def no_exception_handler(localss, locals, corr_rounds, li, step_name, failure_reasoning, use_interrupted_code, code_as_policies):
    for k, v in localss.items():
        locals[corr_rounds][k] = v
    tmp = load_file(HISTORY_TMP_PATH)
    parsed_step_name = step_name.lower()[:-1] if step_name[-1] == '.' else step_name.lower()
    correction = input(f'Please issue further corrections, or is the task "{parsed_step_name}" done: ')
    # For dummy tests
    if correction == 'wrong detection':
        raise WrongDetection('')
    add_to_log(correction, file_path=USER_LOG)
    if 'done' in correction.lower() or 'yes' in correction.lower() or correction.lower() == 'y':
        if use_interrupted_code:
            tmp[li][step_name][f'Outcome {corr_rounds}'] = 'Done.'
            tmp[li][step_name][f'Correction {corr_rounds}'] = 'Done'
            pickle.dump(tmp, open(HISTORY_TMP_PATH, "wb"))
            return True, corr_rounds, None, use_interrupted_code
        else:
            a = input(f'Are you sure I have finish the task: "{parsed_step_name}" ? (y/n)')
            if 'y' in a.lower():
                tmp[li][step_name][f'Outcome {corr_rounds}'] = 'Done.'
                tmp[li][step_name][f'Correction {corr_rounds}'] = 'Done'
                pickle.dump(tmp, open(HISTORY_TMP_PATH, "wb"))
                return True, corr_rounds, None, use_interrupted_code
            else:
                tmp[li][step_name][f'Outcome {corr_rounds}'] = 'Done.'
                tmp[li][step_name][f'Correction {corr_rounds}'] = 'Please continue from interrupted code'
                correction_code = tmp[li][step_name]['code when interrupted']
                tmp[li][step_name][f'error type {corr_rounds}'] = 'None'
                corr_rounds += 1
                tmp[li][step_name][f'code response {corr_rounds}'] = correction_code
                pickle.dump(tmp, open(HISTORY_TMP_PATH, "wb"))
                code_as_policies = correction_code
                use_interrupted_code = True
    elif correction.lower() == 'no' or correction.lower() == 'n':
        correction = input(f'Please issue further corrections: ')
        add_to_log(correction, file_path=USER_LOG)
        tmp[li][step_name][f'Outcome {corr_rounds}'] = 'Done.'
        tmp[li][step_name][f'Correction {corr_rounds}'] = correction
        pickle.dump(tmp, open(HISTORY_TMP_PATH, "wb"))
        code_as_policies, corr_rounds = failure_reasoning(step_name, li, corr_rounds)
    else:   
        tmp[li][step_name][f'Outcome {corr_rounds}'] = 'Done.'
        tmp[li][step_name][f'Correction {corr_rounds}'] = correction
        try:
            if 'code when interrupted' not in tmp[li][step_name].keys():
                current_codeline = get_lines_starting_with(code_as_policies, "move_gripper")
                tmp[li][step_name]['code when interrupted'] = code_as_policies.split(current_codeline)[1]
        except:
            pass
        pickle.dump(tmp, open(HISTORY_TMP_PATH, "wb"))
        code_as_policies, corr_rounds = failure_reasoning(step_name, li, corr_rounds)
    return False, corr_rounds, code_as_policies, use_interrupted_code


def other_exception_handler(localss, locals, corr_rounds, li, step_name):
    correction = input("I cannot execute that correction. Please give me another correction:")
    for k, v in localss.items():
        locals[corr_rounds][k] = v
    tmp = load_file(HISTORY_TMP_PATH)
    add_to_log(correction, file_path=USER_LOG)
    if 'done' in correction.lower() or 'yes' in correction.lower() or correction.lower() == 'y':
        tmp[li][step_name][f'Outcome {corr_rounds}'] = 'Done.'
        tmp[li][step_name][f'Correction {corr_rounds}'] = 'Done'
        pickle.dump(tmp, open(HISTORY_TMP_PATH, "wb"))
        return True
    tmp[li][step_name][f'Outcome {corr_rounds}'] = 'Done'
    tmp[li][step_name][f'Correction {corr_rounds}'] = correction
    pickle.dump(tmp, open(HISTORY_TMP_PATH, "wb"))
    return False



File: utils/io/io_utils.py

import os
import pickle
from collections import defaultdict
import numpy as np

HISTORY_TMP_PATH = "cache/history_tmp.pkl"
USER_LOG = 'user_log.txt'


def read_py(path_to_py):
    f = open(path_to_py, "r")
    return f.read()


def open_file(filename, mode):
    try:
        file = open(filename, mode)
    except FileNotFoundError:
        create_folder(os.path.dirname(filename))
        file = open(filename, 'w')
        file.close()
        file = open(filename, mode)
    return file


def create_folder(folder_path):
    if not os.path.exists(folder_path):
        os.makedirs(folder_path)
        print("Folder created:", folder_path)
    else:
        print("Folder already exists:", folder_path)


def load_file(cache_file):
    if not os.path.exists(os.path.dirname(cache_file)):
        # add_to_log(f"Creating directory for {cache_file}")
        os.makedirs(os.path.dirname(cache_file))
    cache: defaultdict[str, dict] = defaultdict(dict)
    if os.path.exists(cache_file):
        # add_to_log(f"loading cache from {cache_file}")
        cache = pickle.load(open(cache_file, "rb"))
    return cache


def add_to_log(*text, file_path="log.txt", also_print=False):
    if len(text) == 1:
        text = text[0]
    text = str(text)
    with open(file_path, "a+") as file:
        file.seek(0)
        existing_content = file.read()
        file.seek(0, 2)
        if existing_content:
            file.write("\n" + text)
        else:
            file.write(text)
    if also_print:
        print(text)


def delete_file(file_path):
    try:
        # Use os.remove() to delete the file
        os.remove(file_path)
        add_to_log(f"File '{file_path}' has been deleted successfully.")
    except FileNotFoundError:
        add_to_log(f"File '{file_path}' not found.")
    except PermissionError:
        add_to_log(f"Permission denied to delete '{file_path}'.")
    except Exception as e:
        add_to_log(f"An error occurred: {e}")


def save_information_perm(var_dict):
    history_tmp = load_file(HISTORY_TMP_PATH)
    language_instruction = list(history_tmp.keys())[0]
    step_name = list(history_tmp[language_instruction].keys())[-1]
    try:
        previous_tasks_dict = pickle.load(open('cache/task_history/task_history.pkl','rb'))
        newtask_idx = int(list(previous_tasks_dict.keys())[-1]) + 1
    except:
        previous_tasks_dict = {}
        newtask_idx = 1
    previous_tasks_dict[newtask_idx] = step_name
    pickle.dump(previous_tasks_dict, open('cache/task_history/task_history.pkl', "wb"))
    filename = f'./cache/task_history/{newtask_idx}.pkl'
    for var_name, var_value in var_dict.items():
        var_name = var_name.replace(" ", "_")
        save_info(filename, var_name, var_value)


def save_information(var_name, var_value, inter_round):
    filename = 'cache/tmp_info.pkl'
    if type(var_value) != np.ndarray and type(var_value) != int and type(var_value) != str:
        return
    else:
        if type(var_value) == str:
            if any(char.isdigit() for char in var_value):
                save_info(filename, var_name, var_value)
            else:
                return
        save_info(filename, var_name, var_value)


def save_info(filename, info_name, info_value):
    try:
        open(filename)
        # TODO: rb or wb?
        info_dict: dict = pickle.load(open(filename, "rb"))
        info = {info_name: info_value}
        info_dict.update(info)
        pickle.dump(info_dict, open(filename, "wb"))
    except:
        info = {info_name: info_value}
        pickle.dump(info, open(filename, "wb"))


def get_previous_tasks():
    from utils.string_utils import from_dict_to_str
    if not os.path.exists('cache/task_history'):
        os.makedirs('cache/task_history')
    if not os.path.exists('cache/task_history/task_history.pkl'):
        initial_data = {}
        with open('cache/task_history/task_history.pkl', 'wb') as file:
            pickle.dump(initial_data, file)
    try:
        previous_tasks_dict = pickle.load(open('cache/task_history/task_history.pkl','rb'))
    except:
        initial_data = {}
        with open('cache/task_history/task_history.pkl', 'wb') as file:
            pickle.dump(initial_data, file)
        previous_tasks_dict = pickle.load(open('cache/task_history/task_history.pkl','rb'))
    previous_tasks = from_dict_to_str(previous_tasks_dict)
    return previous_tasks

def save_plan_info(constraint, vis_feature=None):
    try:
        previous_dict = pickle.load(open('cache/task_history/constraints.pkl','rb'))
        newtask_idx = int(list(previous_dict.keys())[-1]) + 1
    except:
        previous_dict = {}
        newtask_idx = 1
    if vis_feature is not None:
        previous_dict[newtask_idx] = (constraint, vis_feature)
    else:
        previous_dict[newtask_idx] = constraint
    pickle.dump(previous_dict, open('cache/task_history/constraints.pkl', "wb"))


File: utils/modulable_prompt.py

from utils.io.io_utils import read_py
from utils.string_utils import str_to_dict, dict_to_str
import re

class modulable_prompt:

    def __init__(self, backbone_path, content_path) -> None:
        self.backbone = read_py(backbone_path)
        self.content_str = read_py(content_path)
        self.content_dict = str_to_dict(self.content_str)
        self.prompt = None
        self.form_prompt()

    def form_prompt(self):
        self.prompt = self.backbone.replace("{}", self.content_str)
    
    def update_content(self, new_content):
        ind = list(self.content_dict.keys())[-1] + 1
        self.content_dict.update({ind: new_content})
        self.content_str = dict_to_str(self.content_dict)
        self.form_prompt()

    def add_constraints(self, constraint):
        if type(constraint) == str:
            constraint = [constraint]
        for c in constraint:
            lines = self.backbone.split('\n')
            rules_start_index = lines.index("Rules:")
            for i, line in enumerate(lines):
                if line.startswith('Object state'):
                    rules_end_index = i - 1
                    break
            existing_rules = "\n".join(lines[rules_start_index + 1 : rules_end_index])
            new_rule_number = len(re.findall(r"\d+\.", existing_rules)) + 1
            new_rule = f"{new_rule_number}. {c}"
            updated_rules_section = f"{existing_rules.strip()}\n{new_rule}"
            self.backbone = self.backbone.replace(existing_rules, updated_rules_section)
            self.form_prompt()

    def get_prompt(self):
        return self.prompt

    def set_object_state(self, obj_state):
        lines = self.backbone.split('\n')
        new_lines = []
        for line in lines:
            if line.startswith("Object state:"):
                new_lines.append(f"Object state: {obj_state}")
            else:
                new_lines.append(line)
        self.backbone = '\n'.join(new_lines)
        self.form_prompt()

if __name__ == '__main__':
    prompt_codepolicy = modulable_prompt('prompts/prompt_plan_backbone.txt', 'prompts/prompt_plan_content.txt')
    prompt_codepolicy.add_constraints('The scissors should be put into a drawer that is not full.')
    print(prompt_codepolicy.get_prompt())
    



File: utils/perception/camera.py

realsense_serial_numbers = {}

class multi_cam:
    def __init__(self) -> None:
        pass

    def take_bgrd():
        pass


File: utils/perception/owl_vit.py

from PIL import Image
import torch
from transformers import OwlViTProcessor, OwlViTForObjectDetection
import imageio.v2 as imageio
import matplotlib.pyplot as plt
from transformers.image_utils import ImageFeatureExtractionMixin
import numpy as np
from segment_anything import SamAutomaticMaskGenerator, sam_model_registry, SamPredictor


def show_mask(mask, ax, random_color=False):
    if random_color:
        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)
    else:
        color = np.array([30/255, 144/255, 255/255, 0.6])
    h, w = mask.shape[-2:]
    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)
    ax.imshow(mask_image)
    
def show_points(coords, labels, ax, marker_size=375):
    pos_points = coords[labels==1]
    neg_points = coords[labels==0]
    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)
    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   
    
def show_box(box, ax):
    x0, y0 = box[0], box[1]
    w, h = box[2] - box[0], box[3] - box[1]
    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))  


def clip_with_owl(image, num_objs, sam, obj_name='drawer', visualize=True):
    if torch.cuda.is_available():
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")

    processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
    model = OwlViTForObjectDetection.from_pretrained("google/owlvit-base-patch32")
    predictor = SamPredictor(sam)

    img = image.copy()
    img = Image.fromarray(np.uint8(img)).convert("RGB")
    text_queries = [f"a photo of a {obj_name}"]
    inputs = processor(text=text_queries, images=img, return_tensors="pt").to(device)
    model = model.to(device)
    model.eval()
    with torch.no_grad():
        outputs = model(**inputs)

    mixin = ImageFeatureExtractionMixin()

    # Load example image
    image_size = model.config.vision_config.image_size
    original_image = img.copy()
    img = mixin.resize(img, image_size)
    input_image = np.asarray(img).astype(np.float32) / 255.0

    # Threshold to eliminate low probability predictions
    score_threshold = 0.01

    # Get prediction logits
    logits = torch.max(outputs["logits"][0], dim=-1)
    scores = torch.sigmoid(logits.values).cpu().detach().numpy()

    # Get prediction labels and boundary boxes
    labels = logits.indices.cpu().detach().numpy()
    boxes = outputs["pred_boxes"][0].cpu().detach().numpy()
    if visualize:
        _, ax = plt.subplots(1, 1, figsize=(8, 8))
        ax.imshow(input_image, extent=(0, 1, 1, 0))
        ax.set_axis_off()

    # s_list = []
    # for i in range(len(scores)):
    #     box = boxes[i]
    #     cx, cy, w, h = box
    #     s_list.append(w*h)
    # s_order = np.argsort(np.array(s_list))[::-1]
    # boxes = boxes[s_order]
    # scores = scores[s_order]
    # labels = labels[s_order]
    # boxes = boxes[:5]
    # scores = scores[:5]
    # labels = labels[:5]

    order = np.argsort(scores)[::-1]
    boxes = boxes[order]
    scores = scores[order]
    labels = labels[order]
    
    img = np.asarray(img)
    original_image = np.asarray(original_image)
    height, width = img.shape[:2]
    old_height, old_width = original_image.shape[:2]
    h_ratio = old_height/height
    w_ratio = old_width/width

    # ret_points = []
    # p3d = point3d.copy()
    # p3d = np.reshape(p3d, (old_height, old_width, -1))
    masks = []
    ret_boxes = []
    for i in range(num_objs):
        label = labels[i]
        score = scores[i]
        box = boxes[i]
        cx, cy, w, h = box
        if visualize:
            ax.plot([cx-w/2, cx+w/2, cx+w/2, cx-w/2, cx-w/2],
                    [cy-h/2, cy-h/2, cy+h/2, cy+h/2, cy-h/2], "r")
            ax.text(
                cx - w / 2,
                cy + h / 2 + 0.015,
                f"{text_queries[label]}: {score:1.2f}",
                ha="left",
                va="top",
                color="red",
                bbox={
                    "facecolor": "white",
                    "edgecolor": "red",
                    "boxstyle": "square,pad=.3"
                })

        cx *= width
        w *= width
        cy *= height
        h *= height

        sam_box = np.array([int((cx-w/2)*w_ratio), int((cy-h/2)*h_ratio), int((cx+w/2)*w_ratio), int((cy+h/2)*h_ratio)])
        predictor.set_image(original_image)
        mask, _, _ = predictor.predict(
            point_coords=None,
            point_labels=None,
            box=sam_box[None, :],
            multimask_output=False,
        )
        masks.append(mask[0].copy())
        ret_boxes.append((int((cx-w/2)*w_ratio), int((cy-h/2)*h_ratio), int(w*w_ratio), int(h*h_ratio)))
        if visualize:
            plt.show()
        
            plt.figure(figsize=(10, 10))
            plt.imshow(original_image)
            show_mask(mask[0], plt.gca())
            show_box(sam_box, plt.gca())
            plt.axis('off')
            plt.show()

        # ret_points.append(p3d[int((cy-h/2)*h_ratio): int((cy+h/2)*h_ratio), int((cx-w/2)*w_ratio): int((cx+w/2)*w_ratio)])

    return masks, ret_boxes


File: utils/perception/perception_utils.py

import pickle, json
import time
import cv2
import os
import numpy as np
from PIL import Image
import open3d as o3d
import shutil
import math
from utils.io.io_utils import open_file, delete_file, add_to_log, read_py
from utils.transformation_utils import extract_z_axis
from utils.LLM_utils import query_LLM
import glob
from utils.perception.camera import realsense_serial_numbers

prompt_parse_pos = 'prompts/parse_pos.py'
prompt_get_task_pose = read_py('prompts/get_task_pose_str.txt')
prompt_get_pose_from_str = read_py('prompts/get_pose_from_str.py')
prompt_parse_name_str = 'prompts/parse_name.txt'
prompt_change_frame_str = 'prompts/change_frame.txt'
prompt_get_pos_scale = read_py('prompts/get_pos_scale.txt')
prompt_get_obj_name_from_task = read_py('prompts/get_obj_name_from_task.txt')
prompt_update_obj_state_file = 'prompts/update_obj_state.txt'
prompt_get_initial_obj_state = read_py('prompts/get_ini_obj_state.txt')

# global variables
load_detected_objs = None
popped_detected_objs = []
saved_detected_obj = {}
queried_obj = None
reference_frame = 'object'
RADIUS = 0.0135
TASK = None
REAL_ROBOT = None
policy = None
clip_model = None

def get_considered_classes():
    if TASK == 'drawer':
        considered_classes = [["drawer", "drawer handle"], ["pen"], ["cup", "mug"], ["tape"], ["scissors", "scissors handle"], ["apple"]]
        other_classes = [["table cloth", "wooden table", "wooden floor", "gray cloth", "gray background", "gray board"],["tripod"], ["unrecoginized object"]]
    elif TASK == 'coffee':
        considered_classes = [["coffee maker", "coffee machine", "keurig coffee maker"], ["red cup", "red mug"], ["coffee capsule", "cone"], ["pink cup", "pink mug"], ["white spoon"]]
        other_classes = [["unrecognizable object"], ["robot arm"], ["table cloth"], ["work surface"], ["tripod"]]
    elif TASK == 'cup':
        considered_classes = [["pink cup"], ["green cup", "blue cup"], ["gray rack", "gray oval object"]]
        other_classes = [["robot arm"], ["table cloth"], ["work surface"], ["tripod"]]
    elif TASK == 'lego':
        considered_classes = [["white drawer", "white drawer handle"], ["red block"], ["yellow block"], ["blue block"]]
        other_classes = [["robot arm"], ["table cloth"], ["work surface"], ["tripod"]]
    else:
        raise NotImplementedError
    return considered_classes, other_classes

class ToScaledFloat:
    """
    Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor, or keep as is if already a tensor.
    """

    def __init__(self) -> None:
        pass

    def __call__(self, pic):
        # assert pic.dtype == torch.uint8, f"{pic.dtype}"
        pic = pic.float() / 255.0
        return pic

    def __repr__(self) -> str:
        return f"{self.__class__.__name__}()"

class DinoImageTransform:
    def __init__(self):
        self.transforms = pt_transforms.Compose(
            [
                ToScaledFloat(),
                pt_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
            ]
        )

    def __call__(self, pic):
        return self.transforms(pic)

class detected_object: 
    def __init__(self):
        self.pcd = None
        self.prob = None
        self.quat = None
        self.position = None
        self.vec = None
        self.clip_feature = None
        self.dino_feature = None
        self.detected_pose = None

# ---------------------------------- Initialization ----------------------------------

def set_policy_and_task(real_robot, task):
    global TASK, REAL_ROBOT, policy
    global torch, open_clip, F, pt_transforms, device
    global SamAutomaticMaskGenerator, sam_model_registry, clip_with_owl
    REAL_ROBOT = real_robot
    TASK = task   
    import torch
    from torchvision import transforms as pt_transforms
    import torch.nn.functional as F
    import open_clip
    from segment_anything import SamAutomaticMaskGenerator, sam_model_registry
    from utils.perception.owl_vit import clip_with_owl
    device = "cuda" if torch.cuda.is_available() else "cpu"    
    if REAL_ROBOT:
        import utils.robot.robot_policy as robot_policy
        policy = robot_policy.KptPrimitivePolicy()
    else:
        from utils.robot.dummy_policy import DummyPolicy
        policy = DummyPolicy()

def initialize_detection(first=False, load_image=False, folder_path='cache/image_for_retrieval', image_pattern='*.png', label=['cup', 'drawer']):
    global load_detected_objs, saved_detected_obj, queried_obj, clip_model, clip_preprocess
    queried_obj = detected_object()
    if REAL_ROBOT:
        if first:
            a = input("Can I use last round detection? (y/n)")
        else:
            a = 'y'
        if 'n' in a:
            detect_objs(load_from_cache=False, save_to_cache=False, visualize=False)
            saved_detected_obj = {}
        load_detected_objs = pickle.load(open(f"log/{TASK}/detected_objs.pkl", "rb"))
    else:
        if first:
            a = input("Can I use last round detection? (y/n)")
        else:
            a = 'y'
        if 'n' in a:
            saved_detected_obj = {}
        if first:
            if load_image:
                # Only consider clip feature here
                if clip_model is None:
                    print("Loading CLIP...")
                    clip_model, _, clip_preprocess = open_clip.create_model_and_transforms('ViT-g-14', pretrained='laion2b_s34b_b88k')
                image_files = glob.glob(os.path.join(folder_path, image_pattern))
                img_dict = {}
                for l in label:
                    img_files = [file for file in image_files if l in os.path.basename(file).lower()]
                    img_files = sorted(img_files, key=lambda x: int(x.split(l)[1].split('.png')[0]))
                    img_dict[l] = img_files
                files = {}
                clip_model.to(device=device)
                for l in label:
                    img_files = img_dict[l]
                    all_img = []
                    _files = []
                    for file in img_files:
                        _files.append(file)
                        img = Image.open(file)
                        img = clip_preprocess(img).to(device)
                        all_img.append(img)
                    preprocessed_images = torch.stack(all_img)    
                    with torch.no_grad(), torch.cuda.amp.autocast():
                        image_features = clip_model.encode_image(preprocessed_images)
                    image_features = image_features.cpu().numpy()
                    img_dict[l] = image_features
                    files[l] = _files
                load_detected_objs = create_loaded_objs(img_dict)
            else:
                load_detected_objs = create_loaded_objs(None)
        else:
            load_detected_objs = pickle.load(open(f"log/{TASK}/detected_objs.pkl", "rb"))

def create_loaded_objs(img_dict=None):
    considered_classes, _ = get_considered_classes()
    load_detected_objs = {}
    for cls in considered_classes:
        if img_dict is None:
            obj_name = cls[0]
            load_detected_objs[obj_name] = [(np.ones((10,3)), 0.5, np.random.randn(1024),np.random.randn(512)), (np.ones((10,3)), 0.3, np.random.randn(1024),np.random.randn(512))]
        else:
            if obj_name not in list(img_dict.keys()):
                n = 3
                load_detected_objs[obj_name] = []
                for i in range(n):
                    load_detected_objs[obj_name].append((np.ones((10,3)), 0.5, np.random.randn(1024),np.random.randn(512)))
            else:
                image_feature = img_dict[obj_name]
                n = len(image_feature)
                load_detected_objs[obj_name] = []
                for i in range(n):
                    load_detected_objs[obj_name].append((np.ones((n+2,3)), 0.5, image_feature[i], np.random.randn(512)))
    pickle.dump(load_detected_objs, open(f"log/{TASK}/detected_objs.pkl", "wb"))
    return load_detected_objs

def get_objs():
    global load_detected_objs
    return load_detected_objs

def get_initial_state():
    global load_detected_objs
    # obj_list_str = ''
    # for obj_name in list(load_detected_objs.keys()):
    #     obj_list_str += obj_name + ', '
    # obj_list_str = obj_list_str[:-2]
    # whole_prompt = prompt_get_initial_obj_state + '\n' + 'Object name: ' + obj_list_str +'\n' + 'Object state:'
    # response = query_LLM(whole_prompt, [], "cache/llm_get_initial_state.pkl")
    # initial_state = response.text
    initial_state = 'Object state: N/A'
    return initial_state

def get_extra_classes():
    if TASK == 'drawer':
        return ["scissors", "scissors handle"], 2
    else:
        return None, None

def setup_clip_words():
    considered_classes, other_classes = get_considered_classes()
    clip_candidates = []
    for word in considered_classes + other_classes:
        clip_candidates.extend(word)
    return clip_candidates

def clear_saved_detected_obj():
    global saved_detected_obj
    saved_detected_obj = {}

def set_saved_detected_obj(task_name, task_feature):
    obj_name = get_obj_name_from_task(task_name)
    saved_detected_obj[obj_name] = task_feature

# ---------------------------------- Primitives ----------------------------------

def _change_reference_frame(wrong_direction, right_direction):
    global queried_obj
    with open(prompt_change_frame_str, "r") as template_file:
        template_content = template_file.read()
    values = {"wrong_direction": wrong_direction, "correct_direction": right_direction}
    prompt = template_content.format(**values)
    response = query_LLM(prompt, ['.'], "cache/llm_change_frame.pkl")
    add_to_log(prompt)
    add_to_log(response.text)
    if "left" in response.text:
        correct_forward_vec = get_directional_vec("left")
    if "right" in response.text:
        correct_forward_vec = get_directional_vec("right")
    if "forward" in response.text:
        correct_forward_vec = get_directional_vec("forward")
    if "back" in response.text:
        correct_forward_vec = get_directional_vec("back")
    queried_obj.vec = correct_forward_vec

def get_directional_vec(direction, *kwargs):
    global queried_obj, reference_frame
    if reference_frame == 'absolute':
        if direction == 'forward' or direction == 'along' or direction == 'towards':
            return np.array((1,0,0))
        elif direction == 'backward' or direction == 'back' or direction == 'away':
            return np.array((-1,0,0))
        elif direction == 'left':
            return np.array((0,1,0))
        elif direction == 'right':
            return np.array((0,-1,0))
        elif direction == 'up' or direction == 'above':
            return np.array((0,0,1))
        elif direction == 'down' or direction == 'downward':
            return np.array((0,0,-1))
        else:
            return np.array((1,0,0))
    else:
        if REAL_ROBOT:
            vec = queried_obj.vec.squeeze()
        else:
            vec = np.array((1.,0.,0.))
        vec = vec/np.linalg.norm(vec)
        if direction == 'forward' or direction == 'along' or direction == 'towards':
            vec = vec if vec[0] >=0 else -vec
            return vec
        elif direction == 'backward' or direction == 'back' or direction == 'away':
            vec = -vec if vec[0] >=0 else vec
            return vec
        elif direction == 'left':
            normal_vec = np.array((-vec[1], vec[0], vec[2])) if vec[0] >=0 else np.array((vec[1], -vec[0], -vec[2]))
            return normal_vec
        elif direction == 'right':
            normal_vec = np.array((-vec[1], vec[0], vec[2])) if vec[0] <=0 else np.array((vec[1], -vec[0], -vec[2]))
            return normal_vec
        elif direction == 'up' or direction == 'above':
            vec = np.array((0,0,1))
            return vec
        elif direction == 'down' or direction == 'downward':
            vec = np.array((0,0,-1))
            return vec
        else:
            return np.array((1.,0.,0.))

def get_current_pos(*kwargs):
    if REAL_ROBOT:
        pose = policy.robot_env.robot.get_ee_pose()
        ee_pos = pose[0].numpy()
        return ee_pos
    else:
        return np.array((1.,0.,0.))

def get_current_ori(*kwargs):
    if REAL_ROBOT:
        pose = policy.robot_env.robot.get_ee_pose()
        ee_ori = pose[1].numpy()
        return ee_ori
    else:
        return np.array((1.,0.,0.,0.))

def get_horizontal_ori(get_vec=False):
    if get_vec == False:
        return policy.get_horizontal_ori()
    else:
        quat = policy.get_horizontal_ori()
        vec = extract_z_axis(quat)
        vec[2] = 0.
        vec = -vec if vec[0]<0 else vec
        vec /= np.linalg.norm(vec)
        return vec
    
def get_vertical_ori(get_vec=False):
    if get_vec==False:
        return policy.get_vertical_ori()
    else:
        return np.array((0.,0.,-1.))

def _correct_past_detection(obj_name, obj):
    global load_detected_objs, popped_detected_objs
    obj_name = parse_obj_name(obj_name)
    for i in range(len(load_detected_objs[obj_name])):
        if (obj.pcd.shape == load_detected_objs[obj_name][i][0].shape):
            popped_elem = load_detected_objs[obj_name].pop(i)
            popped_detected_objs.append(popped_elem)
            break

# ---------------------------------- Parsing ----------------------------------

def get_obj_name_from_task(task_name):
    prompt = prompt_get_obj_name_from_task + '\n' + f'Input: {task_name}' + '\n' + 'Output: '
    response = query_LLM(prompt, [], "cache/llm_parse_task_name.pkl")
    obj_name = response.text
    return obj_name

def parse_obj_name(obj_name):
    considered_classes, _ = get_considered_classes()
    with open(prompt_parse_name_str, 'r') as file:
        template_content = file.read()
    values = {"object_class": considered_classes, "object_name": obj_name}
    prompt = template_content.format(**values)
    response = query_LLM(prompt, [], "cache/llm_parse_obj_name.pkl")
    obj_class_idx = int(response.text)
    parsed_obj_name = considered_classes[obj_class_idx][0]
    return parsed_obj_name

def _parse_pos(pos_description, r_frame='object'):
    global reference_frame
    reference_frame = r_frame
    mapping_dict, little_left, little_up, little_forward, more_back = calculate_pos_scale()
    with open(prompt_parse_pos, "r") as template_file:
        template_content = template_file.read()
    values = {"mapping_dict": mapping_dict, "little_left": little_left, "little_up": little_up, "little_forward": little_forward, "more_back": more_back}
    prompt = template_content.format(**values)
    whole_prompt = prompt + '\n' + '\n' + f"""# Query: {pos_description}.""" + "\n"
    response = query_LLM(whole_prompt, ["# Query:"], "cache/llm_response_parse_pos.pkl")
    code_as_policies = response.text
    add_to_log('-'*80)
    add_to_log('*at parse_pos*')
    add_to_log(code_as_policies)
    add_to_log('-'*80)
    locals = {}
    exec(code_as_policies, globals(), locals)
    return locals['ret_val'], code_as_policies

def calculate_pos_scale():
    global queried_obj
    if REAL_ROBOT:
        whole_prompt = prompt_get_pos_scale + '\n\n' + 'Bounding box: ' + str(queried_obj.bounding_box)
    else:
        whole_prompt = prompt_get_pos_scale + '\n\n' + 'Bounding box: (1, 8, 3)'
    response = query_LLM(whole_prompt, ["Bounding box:"], "cache/llm_get_pos_scale.pkl")
    response_json = response.text
    mapping_dict = json.loads(response_json)
    add_to_log(mapping_dict)
    little_forward = mapping_dict["'A little bit' for 'forward' or 'backward'"]/100
    little_left = mapping_dict["'A little bit' for 'left' or 'right'"]/100
    little_up = mapping_dict["'A little bit' for 'up' or 'down'"]/100
    more_back = mapping_dict["'More' for 'forward' or 'back'"]/100
    return mapping_dict, little_left, little_up, little_forward, more_back

# ---------------------------------- Detection ----------------------------------

def _get_task_detection(task_name):
    """Given task name, find corresponding feature"""
    global saved_detected_obj

    obj_name = get_obj_name_from_task(task_name)

    if obj_name in saved_detected_obj.keys():
        compare_feature(saved_detected_obj[obj_name], obj_name, threshold=1)
        return saved_detected_obj[obj_name]
    
    whole_prompt = prompt_get_pose_from_str + '\n' + "# Query: " + f"""the centroid of "{obj_name}"."""
    response = query_LLM(whole_prompt, ["INPUT:"], "cache/llm_response_get_pose_from_str.pkl")
    code_as_policies = response.text
    add_to_log('-'*80 + '*at get_pose_from_str*' + code_as_policies + '-'*80)
    while True:
        locals = {}
        exec(code_as_policies, globals(), locals)
        obj = locals['ret_val']
        if REAL_ROBOT:
            pcd_merged = o3d.io.read_point_cloud(f"log/{TASK}/pcd_merged.pcd")    
            mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1, origin=[0, 0, 0])
            visualized_geometries = [pcd_merged, mesh_frame]
            sphere = o3d.geometry.TriangleMesh.create_sphere(radius=RADIUS); sphere.paint_uniform_color([0.0, 1.0, 0.0])
            sphere.translate(calculate_centroid(obj.pcd))
            visualized_geometries.append(sphere)
            o3d.visualization.draw_geometries(visualized_geometries)
        correct_detection = input("Is this detection correct? (y/n)")
        if 'y' in correct_detection.lower():
            saved_detected_obj[obj_name] = obj.dino_feature
            compare_feature(obj.dino_feature, obj_name, threshold=1)
            return obj.dino_feature
        else:
            _correct_past_detection(obj_name, obj)

def detect(obj_name, visualize=False):
    global load_detected_objs, saved_detected_obj

    parsed_obj_name = parse_obj_name(obj_name)
    obj_name = parsed_obj_name

    if load_detected_objs[obj_name] == [] or load_detected_objs[obj_name] is None:
        while True:
            a = input("It seems that the correct object was not detected, please make sure to place the drawer in a correct position. Reply 'y' once you finish it.")
            if "y" in a:
                break
        if REAL_ROBOT:
            detect_objs(load_from_cache=False, save_to_cache=False, visualize=visualize)
            load_detected_objs = pickle.load(open(f"log/{TASK}/detected_objs.pkl", "rb"))
        else:
            load_detected_objs = create_loaded_objs()

    all_pcds = []
    probs = []
    clip_features = []
    dino_features = []

    for pcd, prob, clip_feature, dino_feature in load_detected_objs[obj_name]:
        all_pcds.append(pcd)
        probs.append(prob)
        clip_features.append(clip_feature)
        dino_features.append(dino_feature)
    if visualize:
        pcd_merged = o3d.io.read_point_cloud(f"log/{TASK}/pcd_merged.pcd")
        mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1, origin=[0, 0, 0])
        visualized_geometries = [pcd_merged, mesh_frame]
        for pcd in all_pcds:
            sphere = o3d.geometry.TriangleMesh.create_sphere(radius=RADIUS)
            sphere.paint_uniform_color([0.0, 1.0, 0.0])
            sphere.translate(np.mean(pcd,axis=0))
            visualized_geometries.append(sphere)
        o3d.visualization.draw_geometries(visualized_geometries)

    all_obj = detected_object()
    all_obj.pcd = all_pcds
    all_obj.prob = probs
    all_obj.clip_feature = clip_features
    all_obj.dino_feature = dino_features
    return all_obj

def sort_from_high_to_low(all_obj, key):
    all_obj_pos = [np.mean(i, axis=0) for i in all_obj.pcd]
    probs = all_obj.prob
    clip_features = all_obj.clip_feature
    dino_features = all_obj.dino_feature
    if key == 'probability':
        order = np.argsort(np.array(probs))[::-1]
    elif key == 'x':
        x_pos = [x[0] for x in all_obj_pos]
        order = np.argsort(np.array(x_pos))[::-1]
    elif key == 'y':
        y_pos = [x[1] for x in all_obj_pos]
        order = np.argsort(np.array(y_pos))[::-1]
    elif key == 'z':
        z_pos = [x[2] for x in all_obj_pos]
        order = np.argsort(np.array(z_pos))[::-1]
    obj_list = []
    for ind in order:
        obj = detected_object()
        obj.pcd = all_obj.pcd[ind]
        obj.prob = probs[ind]
        obj.clip_feature = clip_features[ind]
        obj.dino_feature = dino_features[ind]
        obj_list.append(obj)
    return obj_list

def detect_objs(load_from_cache=False, run_on_server=False, save_to_cache=True, visualize=False):
    global clip_model, clip_preprocess
    parent_folder = 'cache'
    items = os.listdir(parent_folder)
    for item in items:
        item_path = os.path.join(parent_folder, item)    
        if os.path.isdir(item_path) and (item[0].isdigit() or item == TASK):
            try:
                shutil.rmtree(item_path)
                print(f"Folder '{item_path}' and its contents deleted successfully.")
            except OSError as e:
                print(f"Error deleting '{item_path}': {e}")
    try:
        shutil.rmtree(f'log/{TASK}')
        print(f"Folder 'log/{TASK}' and its contents deleted successfully.")
    except OSError as e:
        print(f"Error deleting '{item_path}': {e}")

    print("Loading CLIP model...")
    init_time = time.time()
    if clip_model is None:
        clip_model, _, clip_preprocess = open_clip.create_model_and_transforms('ViT-g-14', pretrained='laion2b_s34b_b88k')
    print("time taken:", time.time() - init_time)

    print("Loading DINOv2 model...")
    init_time = time.time()
    dinov2_model = torch.hub.load("facebookresearch/dinov2", "dinov2_vits14")
    dinov2_model.train(False)
    for param in dinov2_model.parameters():
        param.requires_grad = False
    dinov2_preprocess = DinoImageTransform()
    print("time take: ", time.time() - init_time)

    clip_candidates = setup_clip_words()
    text_features = get_text_features(clip_candidates, clip_model)
    considered_classes, other_classes = get_considered_classes()
    detected_objs = {words[0]: [] for words in considered_classes}
    if load_from_cache:
        print('Loading data...')
        init_time = time.time()
        bgr_images = pickle.load(open(f"cache/{TASK}/bgr_images.pkl", "rb"))
        pcd_merged = o3d.io.read_point_cloud(f"cache/{TASK}/pcd_merged.pcd")
        detected_objs = pickle.load(open(f"cache/{TASK}/detected_objs.pkl", "rb"))
        print("time taken:", time.time() - init_time)
    else:
        print("Computing point cloud...")
        init_time = time.time()
        used_camera_idx = 0
        # TODO: use this after you implement utils/perception/camera.py
        from utils.perception.camera import multi_cam
        bgr_images, pcd_merged, raw_points, _ = multi_cam.take_bgrd(visualize=visualize)
        image = bgr_images[realsense_serial_numbers[used_camera_idx]]
        coord2point_3d = raw_points[used_camera_idx]
        del raw_points
        print("time taken:", time.time() - init_time)
     
        print("Loading SAM model...")
        init_time = time.time()
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) 
        sam = sam_model_registry["vit_h"](checkpoint="cache/sam_vit_h_4b8939.pth")
        mask_generator = SamAutomaticMaskGenerator(
            model=sam,
            points_per_side=32, 
            pred_iou_thresh=0.95,
            stability_score_thresh=0.95,
            box_nms_thresh=0.2,
            crop_n_layers=1,
            crop_overlap_ratio=0.6,
            min_mask_region_area=100
        )
        print("time taken:", time.time() - init_time)
    
        print("Cropping picture...")
        init_time = time.time()
        height, width = image.shape[:2]
        print(width, height)
        coord2point_3d = np.reshape(coord2point_3d, (height, width, -1))
        print("time taken:", time.time() - init_time)

        print("Generating masks...")
        init_time = time.time()
        sam.to(device=device)
        masks = mask_generator.generate(image)
        mask_points = []
        mask_images = []
        mask_preprocessed_images = []
        extra_classes, num = get_extra_classes()
        if extra_classes is not None:
            extra_masks, boxes = clip_with_owl(image, num, sam, obj_name=extra_classes[0], visualize=False)
            for mask, box in zip(extra_masks, boxes):
                tmp_mask = {'segmentation': mask, 'bbox': box, 'area': mask.sum()}
                masks.append(tmp_mask)
            if extra_classes not in considered_classes:
                considered_classes.append(extra_classes)
        masks = sorted(masks, key=(lambda x: x['area']), reverse=True)
        del sam
        del mask_generator
        print("time taken:", time.time() - init_time)

        print("Processing masks...")
        init_time = time.time()
        for i, mask in enumerate(masks):
            # if mask['area'] < 600:
            #     break
            duplicate = False
            for prev_mask in masks[:i]:
                intersection = np.sum(mask['segmentation'] & prev_mask['segmentation'])
                union = np.sum(mask['segmentation'] | prev_mask['segmentation'])
                if intersection / union > 0.1:
                    duplicate = True
                    break
            if duplicate:
                continue

            x, y, w, h = map(int, mask['bbox'])
            print(x, y, w, h)
            # bbox = (y,x,y+h,x+w)
            # project(used_camera_idx,bbox,max_num=300)
            background = 255
            im = np.ones((h, w, 3), dtype=np.uint8) * background
            mask_points.append(coord2point_3d[mask['segmentation']])
            new_mask = mask['segmentation'][y:min(y+h, height), x:min(x+w, width)]
            print(new_mask.shape)
            im[new_mask] = image[y:min(y+h, height), x:min(x+w, width), :][new_mask]
            square_size = max(h, w) + 6
            im = np.pad(im, (((square_size - h) // 2, (square_size - h) // 2), ((square_size - w) // 2, (square_size - w) // 2), (0, 0)), 'constant', constant_values=background)
            # im = image[y:y+h, x:x+w, :]
            mask_images.append(im)
            mask_preprocessed_images.append(clip_preprocess(Image.fromarray(im)).to(device))
        mask_preprocessed_images = torch.stack(mask_preprocessed_images)
        print("time taken:", time.time() - init_time)

        print("Getting labels with CLIP...")
        init_time = time.time()
        clip_model.to(device=device)
        with torch.no_grad(), torch.cuda.amp.autocast():
            image_features = clip_model.encode_image(mask_preprocessed_images)
            image_features /= image_features.norm(dim=-1, keepdim=True)
            raw_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1).cpu()
            print(image_features.shape, text_features.shape)
        clip_model.to(device="cpu")
        image_features = image_features.cpu().numpy()
        print("time taken:", time.time() - init_time)

        print("Compute DINOv2 features...")
        init_time = time.time()
        for i in range(len(mask_images)):
            mask_images[i] = cv2.resize(mask_images[i], (224, 224))
        mask_dino_preprocessed_images = torch.from_numpy(np.transpose(np.stack(mask_images), (0, 3, 1, 2))).to(device)
        dinov2_model.to(device=device)
        dino_image_features = dinov2_model.forward_features(dinov2_preprocess(mask_dino_preprocessed_images))["x_norm_clstoken"]
        dino_image_features = F.normalize(dino_image_features, dim=-1)
        dinov2_model.to(device="cpu")
        dino_image_features = dino_image_features.cpu().numpy()
        print("time taken:", time.time() - init_time)

        print("Postprocessing raw CLIP probabilities...")
        init_time = time.time()
        len_list = [len(words) for words in considered_classes + other_classes]
        probs = torch.zeros(*raw_probs.shape[:1], len(considered_classes) + len(other_classes), dtype=raw_probs.dtype, device=raw_probs.device)
        for i in range(len(considered_classes)):
            probs[..., i] = (raw_probs[..., sum(len_list[:i]) : sum(len_list[:i + 1])].max(dim=-1).values)
        for i in range(len(other_classes)):
            probs[..., i + len(considered_classes)] = (raw_probs[..., sum(len_list[:len(considered_classes) + i]) : sum(len_list[:len(considered_classes) + i + 1])].max(dim=-1).values)
        del raw_probs
        del clip_model
        del clip_preprocess
        print("time taken:", time.time() - init_time)

        print("Filtering detection results...")
        init_time = time.time()
        probs = probs.numpy()
        for i, p in enumerate(probs):
            predicted_class_idx = np.argmax(p)
            predicted_class_name = considered_classes[predicted_class_idx][0] if predicted_class_idx < len(considered_classes) else other_classes[predicted_class_idx - len(considered_classes)][0]
            if detected_objs.get(predicted_class_name) is not None:
                # detected_objs[predicted_class_name].append((mask_points[i], p[predicted_class_idx], norm_vec[i]))
                detected_objs[predicted_class_name].append((mask_points[i], p[predicted_class_idx], image_features[i], dino_image_features[i]))
            if not os.path.exists(f"cache/{predicted_class_idx}_{predicted_class_name}"):
                os.makedirs(f"cache/{predicted_class_idx}_{predicted_class_name}")
            cv2.imwrite(f"cache/{predicted_class_idx}_{predicted_class_name}/{i}_{p[predicted_class_idx]:.4f}.png", mask_images[i][:, :, ::-1])
        print("time taken:", time.time() - init_time)
        
        if save_to_cache:
            pickle.dump(bgr_images, open_file(f"cache/{TASK}/bgr_images.pkl", "wb"))
            o3d.io.write_point_cloud(f"cache/{TASK}/pcd_merged.pcd", pcd_merged)
            pickle.dump(detected_objs, open_file(f"cache/{TASK}/detected_objs.pkl", "wb"))

    delete_file(f"log/{TASK}/bgr_images.pkl")
    delete_file(f"log/{TASK}/pcd_merged.pcd")
    delete_file(f"log/{TASK}/detected_objs.pkl")
    pickle.dump(bgr_images, open_file(f"log/{TASK}/bgr_images.pkl", "wb"))
    o3d.io.write_point_cloud(f"log/{TASK}/pcd_merged.pcd", pcd_merged)
    pickle.dump(detected_objs, open_file(f"log/{TASK}/detected_objs.pkl", "wb"))

    if visualize:
        print("Plotting point cloud...")
        init_time = time.time()
        mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1, origin=[0, 0, 0])
        detected_objects_geomertries = o3d.geometry.TriangleMesh()
        for key, detected_points in detected_objs.items():
            for pcd, _, _ in detected_points:
                sphere = o3d.geometry.TriangleMesh.create_sphere(radius=RADIUS)
                sphere.translate(np.mean(pcd, axis=0))
                detected_objects_geomertries += sphere
        detected_objects_geomertries.paint_uniform_color([0.0, 1.0, 0.0])
        o3d.visualization.draw_geometries([pcd_merged, mesh_frame, detected_objects_geomertries])
        print("time taken:", time.time() - init_time)

def get_text_features(clip_candidates, clip_model):
    global tokenizer
    try:
        text_features = pickle.load(open(f"cache/{TASK}/text_features.pkl", "rb"))
        return text_features
    except:
        if tokenizer is None:
            tokenizer = open_clip.get_tokenizer('ViT-g-14')
        texts = tokenizer([f"A photo of {c}." for c in clip_candidates]).to(device)
        clip_model.to(device=device)
        with torch.no_grad(), torch.cuda.amp.autocast():
            text_features = clip_model.encode_text(texts)
            text_features /= text_features.norm(dim=-1, keepdim=True)
        clip_model.to(device="cpu")
        pickle.dump(text_features, open_file(f"cache/{TASK}/text_features.pkl", "wb"))
        return text_features

# ---------------------------------- Get task pose ----------------------------------

def _get_task_pose(task_name, visualize=True):
    task_name = task_name[:-1] if task_name[-1] == '.' else task_name
    whole_prompt = prompt_get_task_pose + '\n' + '\n' + f"INPUT: the robot's task is to {task_name.lower()}." + "\n"
    add_to_log(whole_prompt)
    response = query_LLM(whole_prompt, ["INPUT:"], "cache/llm_response_get_task_pose.pkl")
    if "response" in response.text.lower():
        response_json = response.text.split('\n', 1)[1]
        add_to_log(response_json)
        pose_dict = json.loads(response_json)
    else:
        pose_dict = json.loads(response.text)
    add_to_log('-'*80)
    add_to_log('*at get_task_pose*')
    add_to_log('get_task_pose:', pose_dict)
    add_to_log('-'*80)
    pos_str = pose_dict['gripper position']
    ori_str = pose_dict['gripper orientation']
    _get_task_detection(task_name)
    pos, ori = get_pose_from_str(pos_str, ori_str, visualize=visualize)
    return pos, ori

def get_pose_from_str(pos_str, ori_str, visualize=False):
    global queried_obj
    pos_feature = pos_str.split('.')[1]
    obj_name = pos_str.split('.')[0]
    if 'absolute_vertical' in ori_str or 'absolute_horizontal' in ori_str:
        ori_feature = ori_str
        whole_prompt = prompt_get_pose_from_str + '\n' + "# Query: " + f"""the {pos_feature} of "{obj_name}"."""
    else:
        ori_feature = ori_str.split('.')[1]
        whole_prompt = prompt_get_pose_from_str + '\n' + "# Query: " + f"""the {pos_feature} and {ori_feature} of "{obj_name}"."""
    response = query_LLM(whole_prompt, ["INPUT:"], "cache/llm_response_get_pose_from_str.pkl")
    code_as_policies = response.text
    add_to_log('-'*80)
    add_to_log('*at get_pose_from_str*')
    add_to_log(code_as_policies)
    add_to_log('-'*80)
    locals = {}
    exec(code_as_policies, globals(), locals)
    obj = locals['ret_val']
    if ori_feature == 'absolute_vertical':
        if 'drawer' in obj_name:
            vertical_quat = get_vertical_ori()
            minor_axis = get_minor_axis_from_pcd(obj.pcd)
            rotate_angle = math.degrees(-math.atan2(minor_axis[1], minor_axis[0]))
            tar_quat = policy.rotate_around_gripper_z_axis(rotate_angle, vertical_quat)
            obj.rotation = tar_quat
            obj.vec = minor_axis
        elif 'shelf' in obj_name:
            obj.rotation = get_current_ori()
            minor_axis = get_minor_axis_from_pcd(obj.pcd)
            obj.vec = minor_axis
        else:
            vertical_quat = get_vertical_ori()
            main_axis = get_x_axis_from_pcd(obj.pcd)
            rotate_angle = math.degrees(-math.atan2(main_axis[1], main_axis[0]))
            tar_quat = policy.rotate_around_gripper_z_axis(rotate_angle, vertical_quat)
            obj.rotation = tar_quat
            obj.vec = main_axis
    elif ori_feature == 'absolute_horizontal':
        obj.rotation = get_horizontal_ori(get_vec=False)
        obj.vec = get_horizontal_ori(get_vec=True)
    else:
        obj.vec = obj.rotation.copy()
        obj.rotation = policy.align_z_axis_with_vector(obj.vec)
    if REAL_ROBOT:
        pcd_merged = o3d.io.read_point_cloud(f"log/{TASK}/pcd_merged.pcd")    
        mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1, origin=[0, 0, 0])
        visualized_geometries = [pcd_merged, mesh_frame]
        for i in range(50):
            sphere = o3d.geometry.TriangleMesh.create_sphere(radius=RADIUS/3)
            sphere.paint_uniform_color([0.0, 1.0, 0.0])
            sphere.translate(calculate_centroid(obj.pcd)+i/100*obj.vec)
            visualized_geometries.append(sphere)
        o3d.visualization.draw_geometries(visualized_geometries)
    obj.position = policy.robot_fingertip_pos_to_ee(obj.position, obj.rotation)
    obj.bounding_box = get_bounding_box_from_pcd(obj.pcd, obj.vec)
    queried_obj.bounding_box = obj.bounding_box
    queried_obj.vec = obj.vec
    if queried_obj.detected_pose is None:
        queried_obj.clip_feature = obj.clip_feature.copy()
        queried_obj.dino_feature = obj.dino_feature.copy()
        queried_obj.detected_pose = (obj.position, obj.rotation)
    return obj.position, obj.rotation

def calculate_centroid(point_cloud):
    centroid = np.mean(point_cloud, axis=0)
    return centroid

def calculate_normal_vector(point_cloud):
    cov_matrix = np.cov(point_cloud, rowvar=False)
    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)
    min_eigenvalue_index = np.argmin(eigenvalues)
    normal_vector = eigenvectors[:, min_eigenvalue_index]
    if normal_vector[0]<0:
        normal_vector = -normal_vector
    normal_vector[2] = 0.
    normal_vector /= np.linalg.norm(normal_vector)
    return normal_vector
    
def calculate_major_axis(point_cloud):
    covariance_matrix = np.cov(point_cloud, rowvar=False)
    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)
    main_axis = eigenvectors[:, np.argmax(eigenvalues)]
    return main_axis
    # return calculate_normal_vector(point_cloud)

def calculate_minor_axis(point_cloud):
    return calculate_normal_vector(point_cloud)

def calculate_tail_point(point_cloud):
    x_axis = get_x_axis_from_pcd(point_cloud)
    centroid = calculate_centroid(point_cloud)
    tail_point = centroid - 0.5 * x_axis
    return tail_point

def get_x_axis_from_pcd(point_cloud):
    if REAL_ROBOT:
        o3d_point_cloud = o3d.geometry.PointCloud()
        o3d_point_cloud.points = o3d.utility.Vector3dVector(point_cloud)
        bbox = o3d_point_cloud.get_oriented_bounding_box()
        eight_vertices = np.asarray(bbox.get_box_points())
        sorted_indices = np.argsort(eight_vertices[:,2])[::-1]
        sorted_array_z = eight_vertices[sorted_indices]
        sorted_array_z = sorted_array_z[:4,:]
        len_xy = np.linalg.norm(sorted_array_z[0]-sorted_array_z[1])
        axis_xy = sorted_array_z[0]-sorted_array_z[1]
        len_yx = np.linalg.norm(sorted_array_z[0]-sorted_array_z[2])
        axis_yx = sorted_array_z[0]-sorted_array_z[2]
        if len_xy > len_yx:
            main_axis = axis_xy
        else:
            main_axis = axis_yx
        main_axis[2] = 0.
        main_axis /= np.linalg.norm(main_axis)
        main_axis = main_axis if main_axis[0]>0 else -main_axis
        return main_axis
    else:
        return np.array((1.,0.,0.))

def get_minor_axis_from_pcd(point_cloud):
    if REAL_ROBOT:
        o3d_point_cloud = o3d.geometry.PointCloud()
        o3d_point_cloud.points = o3d.utility.Vector3dVector(point_cloud)
        bbox = o3d_point_cloud.get_oriented_bounding_box()
        eight_vertices = np.asarray(bbox.get_box_points())
        sorted_indices = np.argsort(eight_vertices[:,2])[::-1]
        sorted_array_z = eight_vertices[sorted_indices]
        sorted_array_z = sorted_array_z[:4,:]
        len_xy = np.linalg.norm(sorted_array_z[0]-sorted_array_z[1])
        axis_xy = sorted_array_z[0]-sorted_array_z[1]
        len_yx = np.linalg.norm(sorted_array_z[0]-sorted_array_z[2])
        axis_yx = sorted_array_z[0]-sorted_array_z[2]
        if len_xy > len_yx:
            minor_axis = axis_yx
        else:
            minor_axis = axis_xy
        minor_axis[2] = 0.
        minor_axis /= np.linalg.norm(minor_axis)
        minor_axis = minor_axis if minor_axis[0]>0 else -minor_axis
        return minor_axis
    else:
        return (1., 0., 0.)

def get_bounding_box_from_pcd(point_cloud, forward_vec):
    if REAL_ROBOT:
        o3d_point_cloud = o3d.geometry.PointCloud()
        o3d_point_cloud.points = o3d.utility.Vector3dVector(point_cloud)
        bbox = o3d_point_cloud.get_oriented_bounding_box()
        eight_vertices = np.asarray(bbox.get_box_points())
        sorted_indices = np.argsort(eight_vertices[:,2])[::-1]
        sorted_array_z = eight_vertices[sorted_indices]
        len_z = sorted_array_z[:4,2].mean() - sorted_array_z[4:,2].mean()
        sorted_array_z = sorted_array_z[:4,:]
        len_xy = np.linalg.norm(sorted_array_z[0]-sorted_array_z[1])
        axis_xy = sorted_array_z[0]-sorted_array_z[1]
        len_yx = np.linalg.norm(sorted_array_z[0]-sorted_array_z[2])
        axis_yx = sorted_array_z[0]-sorted_array_z[2]
        if len_xy > len_yx:
            main_axis = axis_xy
            len_x = len_xy
            len_y = len_yx
        else:
            main_axis = axis_yx
            len_x = len_yx
            len_y = len_xy
        main_axis[2] = 0.
        main_axis /= np.linalg.norm(main_axis)
        minor_axis = np.array((main_axis[1], -main_axis[0], 0.))
        if np.abs(forward_vec.dot(main_axis)) > np.abs(forward_vec.dot(minor_axis)):
            return (round(len_x*100), round(len_y*100), round(len_z*100))
        else:
            return (round(len_y*100), round(len_x*100), round(len_z*100))
    else:
        return (1., 8., 3.)

# ---------------------------------- Image feature ----------------------------------

def compare_text_image_sim(text, vis_feature):
    global clip_model, tokenizer
    vis_feature = torch.tensor(vis_feature, device=device)
    if tokenizer is None:
        tokenizer = open_clip.get_tokenizer('ViT-g-14')
    if clip_model is None:
        clip_model, _, _ = open_clip.create_model_and_transforms('ViT-g-14', pretrained='laion2b_s34b_b88k')
    clip_model = clip_model.to(device=device)
    text = tokenizer(f"A photo of {text}.").to(device=device)
    with torch.no_grad(), torch.cuda.amp.autocast():
        text_feature = clip_model.encode_text(text).squeeze()
        text_feature /= text_feature.norm(dim=-1, keepdim=True)
        vis_feature /= vis_feature.norm(dim=-1, keepdim=True)
        raw_probs = vis_feature.dot(text_feature).item()
    del clip_model
    return raw_probs

def get_detected_feature():
    global queried_obj
    if REAL_ROBOT:
        image_feature = queried_obj.clip_feature.copy()
        dino_image_feature = queried_obj.dino_feature.copy()
        detected_pose = queried_obj.detected_pose
        return (image_feature, dino_image_feature, detected_pose)
    else:
        return (np.random.randn(1024), np.random.randn(512), (np.array((1.,0.,0.)), np.array((1.,0.,0.,0.))))

def compare_feature(img_feature, task_name, visualize=False, threshold=10, metric='L2'):
    """ Delete all irrelevant features """
    global load_detected_objs
    obj_name = get_obj_name_from_task(task_name)
    obj_name = parse_obj_name(obj_name)
    objs_specified = load_detected_objs[obj_name]
    similarity = []
    for _, _, _, new_dino_feature in objs_specified:
        if metric == 'L2':
            similarity.append(((new_dino_feature-img_feature)**2).sum())
        elif metric == 'cosine':
            similarity.append(1-new_dino_feature.dot(img_feature)/(np.linalg.norm(new_dino_feature)*np.linalg.norm(img_feature)))
        else:
            raise NotImplementedError
    add_to_log(similarity)
    order = np.argsort(np.array(similarity))
    picked_obj_idx = list(order[:threshold])
    picked_obj = [objs_specified[i] for i in picked_obj_idx]
    load_detected_objs[obj_name] = picked_obj
    if visualize:
        pcd_merged = o3d.io.read_point_cloud(f'log/{TASK}/pcd_merged.pcd')
        mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1, origin=[0, 0, 0])
        detected_objects_geomertries = o3d.geometry.TriangleMesh()
        for obj in picked_obj:
            pcd, _, _, _ = obj
            p = calculate_centroid(pcd)
            sphere = o3d.geometry.TriangleMesh.create_sphere(radius=RADIUS)
            sphere.translate(p)
            detected_objects_geomertries += sphere
            detected_objects_geomertries.paint_uniform_color([0.0, 1.0, 0.0])
        o3d.visualization.draw_geometries([pcd_merged, mesh_frame, detected_objects_geomertries])

# ---------------------------------- Others ----------------------------------

def _update_object_state(ini_state, task_name):
    with open(prompt_update_obj_state_file, "r") as template_file:
        template_content = template_file.read()
    values = {"initial_state": ini_state, "task_name": task_name}
    prompt = template_content.format(**values)
    response = query_LLM(prompt, [], "cache/llm_update_obj_name.pkl")
    new_obj_state = response.text
    return new_obj_state

def segment_image(image):
    from segment_anything import SamAutomaticMaskGenerator, sam_model_registry
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    print("Loading SAM model...")
    init_time = time.time()
    sam = sam_model_registry["vit_h"](checkpoint="cache/sam_vit_h_4b8939.pth")
    mask_generator = SamAutomaticMaskGenerator(
        model=sam,
        points_per_side=32, 
        pred_iou_thresh=0.95,
        stability_score_thresh=0.95,
        box_nms_thresh=0.2,
        crop_n_layers=1,
        crop_overlap_ratio=0.6,
        min_mask_region_area=100
    )
    print("time taken:", time.time() - init_time)
    print("Generating masks...")
    sam.to(device="cuda")
    masks = mask_generator.generate(image)
    masks = sorted(masks, key=(lambda x: x['area']), reverse=False)
    print("Processing masks...")
    init_time = time.time()
    mask_images = []
    for i, mask in enumerate(masks):
        duplicate = False
        for prev_mask in masks[:i]:
            intersection = np.sum(mask['segmentation'] & prev_mask['segmentation'])
            union = np.sum(mask['segmentation'] | prev_mask['segmentation'])
            if intersection / union > 0.1:
                duplicate = True
                break
        if duplicate:
            continue
        x, y, w, h = map(int, mask['bbox'])
        background = 255
        im = np.ones((h, w, 3), dtype=np.uint8) * background
        new_mask = mask['segmentation'][y:y+h, x:x+w]
        im[new_mask] = image[y:y+h, x:x+w, :][new_mask]
        im = np.pad(im, ((3, 3), (3, 3), (0, 0)), 'constant', constant_values=background)
        # im = image[y:y+h, x:x+w, :]
        mask_images.append(im)

def save_current_image(directory_path, visualize=False):
    if REAL_ROBOT:
        pass

def test_image(text):
    clip_model, _, clip_preprocess = open_clip.create_model_and_transforms('ViT-g-14', pretrained='laion2b_s34b_b88k')
    files = ['cache/image_for_retrieval/cup1.png', 'cache/image_for_retrieval/cup2.png', 'cache/image_for_retrieval/cup3.png', 'cache/image_for_retrieval/cup4.png', 'cache/image_for_retrieval/cup5.png']
    images = [clip_preprocess(Image.open(file)).to(device).to(torch.float) for file in files]
    preprocessed_images = torch.stack(images)
    clip_model = clip_model.to(device)
    with torch.no_grad(), torch.cuda.amp.autocast():
        image_features = clip_model.encode_image(preprocessed_images)
    tokenizer = open_clip.get_tokenizer('ViT-g-14')
    text = tokenizer(f"A photo of {text}.").to(device)
    with torch.no_grad(), torch.cuda.amp.autocast():
        text_feature = clip_model.encode_text(text).squeeze()
        text_feature /= text_feature.norm(dim=-1, keepdim=True)
    for i in image_features:
        i /= i.norm(dim=-1, keepdim=True)
        raw_probs = i.dot(text_feature).item()
        print(raw_probs)
    return raw_probs


if __name__ == '__main__':
    print(1)


File: utils/robot/dummy_policy.py

import numpy as np
from utils.exception_utils import InterruptedByHuman

class DummyPolicy:
    def __init__(self):
        pass

    def close_gripper(self, check_grasp=True):
        return True
        
    def open_gripper(self, width=1.0):
        pass
  
    def rotate_gripper(self, degrees, axis):
        return np.array((1.,0.,0.,0.))


    def move_to_pos(self, ee_pos, tar_quat):
        a = input('Do you want to raise exception?')
        if a == 'y':
            raise InterruptedByHuman('')
        pass

    def rotate_around_gripper_z_axis(self, angle, quat=None):
        return np.array((1.,0.,0.,0.))

    def tilt_leftright(self, degrees):
        return np.array((1.,0.,0.,0.))

    def tilt_updown(self, degrees=None):
        return np.array((1.,0.,0.,0.))

    def align_z_axis_with_vector(self, z_axis, finger_plane='vertical'):
        return np.array((1.,0.,0.,0.))

    def reset(self):
        pass

    def robot_fingertip_pos_to_ee(self, fingertip_pos, ee_quat):
        return np.array((1.,0.,0.))

    def ee_pos_to_fingertip(self, ee_pos, ee_quat):
        return np.array((1.,0.,0.))

    def get_horizontal_ori(self):
        return np.array((1.,0.,0.,0.))

    def get_vertical_ori(self):
        return np.array((1.,0.,0.,0.))


File: utils/robot/panda_env.py

class PandaEnv():
    '''
    Put your own implementation for Franka Panda here
    '''
    def __init__(self):
        pass

    def reset():
        pass


File: utils/robot/robot_policy.py

import time
import numpy as np
from utils.exception_utils import InterruptedByHuman, RobotError, GraspError
from utils.robot.panda_env import PandaEnv
from scipy.spatial.transform import Rotation
from utils.transformation_utils import extract_z_axis, pose_to_mat, quat_to_euler, add_euler, euler_to_quat, quat_to_mat, mat_to_quat, mat_to_euler


GRIPPER_SPEED, GRIPPER_FORCE, GRIPPER_MAX_WIDTH, GRIPPER_TOLERANCE = 0.1, 40, 0.08570, 0.01

def calculate_frame_quaternion(g_x, g_z):
    base_x = np.array([1., 0., 0.])  # Replace with the X-axis vector of the base frame
    base_z = np.array([0., 0., 1.])  # Replace with the Z-axis vector of the base frame
    # Step 1: Calculate Y-axis vector of frame G in the base frame
    base_y = np.cross(base_z, base_x)
    base_y /= np.linalg.norm(base_y)
    # Step 2: Create rotation matrices for the base and frame G
    R_base = np.column_stack((base_x, base_y, base_z))
    R_g = np.column_stack((g_x, np.cross(g_z, g_x), g_z))      
    # Step 3: Compute rotation matrix to transform frame G's axes to the base frame axes
    R_relative_to_base = np.dot(R_base.T, R_g)
    # Step 4: Convert the rotation matrix to a quaternion using scipy's Rotation class
    r = Rotation.from_matrix(R_relative_to_base)
    frame_quaternion = r.as_quat()
    return frame_quaternion

class KptPrimitivePolicy:
    def __init__(self):
        self.robot_env = PandaEnv()
        self.robot_env.reset()

    def close_gripper(self, check_grasp=True):
        self.robot_env.gripper.grasp(speed=GRIPPER_SPEED, force=GRIPPER_FORCE, blocking=True)
        time.sleep(1.8)
        obj_in_gripper = False
        if self.robot_env.gripper.get_state().width > 0.005:
            obj_in_gripper = True
        if obj_in_gripper == False:
            if check_grasp:
                raise GraspError('Grasp Failure')
        return obj_in_gripper
        
    def open_gripper(self, width=1.0):
        self.robot_env.gripper.goto(width*GRIPPER_MAX_WIDTH, speed=GRIPPER_SPEED, force=GRIPPER_FORCE)
  
    def rotate_gripper(self, degrees, axis):
        if axis == 'z':
            tar_quat = self.rotate_around_gripper_z_axis(degrees)
        elif axis == 'x':
            tar_quat = self.tilt_updown(degrees)
        elif axis == 'y':
            tar_quat = self.tilt_leftright(degrees)
        return tar_quat


    def move_to_pos(self, ee_pos, tar_quat):
    
        assert len(tar_quat) == 4
        if ee_pos[2] > 0.5:
            ee_pos[2] = 0.5
        if ee_pos[2] < 0.115:
            ee_pos[2] = 0.115
        # ee_pos = self.robot_fingertip_pos_to_ee(fingertip_pos, tar_quat)
        ret_val = self.robot_env.robot.move_to_ee_pose(position=ee_pos, orientation=tar_quat)
        if ret_val == 1:
            raise InterruptedByHuman('Interrupted by human.')
        elif ret_val == 2:
            raise RobotError('IK did not converge.')

    def rotate_around_gripper_z_axis(self, angle, quat=None):
        theta_rad = np.radians(angle)
        cos_theta = np.cos(theta_rad)
        sin_theta = np.sin(theta_rad)

        r_gd = np.array([[cos_theta, -sin_theta, 0],
                  [sin_theta, cos_theta, 0],
                  [0, 0, 1]])
        pos = self.robot_env.robot.get_ee_pose()
        if quat is None:
            t_bg = pose_to_mat((pos[0].cpu().numpy(), pos[1].cpu().numpy()))
        else:
            t_bg = pose_to_mat((pos[0].cpu().numpy(), np.array(quat)))
        r_bg = t_bg[:3, :3]
        r_bd = r_bg @ r_gd
        r_bd = Rotation.from_matrix(r_bd)
        q_b = r_bd.as_quat()
        return q_b

    def tilt_leftright(self, degrees):
        current_euler = quat_to_euler(self.robot_env.robot.get_ee_pose()[1].numpy())
        angle = np.radians(degrees)
        delta_euler = np.array((0., 0., angle))   # tilt_leftright rotate around absolute z-axis
        new_euler = add_euler(delta_euler, current_euler)
        tar_quat = euler_to_quat(new_euler)
        return tar_quat

    def tilt_updown(self, degrees=None):
        if degrees is None:
            current_z_axis = extract_z_axis(self.robot_env.robot.get_ee_pose()[1].numpy())
            old_x = current_z_axis[0]
            old_y = current_z_axis[1]
            if np.abs(old_y) >= 10e-3:
                c = old_x/old_y
                new_y = 1/np.sqrt(c**2+1) if current_z_axis[1]>0 else -1/np.sqrt(c**2+1)
                new_x = c*new_y
                new_z_axis = np.array((new_x, new_y, 0))
                new_x_axis = self.g_x
            else:
                new_z_axis = np.array((1., 0., 0.))
                new_x_axis = self.g_x
            frame_quaternion = calculate_frame_quaternion(new_x_axis, new_z_axis)
            tar_quat = self.rotate_around_gripper_z_axis(45, frame_quaternion)
        else:
            g_z = extract_z_axis(self.robot_env.robot.get_ee_pose()[1].numpy())
            g_z[2] = 0
            g_x = np.array((g_z[1],-g_z[0],0))
            g_y = np.cross(g_z, g_x)
            g_x = g_x/np.linalg.norm(g_x)
            g_y = g_y/np.linalg.norm(g_y)
            g_z = g_z/np.linalg.norm(g_z)
            r_b = np.stack((g_x,g_y,g_z),axis=1)
            r_sb = r_b
            r_bs = r_sb.T
            mat_g_in_s = quat_to_mat(self.robot_env.robot.get_ee_pose()[1].numpy())
            mat_g_in_b = r_bs.dot(mat_g_in_s)
            euler_g_in_b = mat_to_euler(mat_g_in_b)
            angle = np.radians(degrees)
            delta_euler = np.array((angle, 0., 0.))   # tilt_updown rotate around gripper x-axis
            new_euler = add_euler(delta_euler, euler_g_in_b)
            new_quat_g_in_b = euler_to_quat(new_euler)
            new_mat_g_in_b = quat_to_mat(new_quat_g_in_b)
            new_mat_g_in_s = r_sb.dot(new_mat_g_in_b)
            tar_quat = mat_to_quat(new_mat_g_in_s)
        return tar_quat

    def align_z_axis_with_vector(self, z_axis, finger_plane='vertical'):
        g_z = z_axis
        self.g_z = g_z
        if finger_plane == 'horizontal':
            g_x = np.array([0.,0.,1.])
            self.g_x = g_x
            self.g_y = np.cross(g_z, g_x)
        elif finger_plane == 'vertical':
            if g_z[0]*g_z[1] >=0:
                g_y = np.array([0.,0.,1.])
            else:
                g_y = np.array([0.,0.,-1.])
            g_x = np.cross(g_z, g_y)
            self.g_x = g_x
            self.g_y = g_y
        frame_quaternion = calculate_frame_quaternion(g_x, g_z)
        frame_quaternion = self.rotate_around_gripper_z_axis(45, frame_quaternion)
        return frame_quaternion

    def reset(self,reset_gripper=True):
        self.robot_env.reset(reset_gripper=reset_gripper)

    def robot_fingertip_pos_to_ee(self, fingertip_pos, ee_quat):
        HOME_QUAT = np.array([ 0.9201814 , -0.39136365,  0.00602445,  0.00802529])
        FINGERTIP_OFFSET = np.array([0,0,-0.095])
        home_euler = Rotation.from_quat(HOME_QUAT).as_euler('zyx', degrees=True)
        ee_euler = Rotation.from_quat(ee_quat).as_euler('zyx', degrees=True)
        offset_euler = ee_euler - home_euler
        fingertip_offset_euler = offset_euler * [1,-1,1]
        fingertip_transf = Rotation.from_euler('zyx', fingertip_offset_euler, degrees=True)
        fingertip_offset = fingertip_transf.as_matrix() @ FINGERTIP_OFFSET
        fingertip_offset[2] -= FINGERTIP_OFFSET[2]
        ee_pos = fingertip_pos - fingertip_offset
        return ee_pos

    def ee_pos_to_fingertip(self, ee_pos, ee_quat):
        current_z_aixs = extract_z_axis(ee_quat)
        tip_pos = 0.095 * current_z_aixs + ee_pos
        return tip_pos

    def get_horizontal_ori(self):
        current_z_aixs = extract_z_axis(self.robot_env.robot.get_ee_pose()[1].numpy())
        if np.abs(current_z_aixs[2]) > 0.9:
            target_z_aixs = np.array((1.,0.,0.))
        else:
            current_z_aixs[2] = 0.
            current_z_aixs = -current_z_aixs if current_z_aixs[0]<0 else current_z_aixs
            target_z_aixs = current_z_aixs/np.linalg.norm(current_z_aixs)
        quat_tmp = self.align_z_axis_with_vector(target_z_aixs)
        quat = self.rotate_around_gripper_z_axis(-90, quat_tmp)
        return quat

    def get_vertical_ori(self):
        target_z_aixs = np.array((0.,0.,-1.))
        return self.align_z_axis_with_vector(target_z_aixs)



File: utils/string_utils.py

import numpy as np
import re
import json
import pickle
from utils.io.io_utils import add_to_log, read_py, HISTORY_TMP_PATH, load_file
from utils.LLM_utils import query_LLM

prompt_replace_description_with_value = read_py('prompts/replace_des_with_val.txt')

def format_code(response):
    if "'''" in response.text:
        code = response.text.split("'''")[1]
        if 'Perception error' in response.text:
            error_type = 'Perception error'
        elif 'Planning error' in response.text:
            error_type = 'Planning error'
        else:
            error_type = None
        return error_type, code
    else:
        code = response.text
        error_type = None
        return error_type, code


def format_plan(response, prefix='Response:'):
    lines = response.text.split("\n")
    non_empty_lines = [line for line in lines if line.strip() != ""]
    result = "\n".join(non_empty_lines)
    if prefix is not None:
        plan_raw = result.split(prefix)[1]
        lines = plan_raw.split("\n")
        non_empty_lines = []
        for line in lines:
            if line != '}':
                non_empty_lines.append(line)
            else:
                non_empty_lines.append(line)
                break
        result = "\n".join(non_empty_lines)
        plan_dict = json.loads(result)
    else:
        plan_dict = json.loads(result)
    return plan_dict


def from_dict_to_str(res_dict):
    ret_str = ''
    for k, v in res_dict.items():
        ret_str += str(k)
        ret_str += '. '
        ret_str += str(v)
        ret_str += ' '
    return ret_str


def get_lines_starting_with(text, prefix, first=True):
    lines = text.splitlines()
    move_to_lines = [line for line in lines if line.strip().startswith(prefix)]
    if first:
        result = move_to_lines[0].strip()
        if result[-1] == ";":
            result = result[:-1]
        return result
    else:
        result = [i.strip() for i in move_to_lines]
        return result


def break_plan_into_steps(code_as_policies):
    if '# ' in code_as_policies:
        lines = code_as_policies.strip().split('\n')
        # Initialize variables to store each step's code
        step_codes = []
        current_step = []
        # Iterate through the lines and identify each step's code
        for line in lines:
            if line.startswith("# "):
                if current_step:
                    step_codes.append('\n'.join(current_step))
                    current_step = []
            current_step.append(line)
        # Append the last step's code
        if current_step:
            step_codes.append('\n'.join(current_step))
    else:
        step_codes = [code_as_policies]
    return step_codes


def extract_array_from_str(text):
    matches = re.findall(r'\[([\d\.\s-]+)\]', text)
    if matches:
        numbers_str = matches[0]
        numbers_list = [float(num) for num in numbers_str.split()]
        num_array = np.array(numbers_list)
        return num_array
    else:
        add_to_log("No matching numbers found.")
        return None


def replace_strarray_with_str(input_string, replace_str):
    new_text = re.sub(r'\[([\d\.\s-]+)\]', replace_str, input_string)
    return new_text


def replace_brackets_in_file(file_path, replacement_string):
    with open(file_path, 'r') as file:
        content = file.read()
    modified_content = content.replace("[]", replacement_string)
    return modified_content


def replace_description_with_value(code, pos_description):
    value_line = code.splitlines()[-1]
    if any(char.isdigit() for char in value_line):
        number_matches = str(int(float(re.findall(r'\d+\.\d+', value_line)[0]) * 100)) + 'cm'
    else:
        number_matches = '10cm'
    whole_prompt = prompt_replace_description_with_value + '\n' + '"' + pos_description + '", "' + number_matches + '":'
    response = query_LLM(whole_prompt, [], "cache/llm_replace_des_with_val.pkl")
    new_description = response.text
    add_to_log('old_pos_des:' + pos_description + ', new_pos_des:' + new_description)
    return new_description


def str_to_dict(string):
    paragraphs = string.split('\n\n')
    paragraph_dict = {index: paragraph.strip() for index, paragraph in enumerate(paragraphs)}
    return paragraph_dict


def dict_to_str(dictionary):
    ret_str = ''
    for k, v in dictionary.items():
        if k != len(dictionary)-1:
            ret_str = ret_str + v + '\n\n'
        else:
            ret_str = ret_str + v
    return ret_str


def replace_code_with_no_description(new_pos_description, corr_rounds):
    history_tmp = load_file(HISTORY_TMP_PATH)
    language_instruction = list(history_tmp.keys())[0]
    step_name = list(history_tmp[language_instruction].keys())[-1]
    correction_code = history_tmp[language_instruction][step_name][f'code response {corr_rounds}']
    lines = correction_code.split('\n')
    for i,line in enumerate(lines):
        if "parse_pos" in line:
            if any(char.isdigit() for char in line):
                pass
            else:
                parts = line.split('"')
                assert len(parts) == 3
                parts[1] = new_pos_description
                lines[i] = '"'.join(parts)
                break
    history_tmp[language_instruction][step_name][f'code response {corr_rounds}'] = '\n'.join(lines)
    pickle.dump(history_tmp, open(HISTORY_TMP_PATH, "wb"))


def format_dictionary(dict_input):
    formatted_string = "{\n"
    last_key = list(dict_input.keys())[-1]
    for key, value in dict_input.items():
        if key != last_key:
            formatted_string += f'  "{key}": "{value}",\n'
        else:
            formatted_string += f'  "{key}": "{value}"\n'
    formatted_string += "}"
    return formatted_string


File: utils/transformation_utils.py

from scipy.spatial.transform import Rotation as R
import numpy as np

def calculate_relative_pose(real_pose, detected_pose, is_quat):
    if is_quat == False:
        real_pos, real_vec = real_pose
        detected_pos, detected_vec = detected_pose
        relative_rotation = rotation_matrix_between_vectors(detected_vec, real_vec)
        relative_pose = (real_pos - detected_pos, relative_rotation) # fake + relative = real; detected * relative = real
        return relative_pose
    else:
        real_pos, real_quat = real_pose
        detected_pos, detected_quat = detected_pose
        rr = quaternion_to_rotation_matrix(real_quat)
        dr = quaternion_to_rotation_matrix(detected_quat)
        # relative_rotation = np.linalg.inv(dr).dot(rr)
        relative_rotation = rr.dot(np.linalg.inv(dr))
        relative_pose = (real_pos - detected_pos, relative_rotation) # fake + relative = real; detected * relative = real
        return relative_pose

def get_real_r(relative_rotation, detected_rotation):
    # real_rotation = detected_rotation.dot(relative_rotation)
    real_rotation = relative_rotation.dot(detected_rotation)
    return real_rotation

def rotation_matrix_between_vectors(a, b):
    a = np.array(a)
    b = np.array(b)
    a /= np.linalg.norm(a)
    b /= np.linalg.norm(b)
    if (a == b).all():
        return np.eye(3)
    rotation = R.align_vectors([a], [b])
    return rotation[0].as_matrix()

def extract_z_axis(ori):
    x, y, z, w = ori
    rotation_matrix = np.array([
        [1 - 2*y*y - 2*z*z, 2*x*y - 2*w*z, 2*x*z + 2*w*y],
        [2*x*y + 2*w*z, 1 - 2*x*x - 2*z*z, 2*y*z - 2*w*x],
        [2*x*z - 2*w*y, 2*y*z + 2*w*x, 1 - 2*x*x - 2*y*y]
    ])
    gripper_local_z = np.array([0, 0, 1])
    gripper_z_base = rotation_matrix.dot(gripper_local_z)
    gripper_z_base /= np.linalg.norm(gripper_z_base)
    return gripper_z_base

def quaternion_to_rotation_matrix(quaternion):
    x, y, z, w = quaternion
    rotation_matrix = np.array([
        [1 - 2*y**2 - 2*z**2, 2*x*y - 2*z*w, 2*x*z + 2*y*w],
        [2*x*y + 2*z*w, 1 - 2*x**2 - 2*z**2, 2*y*z - 2*x*w],
        [2*x*z - 2*y*w, 2*y*z + 2*x*w, 1 - 2*x**2 - 2*y**2]
    ])
    return rotation_matrix

def r_to_quat(r):
    rotation = R.from_matrix(r)
    quaternion = rotation.as_quat()
    return quaternion

def get_real_pose(pos, ori, rel_pos, rel_ori):
    ori_r = quaternion_to_rotation_matrix(ori)
    real_r = get_real_r(rel_ori, ori_r)
    real_ori = r_to_quat(real_r)
    real_pos = pos + rel_pos
    return (real_pos, real_ori)

def pose_to_mat(pose):
    homo_pose_mat = np.zeros((4, 4), dtype=np.float32)
    homo_pose_mat[:3, :3] = quat2mat(pose[1])
    homo_pose_mat[:3, 3] = np.array(pose[0], dtype=np.float32)
    homo_pose_mat[3, 3] = 1.
    return homo_pose_mat
    
def quat_to_euler(quat):
    assert quat.shape[-1] == 4
    x, y, z, w = quat[..., 0], quat[..., 1], quat[..., 2], quat[..., 3]
    ysqr = y * y
    t0 = +2.0 * (w * x + y * z)
    t1 = +1.0 - 2.0 * (x * x + ysqr)
    X = np.arctan2(t0, t1)
    t2 = +2.0 * (w * y - z * x)
    t2 = np.clip(t2, a_min=-1.0, a_max=1.0)
    Y = np.arcsin(t2)
    t3 = +2.0 * (w * z + x * y)
    t4 = +1.0 - 2.0 * (ysqr + z * z)
    Z = np.arctan2(t3, t4)
    return np.stack([X, Y, Z], axis=-1)
    
def add_euler(delta, source, degrees=False):
    delta_rot = R.from_euler('xyz', delta, degrees=degrees)
    source_rot = R.from_euler('xyz', source, degrees=degrees)
    new_rot = delta_rot * source_rot
    return new_rot.as_euler('xyz', degrees=degrees)

def euler_to_quat(euler):
    roll, pitch, yaw = euler
    qx = np.sin(roll/2) * np.cos(pitch/2) * np.cos(yaw/2) - np.cos(roll/2) * np.sin(pitch/2) * np.sin(yaw/2)
    qy = np.cos(roll/2) * np.sin(pitch/2) * np.cos(yaw/2) + np.sin(roll/2) * np.cos(pitch/2) * np.sin(yaw/2)
    qz = np.cos(roll/2) * np.cos(pitch/2) * np.sin(yaw/2) - np.sin(roll/2) * np.sin(pitch/2) * np.cos(yaw/2)
    qw = np.cos(roll/2) * np.cos(pitch/2) * np.cos(yaw/2) + np.sin(roll/2) * np.sin(pitch/2) * np.sin(yaw/2)
    
    return [qx, qy, qz, qw]

def quat_to_mat(euler):
    return R.from_euler("xyz", euler).as_quat()

def mat_to_quat(mat):
    return R.from_matrix(mat).as_quat()

def mat_to_euler(mat):
    return T.rmat_to_euler(mat, 'XYZ')

def quat_multiply(quat0, quat1):
    x0, y0, z0, w0 = np.split(quat0, 4, axis=-1)  # (..., 1) for each
    x1, y1, z1, w1 = np.split(quat1, 4, axis=-1)
    return np.concatenate(
        [
            x1 * w0 + y1 * z0 - z1 * y0 + w1 * x0,  # (..., 1)
            -x1 * z0 + y1 * w0 + z1 * x0 + w1 * y0,
            x1 * y0 - y1 * x0 + z1 * w0 + w1 * z0,
            -x1 * x0 - y1 * y0 - z1 * z0 + w1 * w0,
        ],
        axis=-1
    )

