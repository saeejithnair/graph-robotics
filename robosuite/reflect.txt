File: LLM/prompt.py

import os
import time
import openai
import json
import datetime
import numpy as np

class LLMPrompter():
    def __init__(self, gpt_version, api_key) -> None:
        self.gpt_version = gpt_version
        if api_key is None:
            raise ValueError("OpenAI API key is not provided.")
        else:
            openai.api_key = api_key

    def query(self, prompt: str, sampling_params: dict, save: bool, save_dir: str) -> str:
        while True:
            try:
                if 'gpt-4' in self.gpt_version:
                    response = openai.ChatCompletion.create(
                        model=self.gpt_version,
                        messages=[
                                {"role": "system", "content": prompt['system']},
                                {"role": "user", "content": prompt['user']},
                            ],
                        **sampling_params
                        )
                    # print("response: ", response)
                else:
                    response = openai.Completion.create(
                        model=self.gpt_version,
                        prompt=prompt,
                        **sampling_params
                    )
            except Exception as e:
                print("Request failed, sleep 2 secs and try again...", e)
                time.sleep(2)
                continue
            break

        if save:
            key = self.make_key()
            output = {}
            os.system('mkdir -p {}'.format(save_dir))
            if os.path.exists(os.path.join(save_dir, 'response.json')):
                with open(os.path.join(save_dir, 'response.json'), 'r') as f:
                    prev_response = json.load(f)
                    output = prev_response

            with open(os.path.join(save_dir, 'response.json'), 'w') as f:
                if 'gpt-4' in self.gpt_version:
                    output[key] = {
                                'prompt': prompt,
                                'sampling_params': sampling_params,
                                'response': response['choices'][0]['message']["content"].strip()
                            }
                else:
                    output[key] = {
                                'prompt': prompt,
                                'sampling_params': sampling_params,
                                'response': response['choices'][0]['text'].strip(),
                                'logprob': np.mean(response['choices'][0]['logprobs']['token_logprobs'])
                            }
                json.dump(output, f, indent=4)
            
        if 'gpt-4' in self.gpt_version:
            return response['choices'][0]['message']["content"].strip(), None
        else:
            return response['choices'][0]['text'].strip(), np.mean(response['choices'][0]['logprobs']['token_logprobs'])

    def make_key(self):
        return datetime.datetime.now().strftime("%Y%m%d-%H%M%S")


File: LLM/prompts.json

{
    "subgoal-verifier": {
        "template-system": "You are a success verifier that outputs 'Yes' or 'No' to indicate whether the robot goal is satisfied given the robot observations.",
        "template-user": "The robot goal is to [SUBGOAL]. Here are the robot observations after execution:\n[OBSERVATION]\nQ: Is the goal satisfied?\nA:",
        "params": {
            "max_tokens": 256,
            "temperature": 0.0,
            "top_p": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "stop": [",", "."]
        },
        "save": false
    },
    "reasoning-execution": {
        "template-system": "You are expected to provide explanation for a robot failure. You are given the robot actions and observations so far. Briefly explain the failure in 1-2 sentence. Mention relevant time steps if possible.",
        "template-user": "The robot task is to [TASK_NAME]. At [STEP], a failure was identified.\n\n[Robot actions and observations before [STEP]]\n[SUMMARY]\n[Observation at the end of [STEP]]\n[OBSERVATION]\nQ: Infer from [Robot actions and observations before [STEP]] or [Observation at the end of [STEP]], briefly explain what happened at [STEP] and what caused the failure.\nA:",
        "params": {
            "max_tokens": 256,
            "temperature": 0.0,
            "top_p": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0
        },
        "save": true
    },
    "reasoning-execution-steps": {
        "template-system": "",
        "template-user": "[FAILURE_REASON]\nQ: Extract time steps from the above sentence (separated by comma).\n A:",
        "params": {
            "max_tokens": 256,
            "temperature": 0.0,
            "top_p": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0
        },
        "save": false
    },
    "reasoning-execution-no-history": {
        "template-system": "You are expected to provide explanation for a robot failure. You are given robot observations so far. Briefly explain the failure in 1-2 sentence.",
        "template-user": "The robot task is to [TASK_NAME]. At [STEP], a failure was identified.\n\n[Observation at the end of [STEP]]\n[OBSERVATION]\nQ: Infer from [Observation at the end of [STEP]], briefly explain what happened at [STEP] and what caused the failure.\nA:",
        "params": {
            "max_tokens": 256,
            "temperature": 0.0,
            "top_p": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0
        },
        "save": true
    },
    "reasoning-plan": {
        "template-system": "You are expected to provide explanation for a robot failure. You are given the current robot state, the goal condition, and the robot plan. Briefly explain what was wrong with the robot plan in 1-2 sentence.",
        "template-user": "The robot task is to [TASK_NAME]. The task is considered successful if [SUCCESS_CONDITION]\nHere's the robot observation at the end of the task execution:\n[CURRENT_STATE]\nThe robot plan is:\n[OBSERVATION]\nQ: Known that all actions in the robot plan were executed successfully, what's wrong with the robot plan that caused the robot to fail?\nA:",
        "params": {
            "max_tokens": 256,
            "temperature": 0.0,
            "top_p": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0
        },
        "save": true
    },
    "reasoning-plan-steps": {
        "template-system": "Given the identified failure reason, you are expected to output an earliest time step that is relevant to the failure. You must output a time step.",
        "template-user": "[PREV_PROMPT]\nQ: Which time step is most relevant to the above failure?\nA:",
        "params": {
            "max_tokens": 256,
            "temperature": 0.0,
            "top_p": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "stop": [",", "."]
        },
        "save": false
    },
    "correction": {
        "template-system": "[PREFIX]",
        "template-user": "Task: [TASK_NAME]\nInitial plan:\n[PLAN]\nFailure reason: [FAILURE_REASON]\nCurrent state: [CURRENT_STATE]\nSuccess state: [SUCCESS_CONDITION]\nCorrection plan:",
        "params": {
            "max_tokens": 256,
            "temperature": 0.0,
            "top_p": 1,
            "frequency_penalty": 0.5,
            "presence_penalty": 0.3
        },
        "save": true
    }
}



File: README.md

# REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction
#### [Zeyi Liu*](https://lzylucy.github.io/), [Arpit Bahety*](https://arpitrf.github.io/), [Shuran Song](https://shurans.github.io/)

_Columbia University in the City of New York_


[Project Page](https://robot-reflect.github.io/) | [arXiV](https://arxiv.org/abs/2306.15724) | [Summary Video](https://www.youtube.com/watch?v=Proiua4NNTk) | [Dataset](https://www.cs.columbia.edu/~liuzeyi/reflect_data)

---
REFLECT is a framework that that leverages Large Language Models (LLMs) for robot failure explanation and correction, based on a summary of robot past experiences generated from multimodal sensory data. The framework is able to tackle both execution and planning failures across a veriety of tasks.

![Teaser](figs/teaser.png)

## Method
### 1. Hierarchical Summary of Robot Experiences
![Summary](figs/method-summary.jpg)
**Hierarchical Robot Summary** is composed of: a) a sensory-input summary that converts multi-sensory robot data (RGB-D, sound, robot states) into task-informed scene graphs and audio summary; b) an event-based summary that generates captions for key event frames; c) a subgoal-based summary that contains the end frame of each subgoal.

### 2. Progressive Failure Explanation with LLM
![Reasoning](figs/method-reasoning.jpg)
**Progressive Failure Explanation** verifies success for each subgoal. If a subgoal fails, the algorithm enters the *execution analysis* stage for detailed explanation on the execution. If all subgoals are satisfied, the algorithm enters *planning analysis* stage to check errors in the robot plan.

### 3. Failure Correction Planner
<img src="figs/method-correction.gif" width="460" height="260"/>

The **failure correction planner** generates an executable plan for the robot to recover from the failure and complete the task by taking in the task description, the original robot plan, the failure explanation, the final state of the original execution, and the expected goal state.


## Codebase
To get started, create a conda environment from the provided env.yml file:
```
conda env create -f env.yml
conda activate reflect_env
python -m pip install git+https://github.com/openai/CLIP.git
```

The dependencies have been tested on `Ubuntu 20.04.6 LTS, Python 3.9.13`.
We provide a [demo colab](https://github.com/columbia-ai-robotics/reflect/blob/main/demo.ipynb) that shows how one can generate failure data in the AI2THOR simulation and use the REFLECT framework to analyze and correct the failures.

Issues or pull requests are welcome!

## Extensions
Here are few suggestions for extending the framework:
1. Generate more failure scenarios (more tasks, more scenes, more failure types) in either simulation or real world.
2. Improve the current perception module by integrating latest vision/audio models that could increase accuracy or capture more low-level information about the environment.
- [Object detection] You may replace the ground truth object detection obtained from simulation with your own object detector [here](https://github.com/columbia-ai-robotics/reflect/blob/5736f88cade931bfcf670df82e38580d9c538771/main/get_local_sg.py#L38).
- [Object state] The codebase supports ground truth object states obtained from simulation and state detection using [CLIP](https://github.com/openai/CLIP). You may replace it with your own method [here](https://github.com/columbia-ai-robotics/reflect/blob/5736f88cade931bfcf670df82e38580d9c538771/main/scene_graph.py#L291).
- [Spatial relations] The codebase uses heuristics on object point clouds to compute inter-object spatial relations. You may replace it with your own method [here](https://github.com/columbia-ai-robotics/reflect/blob/5736f88cade931bfcf670df82e38580d9c538771/main/scene_graph.py#L246).
- [Audio] The codebase uses [WAV2CLIP](https://github.com/descriptinc/lyrebird-wav2clip) for audio detection. You may replace it with your own model [here](https://github.com/columbia-ai-robotics/reflect/blob/main/main/audio.py).
3. Integrate the framework with an existing policy.
4. More applications of the robot experience summary, such as safety or efficiency analysis of robot behaviors, or for human-robot collaboration.

Feel free to contact [Zeyi](https://lzylucy.github.io/) at `liuzeyi [at] stanford [dot] edu` if you have ideas to extend the framework or applying it on new tasks!

## Acknowledgement
Some functions of this codebase are adapted from the codebase of [Language Models as Zero-Shot Planners](https://github.com/huangwl18/language-planner) by Huang et.al and [Socratic Models](https://github.com/google-research/google-research/tree/master/socraticmodels) by Zeng et.al.

## Citation
If you find this codebase useful, feel free to cite our work!
<div style="display:flex;">
<div>

```bibtex
@article{liu2023reflect,
  title={REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction},
  author={Liu, Zeyi and Bahety, Arpit and Song, Shuran},
  journal={arXiv preprint arXiv:2306.15724},
  year={2023}
}
```



File: main/action_primitives.py

import time
import math
import numpy as np
from task_utils import *
from constants import *

def navigate_to_obj(taskUtil, obj_type, to_drop=False, failure_injection_idx=0, obj_id=None, replan=False, fail_execution=False):
    print("[INFO] Execute action: Navigate to", obj_type)
    obj_type_in_sim = obj_type
    if obj_type in NAME_MAP:
        obj_type_in_sim = NAME_MAP[obj_type]
    drop_failure_injected = False

    if taskUtil.chosen_failure == "wrong_perception":
        if obj_type == taskUtil.failure_injection_params['correct_obj_type']:
            obj_type = taskUtil.failure_injection_params['wrong_obj_type']

    if fail_execution:
        e = taskUtil.controller.last_event
        save_data(taskUtil, e, replan=replan)
        start_frame = taskUtil.counter + 1
        taskUtil.nav_actions[(start_frame, start_frame)] = 'Move to ' + obj_type_in_sim.lower()
        return False

    if obj_id is not None:
        obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["objectId"] == obj_id)
    elif '-' in obj_type:
        for key, val in taskUtil.unity_name_map.items():
            if val == obj_type:
                obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["name"] == key)
    else:
        obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["objectType"] == obj_type)

    # BFS search for poth
    closest_pos = closest_position(obj["position"], taskUtil.reachable_positions)
    robot_pos = taskUtil.controller.last_event.metadata['agent']['position']
    target_pos_val = [closest_pos['x'], closest_pos['z']]
    # print("robot_pos, target_pos, closest_to_target_pos: ", robot_pos, target_pos_val, closest_pos)
    taskUtil.grid_size = taskUtil.grid.shape[0]
    # calculate taskUtil.grid_index from taskUtil.grid_value
    for row in range(taskUtil.grid.shape[0]):
        for col in range(taskUtil.grid.shape[1]):
            if [round(robot_pos['x'], 2), round(robot_pos['z'], 2)] == [taskUtil.grid[row, col, 0], taskUtil.grid[row, col, 1]]:
                robot_x = row
                robot_y = col
            if [round(target_pos_val[0], 2), round(target_pos_val[1], 2)] == [taskUtil.grid[row, col, 0], taskUtil.grid[row, col, 1]]:
                target_x = row
                target_y = col
    target_pos = [target_x, target_y]
    # print("*** start, goal: ", robot_x, robot_y, target_pos)
    path = findPath(taskUtil.grid, x=robot_x, y=robot_y, target_pos=target_pos, reachable_points=taskUtil.reachable_points)
    # print("path: ", path)

    if path is None:
        print("[ERROR] No valid path is found from robot to target object")
        save_data(taskUtil, taskUtil.controller.last_event, replan=replan)
        taskUtil.controller.step(action="Done")
        return

    start_frame = taskUtil.counter + 1
    for p in path:
        x = taskUtil.grid[p.x,p.y][0]
        z = taskUtil.grid[p.x,p.y][1]
        y = 0.9
        # print("p: ", p, x, z)
        e = taskUtil.controller.step(
            action="Teleport",
            position=dict(x=x, y=y, z=z),
            forceAction=True,
            # horizon=30,
            standing=True
        )
        # print("e: ", e)
        save_data(taskUtil, e, replan=replan)
        taskUtil.controller.step(action="Done")
        # dropping injection
        obj_in_hand = False
        for o in taskUtil.controller.last_event.metadata['objects']:
            if o['isPickedUp'] == True:
                obj_in_hand = True
                break
        add_failure = np.random.uniform()
        if not taskUtil.failure_added and taskUtil.chosen_failure == 'drop' and obj_in_hand and add_failure > 0.5 and to_drop:
            # drop action primitive
            print("injected drop at step", taskUtil.counter)
            drop(taskUtil, failure_injection_idx)
            drop_failure_injected = True
    robot_pos = taskUtil.controller.last_event.metadata['agent']['position']
    look_at(taskUtil, target_pos=obj["position"], robot_pos=robot_pos, replan=replan)
    end_frame = taskUtil.counter
    taskUtil.nav_actions[(start_frame, end_frame)] = 'Move to ' + obj_type_in_sim.lower()
    taskUtil.controller.step(action="Done")
    if to_drop:
        return drop_failure_injected
    else:
        return True


def pick_up(taskUtil, obj_type, fail_execution=False, replan=False):
    print("[INFO] Execute action: Picking up", obj_type)
    obj_type_in_sim = obj_type
    if obj_type in NAME_MAP:
        obj_type_in_sim = NAME_MAP[obj_type]

    if taskUtil.chosen_failure == "wrong_perception":
        if obj_type == taskUtil.failure_injection_params['correct_obj_type']:
            obj_type = taskUtil.failure_injection_params['wrong_obj_type']

    # if the Sliced/Cracked object does not exist, then pick up the original object
    # if the original object does not exist, then pick up the Sliced/Cracked object
    obj_types = sorted([obj["objectType"] for obj in taskUtil.controller.last_event.metadata["objects"]])
    if obj_type in OBJ_UNSLICED_MAP:
        obj_unsliced_type = OBJ_UNSLICED_MAP[obj_type]
        if obj_unsliced_type in obj_types and obj_type not in obj_types:
            obj_type = obj_unsliced_type
    elif obj_type in OBJ_SLICED_MAP:
        obj_sliced_type = OBJ_SLICED_MAP[obj_type]
        if obj_sliced_type in obj_types:
            obj_type = obj_sliced_type

    e = taskUtil.controller.last_event
    objs = [obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["objectType"] == obj_type]
    
    # thor specific - to avoid picking up the largest slice (e.g. Lettuce_6_Slice_8 is smallest and Lettuce_6_Slice_1 is the largest)
    if 'LettuceSliced' in obj_type or 'AppleSliced' in obj_type:
        objs = sorted(objs, key=lambda x: int(x['name'].split('_')[-1])*-1)
    if 'PotatoSliced' in obj_type:
        objs = objs[2:]
    if 'TomatoSliced' in obj_type:
        objs = objs[4:]
    if "EggCracked" in obj_type:
        objs = ""
    
    if fail_execution == True or len(objs) == 0:
        if len(objs) == 0:
            print("Cannot find the target object to pick up")
        save_data(taskUtil, e, replan=replan)
        taskUtil.interact_actions[taskUtil.counter] = "Pick up " + obj_type_in_sim.lower()
        return

    # if navigation is required
    if not objs[0]['visible'] and objs[0]['objectType'] not in taskUtil.objs_w_unk_loc:
        navigate_to_obj(taskUtil, objs[0]['objectType'], replan=replan)

    if (taskUtil.chosen_failure == 'blocking' and taskUtil.failure_injection_params['src_obj_type'] == obj_type) \
        and obj_is_blocked(taskUtil, obj_type) or (taskUtil.chosen_failure == "drop" and taskUtil.failure_added is True and replan is False):
        save_data(taskUtil, e, replan=replan)
        taskUtil.interact_actions[taskUtil.counter] = "Pick up " + obj_type_in_sim.lower()
        return
       
    for obj in objs:
        obj_id = obj['objectId']
        obj_pos = obj['position']
        # look at object
        robot_pos = taskUtil.controller.last_event.metadata['agent']['position']
        look_at(taskUtil, target_pos=obj_pos, robot_pos=robot_pos, replan=replan)
        e = taskUtil.controller.step(
            action="PickupObject",
            objectId=obj_id,
            forceAction=False,
            manualInteract=False
        )
        #print("PickUpObject: ", e)
        if replan:
            reasoning_info = taskUtil.get_reasoning_json()
            if 'drop' in reasoning_info['pred_failure_reason'] and reasoning_info['gt_failure_step'] in reasoning_info['pred_failure_step']:
                e = taskUtil.controller.step(
                    action="PickupObject",
                    objectId=obj_id,
                    forceAction=True,
                    manualInteract=False
                )
                #print("PickUpDroppedObject: ", e)
            if e.metadata['lastActionSuccess']:
                break
    save_data(taskUtil, e, replan=replan)
    taskUtil.interact_actions[taskUtil.counter] = "Pick up " + obj_type_in_sim.lower()
    taskUtil.controller.step(action="Done")
    time.sleep(1)


def put_in(taskUtil, src_obj_type, target_obj_type, fail_execution=False, replan=False):
    print(f"[INFO] Execute action: Putting {src_obj_type} in {target_obj_type}")
    src_obj_type_in_sim = src_obj_type
    if src_obj_type in NAME_MAP:
        src_obj_type_in_sim= NAME_MAP[src_obj_type]
    target_obj_type_in_sim = target_obj_type
    if target_obj_type in NAME_MAP:
        target_obj_type_in_sim = NAME_MAP[target_obj_type]
    
    if taskUtil.chosen_failure == "wrong_perception":
        if src_obj_type == taskUtil.failure_injection_params['correct_obj_type']:
            src_obj_type = taskUtil.failure_injection_params['wrong_obj_type']
        elif target_obj_type == taskUtil.failure_injection_params['correct_obj_type']:
            target_obj_type = taskUtil.failure_injection_params['wrong_obj_type']

    src_obj = None
    for obj in taskUtil.controller.last_event.metadata["objects"]:
        if obj['isPickedUp']:
            src_obj = obj
            break
    if src_obj is None:
        print("The robot is not holding anything")
    elif src_obj['objectType'] != src_obj_type:
        print(f"The robot is not holding {src_obj_type}")
    else:
        print("The robot is holding:", src_obj['objectId'], src_obj['objectType'])
    
    if fail_execution or src_obj is None:
        e = taskUtil.controller.last_event
        save_data(taskUtil, e, replan=replan)
        taskUtil.interact_actions[taskUtil.counter] = "Put " + src_obj_type_in_sim.lower() + " inside " + target_obj_type_in_sim.lower()
        return

    # thor-specific, put in sink sometimes does not work as expected
    if target_obj_type == 'Sink':
        target_obj_type = 'SinkBasin'
  
    #if there are multiple instances
    found_obj = False
    for obj_unity_name, v in taskUtil.unity_name_map.items():
        if v == target_obj_type:
            target_obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["name"] == obj_unity_name)
            found_obj = True
            break
    if not found_obj:
        target_obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["objectType"] == target_obj_type)
    target_obj_id = target_obj['objectId']
    target_obj_pos = target_obj['position']

    # if navigation is required
    if not target_obj['visible'] and target_obj['objectType'] not in taskUtil.objs_w_unk_loc:
        navigate_to_obj(taskUtil, target_obj['objectType'], obj_id=target_obj['objectId'], replan=replan)

    # look at object
    robot_pos = taskUtil.controller.last_event.metadata['agent']['position']
    look_at(taskUtil, target_pos=target_obj_pos, robot_pos=robot_pos, replan=replan)

    # can only put one object in microwave
    if target_obj_type == 'Microwave' and len(target_obj['receptacleObjectIds']) > 0:
        print("Microwave already contains an object: ", target_obj['receptacleObjectIds'])
        e = taskUtil.controller.last_event
        save_data(taskUtil, e, replan=replan)
        taskUtil.interact_actions[taskUtil.counter] = "Put " + src_obj_type_in_sim.lower() + " inside " + target_obj_type_in_sim.lower()
        return
    
    if target_obj_type == 'Toaster' and target_obj['isToggled']:
        place_obj_in_small_receptacle(taskUtil, target_obj_pos, replan=replan)
    else:
        if src_obj:
            e = taskUtil.controller.step(
                action="PutObject",
                objectId=target_obj_id,
                forceAction=False,
                placeStationary=True
            )
            if e.metadata['lastActionSuccess']:
                save_data(taskUtil, e, replan=replan)
                taskUtil.controller.step(action="Done")
                time.sleep(1)
            else:
                print("thor put_obj did not work, try place obj in small recetacle primitive")
                if target_obj_type in ["CoffeeMachine", "Microwave"]:
                    save_data(taskUtil, e, replan=replan)
                else:
                    place_obj_in_small_receptacle(taskUtil, target_obj_pos, replan=replan)
 
    taskUtil.interact_actions[taskUtil.counter] = "Put " + src_obj_type_in_sim.lower() + " inside " + target_obj_type_in_sim.lower()


def put_on(taskUtil, src_obj_type, target_obj_type, fail_execution=False, target_obj_id=None, replan=False):
    print(f"[INFO] Execute action: Putting {src_obj_type} on {target_obj_type}")
    src_obj_type_in_sim = src_obj_type
    if src_obj_type in NAME_MAP:
        src_obj_type_in_sim = NAME_MAP[src_obj_type]
    if taskUtil.chosen_failure == 'ambiguous_plan' and target_obj_type.split("-")[0] == taskUtil.failure_injection_params['ambi_obj_type']:
        target_obj_type_in_sim = target_obj_type.split('-')[0]
        if target_obj_type_in_sim in NAME_MAP:
            target_obj_type_in_sim = NAME_MAP[target_obj_type_in_sim]
    else:
        target_obj_type_in_sim = target_obj_type
        if target_obj_type in NAME_MAP:
            target_obj_type_in_sim = NAME_MAP[target_obj_type]

    if taskUtil.chosen_failure == "wrong_perception":
        if src_obj_type == taskUtil.failure_injection_params['correct_obj_type']:
            src_obj_type = taskUtil.failure_injection_params['wrong_obj_type']
        elif target_obj_type == taskUtil.failure_injection_params['correct_obj_type']:
            target_obj_type = taskUtil.failure_injection_params['wrong_obj_type']
    
    src_obj = None
    for obj in taskUtil.controller.last_event.metadata["objects"]:
        if obj['isPickedUp']:
            src_obj = obj
            break
    if src_obj is None:
        print("The robot is not holding anything")
    elif src_obj['objectType'] != src_obj_type:
        print(f"The robot is not holding {src_obj_type}")
    else:
        print("The robot is holding: ", src_obj['objectId'], src_obj['objectType'])

    e = taskUtil.controller.last_event
    if fail_execution or src_obj is None or src_obj['objectType'] != src_obj_type:
        save_data(taskUtil, e, replan=replan)
        if target_obj_type_in_sim.lower() == "sink":
            taskUtil.interact_actions[taskUtil.counter] = "Put " + src_obj_type_in_sim.lower() + " inside " + target_obj_type_in_sim.lower()
        else:
            taskUtil.interact_actions[taskUtil.counter] = "Put " + src_obj_type_in_sim.lower() + " on " + target_obj_type_in_sim.lower()
        return

    if target_obj_id is None:
        if "-" in target_obj_type and target_obj_type.split("-")[0] in ['StoveBurner', 'CounterTop']:
            for key, val in taskUtil.unity_name_map.items():
                if val == target_obj_type:
                    target_obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["name"] == key)
                    break
        else:
            target_obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["objectType"] == target_obj_type)
    # if the exact object instance is specified
    else:
        target_obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["objectId"] == target_obj_id)

    target_obj_id = target_obj['objectId']
    target_obj_pos = target_obj['position']
    if target_obj['objectType'] not in taskUtil.objs_w_unk_loc:
        navigate_to_obj(taskUtil, target_obj['objectType'], replan=replan)
    
    # look at object
    robot_pos = taskUtil.controller.last_event.metadata['agent']['position']
    look_at(taskUtil, target_pos=target_obj_pos, robot_pos=robot_pos, replan=replan)
    e = taskUtil.controller.step(
        action="PutObject",
        objectId=target_obj_id,
        forceAction=False,
        placeStationary=True
    )
    
    # if not successful, try standing
    if not e.metadata['lastActionSuccess']:
        taskUtil.controller.step(action="Stand")
        e = taskUtil.controller.step(
            action="PutObject",
            objectId=target_obj_id,
            forceAction=False,
            placeStationary=True
        )
    taskUtil.controller.step(action="Done")

    # print("PutObject: ", e)
    save_data(taskUtil, e, replan=replan)

    # if still not successful, try pre-defined primitives
    if not e.metadata['lastActionSuccess']:
        print("thor put_obj did not work, applying self-defined primitives")
        if taskUtil.folder_name.split("/")[1] == 'heatPotato-3':
            taskUtil.controller.step(
                action="MoveHeldObjectAhead",
                moveMagnitude=0.4,
                forceVisible=False
            )
            e = taskUtil.controller.step(
                action="DropHandObject",
                forceAction=False
            )
        if target_obj_type.split("-")[0] == 'CounterTop':
            place_obj_on_large_receptacle(taskUtil, src_obj, target_obj_type, target_obj_id=target_obj_id, replan=replan)
    else:
        new_src_obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["objectId"] == src_obj['objectId'])
        look_at(taskUtil, target_pos=new_src_obj['position'], robot_pos=robot_pos, replan=replan)
        
    if target_obj_type_in_sim.lower() == "sink":
        taskUtil.interact_actions[taskUtil.counter] = "Put " + src_obj_type_in_sim.lower() + " inside " + target_obj_type_in_sim.lower()
    else:
        taskUtil.interact_actions[taskUtil.counter] = "Put " + src_obj_type_in_sim.lower() + " on " + target_obj_type_in_sim.lower()
    time.sleep(1)
    

def toggle_on(taskUtil, obj_type, fail_execution=False, replan=False):
    print("[INFO] Execute action: Toggling on", obj_type)
    e = taskUtil.controller.last_event
    if taskUtil.chosen_failure == 'ambiguous_plan' and obj_type.split("-")[0] == taskUtil.failure_injection_params['ambi_obj_type']:
        obj_type_in_sim = obj_type.split('-')[0]
        if obj_type_in_sim in NAME_MAP:
            obj_type_in_sim = NAME_MAP[obj_type_in_sim]
    else:
        obj_type_in_sim = obj_type
        if obj_type in NAME_MAP:
            obj_type_in_sim = NAME_MAP[obj_type]

    if taskUtil.chosen_failure == "wrong_perception":
        if obj_type == taskUtil.failure_injection_params['correct_obj_type']:
            obj_type = taskUtil.failure_injection_params['wrong_obj_type']
    
    if fail_execution:
        e = taskUtil.controller.last_event
        save_data(taskUtil, e, replan=replan)
        taskUtil.interact_actions[taskUtil.counter] = "Toggle on " + obj_type_in_sim.lower()
        return

    found_obj = False
    for obj_unity_name, v in taskUtil.unity_name_map.items():
        if v == obj_type:
            obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["name"] == obj_unity_name)
            found_obj = True
            break
    if not found_obj:
        obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["objectType"] == obj_type)
    obj_id = obj['objectId']

    # if navigation is required
    if not obj['visible'] and obj['objectType'] not in taskUtil.objs_w_unk_loc:
        navigate_to_obj(taskUtil, obj['objectType'], obj_id=obj['objectId'], replan=replan)
    else:
        # look at object
        robot_pos = taskUtil.controller.last_event.metadata['agent']['position']
        look_at(taskUtil, target_pos=obj['position'], robot_pos=robot_pos, replan=replan)
    
    # retrieve the stove knob corresponding to the chosen stove burner
    if "StoveBurner" in obj_type:
        for o in taskUtil.controller.last_event.metadata["objects"]:
            if 'StoveKnob' in o['objectType'] and o['controlledObjects'] is not None \
                    and o['controlledObjects'][0] == obj_id:
                obj_id = o['objectId']

    execute_action = True
    if obj_type == 'Television':
        remote_control_obj = [obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["objectType"] == 'RemoteControl']
        if len(remote_control_obj) > 0 and not remote_control_obj[0]['isPickedUp']:
            execute_action = False
   
    if execute_action:
        e = taskUtil.controller.step(
            action="ToggleObjectOn",
            objectId=obj_id,
            forceAction=True
        )
        # print("ToggleObjectOn: ", e)
    save_data(taskUtil, e, replan=replan)
    taskUtil.interact_actions[taskUtil.counter] = "Toggle on " + obj_type_in_sim.lower()
    taskUtil.controller.step(action="Done")
    # add sound to dict if action success
    if execute_action and e.metadata["lastActionSuccess"] and ("Toggle on " + obj_type.split('-')[0]) in SOUND_PATH:
        taskUtil.sounds[taskUtil.counter-1] = SOUND_PATH["Toggle on " + obj_type.split('-')[0]]
    time.sleep(1)

    # Post-processing for cleaning and filling up with water (need to do this as thor put_in primitive does not put the object directly below the faucet)
    faucet_objs = [o for o in taskUtil.controller.last_event.metadata["objects"] if o["objectType"] == "Faucet"]
    src_obj = next(o for o in taskUtil.controller.last_event.metadata["objects"] if o["objectId"] == obj['objectId'])
    if src_obj['isToggled']:
        # if single faucet:
        if len(faucet_objs) == 1:
            for obj in taskUtil.controller.last_event.metadata["objects"]:
                parentReceptacles = obj['parentReceptacles']
                if parentReceptacles is not None:
                    for parent in parentReceptacles:
                        # change state for object in sink
                        if "Sink" in parent:
                            e = taskUtil.controller.step(
                                action="CleanObject",
                                objectId=obj['objectId'],
                                forceAction=True
                            )
                            e = taskUtil.controller.step(
                                action="FillObjectWithLiquid",
                                objectId=obj['objectId'],
                                fillLiquid='water',
                                forceAction=True
                            )
        # if multiple faucets:
        else:
            src_obj_parent_receptacle = [p for p in src_obj['parentReceptacles'] if "SinkBasin" in p] 
            for obj in taskUtil.controller.last_event.metadata["objects"]:
                parentReceptacles = obj['parentReceptacles']
                if parentReceptacles is not None:
                    for parent in parentReceptacles:
                        if parent in src_obj_parent_receptacle:
                            e = taskUtil.controller.step(
                                action="CleanObject",
                                objectId=obj['objectId'],
                                forceAction=True
                            )
                            e = taskUtil.controller.step(
                                action="FillObjectWithLiquid",
                                objectId=obj['objectId'],
                                fillLiquid='water',
                                forceAction=True
                            )


def toggle_off(taskUtil, obj_type, fail_execution=False, replan=False):
    print(f"[INFO] Execute action: Toggling off", obj_type)
    e = taskUtil.controller.last_event
    obj_type_in_sim = obj_type
    if obj_type in NAME_MAP:
        obj_type_in_sim = NAME_MAP[obj_type]

    if taskUtil.chosen_failure == "wrong_perception":
        if obj_type == taskUtil.failure_injection_params['correct_obj_type']:
            obj_type = taskUtil.failure_injection_params['wrong_obj_type']

    if fail_execution:
        e = taskUtil.controller.last_event
        save_data(taskUtil, e, replan=replan)
        taskUtil.interact_actions[taskUtil.counter] = "Toggle off " + obj_type_in_sim.lower()
        return
    
    found_obj = False
    for obj_unity_name, v in taskUtil.unity_name_map.items():
        if v == obj_type:
            obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["name"] == obj_unity_name)
            found_obj = True
            break
    if not found_obj:
        obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["objectType"] == obj_type)
    obj_id = obj['objectId']
    
    # if navigation is required
    if not obj['visible'] and obj['objectType'] not in taskUtil.objs_w_unk_loc:
        navigate_to_obj(taskUtil, obj['objectType'], obj_id=obj['objectId'], replan=replan)
    else:
        # look at object
        robot_pos = taskUtil.controller.last_event.metadata['agent']['position']
        look_at(taskUtil, target_pos=obj['position'], robot_pos=robot_pos, replan=replan)
    
    # retrieve the stove knob corresponding to the chosen stove burner
    if "StoveBurner" in obj_type:
        for o in taskUtil.controller.last_event.metadata["objects"]:
            if 'StoveKnob' in o['objectType'] and o['controlledObjects'] is not None \
                    and o['controlledObjects'][0] == obj_id:
                obj_id = o['objectId']

    execute_action = True
    # should hold remote control to open TV
    if obj_type == 'Television':
        remote_control_obj = [obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["objectType"] == 'RemoteControl']
        if len(remote_control_obj) > 0 and not remote_control_obj[0]['isPickedUp']:
            execute_action = False
    
    if execute_action:
        e = taskUtil.controller.step(
            action="ToggleObjectOff",
            objectId=obj_id,
            forceAction=False
        )
        # print("ToggleObjectOff: ", e)
    save_data(taskUtil, e, replan=replan)
    taskUtil.interact_actions[taskUtil.counter] = "Toggle off " + obj_type_in_sim.lower()
    taskUtil.controller.step(action="Done")
    # add sound to dict
    if execute_action and e.metadata["lastActionSuccess"] and ("Toggle off " + obj_type.split('-')[0]) in SOUND_PATH:
        taskUtil.sounds[taskUtil.counter-1] = SOUND_PATH["Toggle off " + obj_type.split('-')[0]]
    time.sleep(1)


def open_obj(taskUtil, obj_type, fail_execution=False, replan=False):
    print("[INFO] Execute action: Opening", obj_type)
    src_obj_type_in_sim = obj_type
    if obj_type in NAME_MAP:
        obj_type_in_sim = NAME_MAP[obj_type]

    if taskUtil.chosen_failure == "wrong_perception":
        if obj_type == taskUtil.failure_injection_params['correct_obj_type']:
            obj_type = taskUtil.failure_injection_params['wrong_obj_type']
    
    if fail_execution:
        e = taskUtil.controller.last_event
        save_data(taskUtil, e, replan=replan)
        taskUtil.interact_actions[taskUtil.counter] = "Open " + obj_type_in_sim.lower()
        return

    found_obj = False
    for obj_unity_name, v in taskUtil.unity_name_map.items():
        if v == obj_type:
            obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["name"] == obj_unity_name)
            found_obj = True
            break
    if not found_obj:
        obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["objectType"] == obj_type)
    obj_id = obj['objectId']
    
    # if navigation is required
    if not obj['visible'] and obj['objectType'] not in taskUtil.objs_w_unk_loc:
        navigate_to_obj(taskUtil, obj['objectType'], obj_id=obj['objectId'], replan=replan)
    else:
        # look at object
        robot_pos = taskUtil.controller.last_event.metadata['agent']['position']
        look_at(taskUtil, target_pos=obj['position'], robot_pos=robot_pos, replan=replan)

    e = taskUtil.controller.step(
        action="OpenObject",
        objectId=obj_id,
        forceAction=False
    )
    # print("Open object: ", e)
        
    save_data(taskUtil, e, replan=replan)
    taskUtil.interact_actions[taskUtil.counter] = "Open " + src_obj_type_in_sim.lower()
    taskUtil.controller.step(action="Done")
    # if e.metadata["lastActionSuccess"] and ("Open " + obj_type.split('-')[0]) in SOUND_PATH:
    #     taskUtil.sounds[taskUtil.counter-1] = SOUND_PATH["Open " + obj_type.split('-')[0]]
    time.sleep(1)


def close_obj(taskUtil, obj_type, fail_execution=False, replan=False):
    print("[INFO] Execute action: Closing", obj_type)
    obj_type_in_english = obj_type
    if obj_type in NAME_MAP:
        obj_type_in_english = NAME_MAP[obj_type]

    if taskUtil.chosen_failure == "wrong_perception":
        if obj_type == taskUtil.failure_injection_params['correct_obj_type']:
            obj_type = taskUtil.failure_injection_params['wrong_obj_type']

    if fail_execution:
        e = taskUtil.controller.last_event
        save_data(taskUtil, e, replan=replan)
        taskUtil.interact_actions[taskUtil.counter] = "Close " + obj_type_in_english.lower()
        return
    
    found_obj = False
    for obj_unity_name, v in taskUtil.unity_name_map.items():
        if v == obj_type:
            obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["name"] == obj_unity_name)
            found_obj = True
            break
    if not found_obj:
        obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["objectType"] == obj_type)
    obj_id = obj['objectId']
    
    # if navigation is required
    if not obj['visible'] and obj['objectType'] not in taskUtil.objs_w_unk_loc:
        navigate_to_obj(taskUtil, obj['objectType'], obj_id=obj['objectId'], replan=replan)
    else:
        # look at object
        robot_pos = taskUtil.controller.last_event.metadata['agent']['position']
        look_at(taskUtil, target_pos=obj['position'], robot_pos=robot_pos, replan=replan)
    
    e = taskUtil.controller.step(
        action="CloseObject",
        objectId=obj_id,
        forceAction=False
    )
    # print("Close object: ", e)
    save_data(taskUtil, e, replan=replan)
    taskUtil.interact_actions[taskUtil.counter] = "Close " + obj_type_in_english.lower()
    taskUtil.controller.step(action="Done")
    # if e.metadata["lastActionSuccess"] and ("Close " + obj_type.split('-')[0]) in SOUND_PATH:
    #     taskUtil.sounds[taskUtil.counter-1] = SOUND_PATH["Close " + obj_type.split('-')[0]]
    time.sleep(1)


def slice_obj(taskUtil, obj_type, fail_execution=False, replan=False):
    print("[INFO] Execute action: Slicing", obj_type)
    obj_type_in_sim = obj_type
    if obj_type in NAME_MAP:
        obj_type_in_sim = NAME_MAP[obj_type]

    if taskUtil.chosen_failure == "wrong_perception":
        if obj_type == taskUtil.failure_injection_params['correct_obj_type']:
            obj_type = taskUtil.failure_injection_params['wrong_obj_type']

    e = taskUtil.controller.last_event
    if fail_execution:
        save_data(taskUtil, e, replan=replan)
        taskUtil.interact_actions[taskUtil.counter] = "Slice " + obj_type_in_sim.lower()
        return
    
    knife_obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["objectType"] == 'Knife')
    
    obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["objectType"] == obj_type)
    obj_id = obj['objectId']
    obj_pos = obj['position']

    # if navigation is required
    if not obj['visible'] and obj['objectType'] not in taskUtil.objs_w_unk_loc:
        navigate_to_obj(taskUtil, obj['objectType'], replan=replan)
    
    if knife_obj['isPickedUp']:
        robot_pos = taskUtil.controller.last_event.metadata['agent']['position']
        look_at(taskUtil, target_pos=obj_pos, robot_pos=robot_pos, replan=replan)
        obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["objectType"] == obj_type)
        obj_id = obj['objectId']
        e = taskUtil.controller.step(
            action="SliceObject",
            objectId=obj_id,
            forceAction=False
        )
        save_data(taskUtil, e, replan=replan)
        taskUtil.interact_actions[taskUtil.counter] = "Slice " + obj_type_in_sim.lower()
        taskUtil.controller.step(action="Done")
        if e.metadata["lastActionSuccess"] and ("Slice " + obj_type.split('-')[0]) in SOUND_PATH:
            taskUtil.sounds[taskUtil.counter-1] = SOUND_PATH["Slice " + obj_type.split('-')[0]]
    else:
        save_data(taskUtil, e, replan=replan)
        taskUtil.interact_actions[taskUtil.counter] = "Slice " + obj_type_in_sim.lower()

# Primitive 10
def crack_obj(taskUtil, obj_type, fail_execution=False, replan=False):
    obj_type_in_sim = obj_type
    if obj_type in NAME_MAP:
        obj_type_in_sim = NAME_MAP[obj_type]

    if taskUtil.chosen_failure == "wrong_perception":
        if obj_type == taskUtil.failure_injection_params['correct_obj_type']:
            obj_type = taskUtil.failure_injection_params['wrong_obj_type']

    obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["objectType"] == obj_type)
    # skip the action if failure is injected or the object is not picked up by the robot
    if fail_execution or not obj['isPickedUp']:
        e = taskUtil.controller.last_event
        save_data(taskUtil, e, replan=replan)
        taskUtil.interact_actions[taskUtil.counter] = "Crack " + obj_type_in_sim.lower()
        return

    obj_id = obj['objectId']
    e = taskUtil.controller.step(
        action="BreakObject",
        objectId=obj_id,
        forceAction=False
    )
    print("BreakObject: ", e)

    if e.metadata["lastActionSuccess"] and ("Crack " + obj_type.split('-')[0]) in SOUND_PATH:
        taskUtil.sounds[taskUtil.counter] = SOUND_PATH["Crack " + obj_type.split('-')[0]]

    # after cracking, the cracked object is dropped in thor. So, need to pick it up again
    obj_types = sorted([obj["objectType"] for obj in taskUtil.controller.last_event.metadata["objects"]])
    if obj_type in OBJ_SLICED_MAP:
        obj_slice_type = OBJ_SLICED_MAP[obj_type]
        if obj_slice_type in obj_types:
            obj_type = obj_slice_type
    obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["objectType"] == obj_type)
    obj_id = obj['objectId']
    e = taskUtil.controller.step(
            action="PickupObject",
            objectId=obj_id,
            forceAction=True,
            manualInteract=False
        )
    print("PickUpObject (cracked): ", e)
    save_data(taskUtil, e, replan=replan)
    taskUtil.interact_actions[taskUtil.counter] = "Crack " + obj_type_in_sim.lower()  
    taskUtil.controller.step(action="Done")
    time.sleep(1)


def pour(taskUtil, src_obj_type, target_obj_type, fail_execution=False, replan=False):
    print(f"[INFO] Execute action: Pouring liquid from {src_obj_type} to {target_obj_type}")
    liquid_type = None
    src_obj_type_in_sim = src_obj_type
    if src_obj_type in NAME_MAP:
        src_obj_type_in_sim = NAME_MAP[src_obj_type]
    target_obj_type_in_sim = target_obj_type
    if target_obj_type in NAME_MAP:
        target_obj_type_in_sim = NAME_MAP[target_obj_type]

    if taskUtil.chosen_failure == "wrong_perception":
        if src_obj_type == taskUtil.failure_injection_params['correct_obj_type']:
            src_obj_type = taskUtil.failure_injection_params['wrong_obj_type']
        elif target_obj_type == taskUtil.failure_injection_params['correct_obj_type']:
            target_obj_type = taskUtil.failure_injection_params['wrong_obj_type']

    target_obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["objectType"] == target_obj_type)
    target_obj_id = target_obj['objectId']
    src_obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["objectType"] == src_obj_type)
    src_obj_id = src_obj['objectId']

    # if navigation is required
    if not target_obj['visible'] and target_obj['objectType'] not in taskUtil.objs_w_unk_loc:
        navigate_to_obj(taskUtil, target_obj['objectType'], replan=replan)

    # if obj_in_hand is True and obj_in_hand has liquid
    obj_in_hand = None
    for obj in taskUtil.controller.last_event.metadata['objects']:
        if obj['isPickedUp'] == True:
            obj_in_hand = obj
            break
    
    if obj_in_hand is not None and obj_in_hand["isFilledWithLiquid"] and src_obj_id == obj_in_hand['objectId']:
        liquid_type = obj_in_hand['fillLiquid']

        if fail_execution:
            e = taskUtil.controller.last_event
            save_data(taskUtil, e, replan=replan)
            # if liquid_type is not None:
            #     taskUtil.interact_actions[taskUtil.counter] = f"Pour {liquid_type.lower()} from " + src_obj_type_in_sim.lower() + " into " + target_obj_type_in_sim.lower() 
            # else:
            #     taskUtil.interact_actions[taskUtil.counter] = f"Pour liquid from {src_obj_type_in_sim.lower()} into {target_obj_type_in_sim.lower()}"
            return
    
        e = taskUtil.controller.step(
            action="EmptyLiquidFromObject",
            objectId=src_obj_id,
            forceAction=False
        )
        e = taskUtil.controller.step(
            action="FillObjectWithLiquid",
            objectId=target_obj_id,
            fillLiquid=liquid_type.lower(),
            forceAction=False
        )
        # print("FillWithLiquid: ", e)
        save_data(taskUtil, e, replan=replan)
        # if liquid_type is not None:
        #     taskUtil.interact_actions[taskUtil.counter] = f"Pour {liquid_type.lower()} from " + src_obj_type_in_sim.lower() + " into " + target_obj_type_in_sim.lower() 
        # else:
        #     taskUtil.interact_actions[taskUtil.counter] = f"Pour liquid from {src_obj_type_in_sim.lower()} into {target_obj_type_in_sim.lower()}"
        taskUtil.controller.step(action="Done")

    if liquid_type is not None and e.metadata["lastActionSuccess"] and f"Pour {liquid_type.lower()} into {target_obj_type.split('-')[0]}" in SOUND_PATH:
        taskUtil.sounds[taskUtil.counter-1] = SOUND_PATH[f"Pour {liquid_type.lower()} into {target_obj_type.split('-')[0]}"]
    time.sleep(1)


def drop(taskUtil, failure_injection_idx):
    obj_in_hand = None
    for obj in taskUtil.controller.last_event.metadata['objects']:
        if obj['isPickedUp'] == True:
            obj_in_hand = obj
            break
    if obj_in_hand is not None:
        e = taskUtil.controller.step(
            action="DropHandObject",
            forceAction=True
        )
        if e.metadata["lastActionSuccess"] and ("Drop " + obj_in_hand['objectType'].split('-')[0]) in SOUND_PATH:
            taskUtil.sounds[taskUtil.counter] = SOUND_PATH["Drop " + obj_in_hand['objectType'].split('-')[0]]
        # print("DropHandObject: ", e)
        obj_id = obj_in_hand['objectId']
        if obj_in_hand['objectType'] == "Egg":
            e = taskUtil.controller.step(
                action="BreakObject",
                objectId=obj_id,
                forceAction=False
            )
        # if the object is a container and filled with liquid, empty it
        if obj_in_hand['isFilledWithLiquid']:
            e = taskUtil.controller.step(
                action="EmptyLiquidFromObject",
                objectId=obj_id,
                forceAction=True
            )
        taskUtil.gt_failure['gt_failure_reason'] = "Dropped " + obj_in_hand['objectType']
        taskUtil.gt_failure['gt_failure_step'] = taskUtil.counter+1
        taskUtil.objs_w_unk_loc.append(obj_in_hand['objectType'])
        taskUtil.failures_already_injected.append([taskUtil.chosen_failure, failure_injection_idx])
        taskUtil.failure_added = True


def dirty_obj(taskUtil, obj_type):
    src_obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["objectType"] == obj_type)
    e = taskUtil.controller.step(
        action="DirtyObject",
        objectId=src_obj["objectId"],
        forceAction=True
    )
    print("DirtyObject: ", e)
    taskUtil.controller.step(action="Done")


def fill_obj(taskUtil, obj_type, liquid_type):
    obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["objectType"] == obj_type)
    e = taskUtil.controller.step(
        action="FillObjectWithLiquid",
        objectId=obj['objectId'],
        fillLiquid=liquid_type,
        forceAction=True
    )
    print("FillWithLiquid: ", e)
    taskUtil.controller.step(action="Done")

def tilt_camera(task, final_tilt):
    if final_tilt > 0:
        e = task.controller.step(
            action="LookDown",
            degrees=final_tilt
        )
    elif final_tilt < 0:
        e = task.controller.step(
            action="LookUp",
            degrees=-final_tilt
        )
    # print("Tilt camera: ", e)

def look_at(task, robot_pos=None, target_pos=None, replan=False, center_to_camera_disp=0.6, rep=0):
    robot_y = robot_pos['y'] + center_to_camera_disp
    yaw = np.arctan2(target_pos['x']-robot_pos['x'], target_pos['z']-robot_pos['z'])
    yaw = math.degrees(yaw)

    tilt = -np.arctan2(target_pos['y']-robot_y, np.sqrt((target_pos['z']-robot_pos['z'])**2 + (target_pos['x']-robot_pos['x'])**2))
    tilt = np.round(np.math.degrees(tilt),1)
    org_tilt = task.controller.last_event.metadata['agent']['cameraHorizon']
    final_tilt = tilt - org_tilt
    if tilt > 60:
        final_tilt = 60
    if tilt < -30:
        final_tilt = -30
    final_tilt = np.round(final_tilt, 1)

    e = task.controller.step(action="Teleport", **robot_pos, rotation=dict(x=0, y=yaw, z=0), forceAction=True)
    save_data(task, e, replan=replan)
    task.controller.step(action="Done")
    # print("tilt degree: ", final_tilt)
    if final_tilt > 0:
        e = task.controller.step(
            action="LookDown",
            degrees=final_tilt
        )
    elif final_tilt < 0:
        e = task.controller.step(
            action="LookUp",
            degrees=-final_tilt
        )
    save_data(task, e, replan=replan)


def place_obj(taskUtil, failure_injection_params):
    if taskUtil.chosen_failure == "occupied_put":
        src_obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["objectType"] == failure_injection_params['src_obj_type'])
        target_obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["objectType"] == failure_injection_params['target_obj_type'])
        e = taskUtil.controller.step(
            action="PickupObject",
            objectId=src_obj['objectId'],
            forceAction=True,
            manualInteract=False
        )
        taskUtil.controller.step(action='Done')
        if failure_injection_params['target_obj_type'] == 'Microwave':
            taskUtil.controller.step(
                action="OpenObject",
                objectId=target_obj['objectId'],
                forceAction=True
            )
            e = taskUtil.controller.step(
                action="PutObject",
                objectId=target_obj['objectId'],
                forceAction=True
            )
            taskUtil.controller.step(
                action="CloseObject",
                objectId=target_obj['objectId'],
                forceAction=True
            )
            taskUtil.controller.step(action='Done')
        else:
            e = taskUtil.controller.step(
                action="PutObject",
                objectId=target_obj['objectId'],
                forceAction=True
            )
    elif taskUtil.chosen_failure == "occupied":
        target_obj_type = failure_injection_params['target_obj_type']
        if "-" in target_obj_type and target_obj_type.split("-")[0] in ['StoveBurner', 'CounterTop']:
            for key, val in taskUtil.unity_name_map.items():
                if val == target_obj_type:
                    target_obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["name"] == key)
                    break
        else:
            target_obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["objectType"] == failure_injection_params['target_obj_type'])

        objectPoses = []
        place_location = target_obj['position'].copy()
        objs = taskUtil.controller.last_event.metadata["objects"]
        for obj in objs:
            obj_name = obj['name']
            obj_type = obj['objectType']
            pos = obj['position']
            rot = obj['rotation']
            if not obj['pickupable'] and not obj['moveable']:
                continue
            if obj_type == failure_injection_params['src_obj_type']:
                pos = place_location
                pos['x'] += failure_injection_params['disp_x']
                pos['z'] += failure_injection_params['disp_z']
                pos['y'] += failure_injection_params['disp_y']
            # print("object name: ", obj_name)
            temp_dict = {'objectName': obj_name, 'position': pos, 'rotation': rot}
            objectPoses.append(temp_dict)

        e = taskUtil.controller.step(
            action='SetObjectPoses',
            objectPoses=objectPoses,
            placeStationary = False
        )
        print("SetObjectPoses: ", e)
        taskUtil.controller.step(
            action="AdvancePhysicsStep",
            timeStep=0.01
        )
        taskUtil.controller.step(action='Done')
    else:
        target_obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["objectType"] == failure_injection_params['target_obj_type'])
        place_location = target_obj['position'].copy()
        objs = taskUtil.controller.last_event.metadata["objects"]
        objectPoses = []
        for obj in objs:
            obj_name = obj['name']
            obj_type = obj['objectType']
            pos = obj['position']
            rot = obj['rotation']
            if not obj['pickupable'] and not obj['moveable']:
                continue
            if obj_type == failure_injection_params['src_obj_type']:
                pos = place_location
                pos['x'] += failure_injection_params['disp_x']
                pos['z'] += failure_injection_params['disp_z']
                pos['y'] += failure_injection_params['disp_y']
            # print("object name: ", obj_name)
            temp_dict = {'objectName': obj_name, 'position': pos, 'rotation': rot}
            objectPoses.append(temp_dict)
        e = taskUtil.controller.step(
            action='SetObjectPoses',
            objectPoses=objectPoses,
            placeStationary = False
        )
        taskUtil.controller.step(action='Done')
        print("SetObjectPoses: ", e)


def place_obj_in_small_receptacle(task, place_location, replan=False):
    print("[INFO] Running primitive to place object in small receptacle")
    robot_pos = task.controller.last_event.metadata['agent']['position']
    tilt = task.controller.last_event.metadata['agent']['cameraHorizon']
    dist = np.sqrt((robot_pos['x'] - place_location['x'])**2 + (robot_pos['z'] - place_location['z'])**2)
    #print("tilt, dist: ", tilt, dist)
    tilt = np.round(tilt, 1)
    dist = np.round(dist, 1) - 0.4
    # Look straight (tilt = 0)
    if tilt > 0:
        e = task.controller.step(
            action="LookUp",
            degrees=tilt
        )
    else:
        e = task.controller.step(
            action="LookDown",
            degrees=tilt
        )
    #print("Look: ", e)
    task.controller.step(action="Done")

    # Move object over receptacle
    e = task.controller.step(
        action="MoveHeldObjectAhead",
        moveMagnitude=dist,
        forceVisible=False
    )
    task.controller.step(action='Done')
    #print("move object: ", e)
    
    # Drop object
    e = task.controller.step(
        action="DropHandObject",
        forceAction=False
    )
    task.controller.step(action='Done')
    #print("drop object: ", e)

    # Look at the receptacle again
    if tilt > 0:
        e = task.controller.step(
            action="LookDown",
            degrees=tilt
        )
    else:
        e = task.controller.step(
            action="LookUp",
            degrees=tilt
        )
    #print("Look: ", e)
    save_data(task, e, replan=replan)
    task.controller.step(action="Done")
    time.sleep(1)


# helper function to place an object on a large receptacle such as a countertop
def place_obj_on_large_receptacle(task, src_obj, target_obj_type, thresh=0.8, target_obj_id=None, replan=False):
    print("[INFO] Running primitive to place object on large receptacle")
    if target_obj_id is None:
        print("target object id is not specified.")
    else:
        print("target object id:", target_obj_id)
    target_obj_type_in_sim = target_obj_type
    if target_obj_type in NAME_MAP:
        target_obj_type_in_sim = NAME_MAP[target_obj_type]
    src_obj_type, src_obj_id = src_obj['objectType'], src_obj['objectId']
    src_obj_type_in_sim = src_obj_type
    if src_obj_type in NAME_MAP:
        src_obj_type_in_sim = NAME_MAP[src_obj_type]

    if task.chosen_failure == "wrong_perception":
        if src_obj_type == task.failure_injection_params['correct_obj_type']:
            src_obj_type = task.failure_injection_params['wrong_obj_type']
        elif target_obj_type == task.failure_injection_params['correct_obj_type']:
            target_obj_type = task.failure_injection_params['wrong_obj_type']
    
    target_objs = []
    if target_obj_id is None:
        if "-" not in target_obj_type:
            robot_pos = task.controller.last_event.metadata['agent']['position']
            for obj in task.controller.last_event.metadata["objects"]:
                if obj["objectType"] == target_obj_type:
                    temp_obj_pos = obj['position']
                    dist = np.sqrt((robot_pos['x'] - temp_obj_pos['x'])**2 + (robot_pos['z'] - temp_obj_pos['z'])**2)
                    tup = (dist, obj)
                    target_objs.append(tup)
            target_objs = sorted(target_objs, key=lambda d: d[0])
        else:
            for obj_unity_name, v in task.unity_name_map.items():
                if v == target_obj_type:
                    target_obj = next(obj for obj in task.controller.last_event.metadata["objects"] if obj["name"] == obj_unity_name)
                    target_objs.append((0, target_obj))
    else: # target_obj_id is specified
        target_obj = next(obj for obj in task.controller.last_event.metadata["objects"] if obj["objectId"] == target_obj_id)
        target_objs.append((0, target_obj))

    # check if target object is in view
    found_obj = False
    for dist, target_obj in target_objs:
        target_obj_id = target_obj['objectId']
        e = task.controller.step(
            action="GetSpawnCoordinatesAboveReceptacle",
            objectId=target_obj_id,
            anywhere=False
        )
        #print("spawnPoints: ", e)
        if e.metadata['lastActionSuccess'] and len(e.metadata['actionReturn']) > 0:
            found_obj = True
            print("receptacle found in current view")
            break

    # navigate to the closest target object
    if not found_obj:
        for i in range(len(target_objs)):
            _, target_obj = target_objs[i]
            print("[INFO] Navigate to the closest target object:", target_obj['objectId'])
            navigate_to_obj(task, target_obj['objectType'], obj_id=target_obj['objectId'], replan=replan)
            e = task.controller.step(
                action="GetSpawnCoordinatesAboveReceptacle",
                objectId=target_obj['objectId'],
                anywhere=True
            )
            # print("GetSpawnCoordinatesAboveReceptacle: ", e)
            if e.metadata['actionReturn'] is not None:
                break
        target_obj_type_in_sim = NAME_MAP[task.unity_name_map[target_obj['name']]]

    print("chosen counterTop:", target_obj_id)
    task.controller.step(action="Done")
    time.sleep(1)
    place_locations = e.metadata['actionReturn']
    # print("total potential place points: ", len(place_locations))
    
    # find valid locations on the receptacle to put object
    placed = False
    visible = False
    robot_pos = task.controller.last_event.metadata['agent']['position']

    counter = 0
    while (not placed or not visible):
        counter += 1
        # if too many trials, just drop the object
        if counter > 200:
            e = task.controller.step(
                action="DropHandObject",
                forceAction=False
            )
            print("DropHandObject: ", e)
            break
        visible = False
        placed = False
        place_location = np.random.choice(place_locations)
        # place point should be close enough to robot
        dist = np.sqrt((robot_pos['x'] - place_location['x'])**2 + (robot_pos['z'] - place_location['z'])**2)
        if dist > thresh:
            continue
        e = task.controller.step(
            action="PlaceObjectAtPoint",
            objectId=src_obj_id,
            position=place_location
        )
        # print("PlaceObjectAtPoint: ", e)
        task.controller.step(action="Done")
        if e.metadata['lastActionSuccess']:
            placed = True
        if src_obj['visible']:
            visible = True
    
    save_data(task, e, replan=replan)
    look_at(task, robot_pos, place_location, replan)
    task.interact_actions[task.counter] = "Put " + src_obj_type_in_sim.lower() + " on " + target_obj_type_in_sim.lower()


File: main/audio.py

import os
import wav2clip
import numpy as np
import itertools
import pickle
from PIL import Image
from moviepy.editor import VideoFileClip

from clip_utils import get_text_feats, get_nn_text_w_audio, get_img_feats
from constants import NAME_MAP

audio2label = {
    "toggle-on-faucet.wav": "water runs in sink",
    "toggle-on-toaster.wav": "toaster turns on",
    "toggle-on-stoveburner.wav": "stove burner turns on",
    "drop-pot.wav": "object drops or cracks on hard surface",
    "drop-plastic-bowl.wav": "object drops or cracks on hard surface",
    "drop-egg.wav": "object drops or cracks on hard surface",
    "slice-bread.wav": "slice bread",
    "toggle-on-microwave.wav": "microwave turns on",
    "toggle-on-coffeemachine.wav": "coffee machine turns on",
    "pour-water-in-sink.wav": "water runs in sink",
    "open-fridge.wav": "fridge opens",
    "close-fridge.wav": "fridge closes",
    "open-microwave.wav": "microwave opens",
    "close-microwave.wav": "microwave closes",
    "crack-egg.wav": "egg cracks"
}

TEXT_LIST = list(set([item for item in audio2label.values()]))

model = wav2clip.get_model()
model.eval()

def process_sound(data_path, object_list=None):
    def to_ranges(iterable):
        iterable = sorted(set(iterable))
        for key, group in itertools.groupby(enumerate(iterable),
                                            lambda t: t[1] - t[0]):
            group = list(group)
            yield group[0][1], group[-1][1]

    with open(os.path.join(data_path, "interact_actions.pickle"), 'rb') as f:
        interact_actions = pickle.load(f)
    interact_steps = interact_actions.keys()

    pred_sounds = {}
    clip = VideoFileClip(os.path.join(data_path, "original-video.mp4"))
    total_frames = int(clip.fps * clip.duration)

    frames_w_sound = []
    for cur_frame in range(0, total_frames):
        subclip = clip.subclip(cur_frame, cur_frame+1)
        max_volume = subclip.audio.max_volume()

        if max_volume > 0.01:
            frames_w_sound.append(cur_frame)

    frame_ranges = list(to_ranges(frames_w_sound))
    # print("frame ranges:", frame_ranges)

    text_list = []
    for label in TEXT_LIST:
        if "drops" in label:
            text_list.append(label)
        for obj_class in object_list:
            if obj_class in NAME_MAP:
                obj_name = NAME_MAP[obj_class]
            else:
                obj_name = obj_class.lower()
            if obj_name in label:
                text_list.append(label)
                break
    # print("text list:", text_list)
    text_feats = get_text_feats(text_list)
    # print("text embedding shape", text_feats.shape)

    for frame_range in frame_ranges:
        # print(f"FRAME {start_frame}")
        subclip = clip.subclip(frame_range[0], frame_range[1]+1)
        signal = subclip.audio.to_soundarray().astype(np.float32)

        if len(signal.shape) == 2 and signal.shape[1] == 2:
            signal = np.mean(signal, axis=1)

        norm_signal = signal / np.max(np.abs(signal))
        audio_feats = wav2clip.embed_audio(norm_signal, model)[0]
        audio_feats /= np.linalg.norm(audio_feats)

        img = np.array(Image.open(os.path.join(data_path, 'ego_img/img_step_{}.png'.format(frame_range[0]+1))).convert("RGB"))
        img_feats = get_img_feats(img)[0]
        # print("image embedding shape", img_feats.shape)

        top_k, weight = 3, 0
        for frame in range(frame_range[0], frame_range[1]+1):
            if frame in interact_steps:
                weight = 2
        sorted_texts, sorted_scores = get_nn_text_w_audio(text_list, text_feats, img_feats, audio_feats, weight=weight)
        # for text, score in zip(sorted_texts[:top_k], sorted_scores[:top_k]):
        #     print(f"weight: {weight}, score: {score}, event: {text}")
        # print("=========================================")
        
        for frame in range(frame_range[0], frame_range[1]+1):
            pred_sounds[frame] = sorted_texts[0]

    return pred_sounds



File: main/clip_utils.py

import clip
import numpy as np
from PIL import Image
import torch

device = f'cuda:0' if torch.cuda.is_available() else 'cpu'
torch.set_grad_enabled(False)

clip_version = "ViT-B/16"
clip_feat_dim = {'RN50': 1024, 'RN101': 512, 'RN50x4': 640, 'RN50x16': 768, 'RN50x64': 1024, 'ViT-B/32': 512, 'ViT-B/16': 512, 'ViT-L/14': 768}[clip_version]
model, preprocess = clip.load(clip_version, device=device)  # clip.available_models()
model.eval()

def get_text_feats(in_text, batch_size=64):
  text_tokens = clip.tokenize(in_text).to(device)
  text_id = 0
  text_feats = np.zeros((len(in_text), clip_feat_dim), dtype=np.float32)
  while text_id < len(text_tokens):  # Batched inference.
    batch_size = min(len(in_text) - text_id, batch_size)
    text_batch = text_tokens[text_id:text_id+batch_size]
    with torch.no_grad():
      batch_feats = model.encode_text(text_batch).float()
    batch_feats /= batch_feats.norm(dim=-1, keepdim=True)
    batch_feats = np.float32(batch_feats.cpu())
    text_feats[text_id:text_id+batch_size, :] = batch_feats
    text_id += batch_size
  return text_feats

def get_img_feats(img):
  img_pil = Image.fromarray(np.uint8(img))
  img_in = preprocess(img_pil)[None, ...]
  with torch.no_grad():
    img_feats = model.encode_image(img_in.to(device)).float()
  img_feats /= img_feats.norm(dim=-1, keepdim=True)
  img_feats = np.float32(img_feats.cpu())
  return img_feats

def get_nn_text(raw_texts, text_feats, img_feats):
  scores = text_feats @ img_feats.T
  scores = scores.squeeze()
  high_to_low_ids = np.argsort(scores).squeeze()[::-1]
  high_to_low_texts = [raw_texts[i] for i in high_to_low_ids]
  high_to_low_scores = np.sort(scores).squeeze()[::-1]
  return high_to_low_texts, high_to_low_scores

def get_nn_text_w_audio(raw_texts, text_feats, img_feats, audio_feats, weight):
  scores = text_feats @ audio_feats.T + weight * text_feats @ img_feats.T
  scores = scores.squeeze()
  high_to_low_ids = np.argsort(scores).squeeze()[::-1]
  high_to_low_texts = [raw_texts[i] for i in high_to_low_ids]
  high_to_low_scores = np.sort(scores).squeeze()[::-1]
  return high_to_low_texts, high_to_low_scores
  


File: main/constants.py

from typing import Tuple

########################################################################################################################
# Special objects
########################################################################################################################
BULKY_OBJECTS = [
    "first countertop",
    "second countertop",
    "third countertop",
    "fourth countertop",
    "first stove burner",
    "second stove burner",
    "third stove burner",
    "fourth stove burner",  
    "faucet",  
    "sink basin",
    "sink",
    "window",
    "wall",
    "drawer",
    "cabinet",
    "fridge",
    "coffee machine"
]

NAME_MAP = {
    "TomatoSliced": "tomato slice",
    "PotatoSliced": "potato slice",
    "LettuceSliced": "lettuce slice",
    "BreadSliced": "bread slice",
    "EggCracked": "cracked egg",
    "AppleSliced": "apple slice",
    "HousePlant": "house plant",
    "CounterTop-1": "first countertop",
    "CounterTop-2": "second countertop",
    "CounterTop-3": "third countertop",
    "CounterTop-4": "fourth countertop",
    "StoveBurner-1": "first stove burner",
    "StoveBurner-2": "second stove burner",
    "StoveBurner-3": "third stove burner",
    "StoveBurner-4": "fourth stove burner",
    "Cabinet-1": "first cabinet",
    "Cabinet-2": "second cabinet",
    "Cabinet-3": "third cabinet",
    "Cabinet-4": "fourth cabinet",
    "Cabinet-5": "fifth cabinet",
    "Cabinet-6": "sixth cabinet",
    "Cabinet-7": "seventh cabinet",
    "Cabinet-8": "eight cabinet",
    "Cabinet-9": "ninth cabinet",
    "Cabinet-10": "tenth cabinet",
    "Cabinet-11": "eleventh cabinet",
    "Cabinet-12": "twelveth cabinet",
    "Faucet-1": "first faucet",
    "Faucet-2": "second faucet",
    "Sink-1": "first sink",
    "Sink-2": "second sink",
    "StoveBurner": "stove burner",
    'AlarmClock': "alarm clock",
    'BaseballBat': "baseball bat",
    'BathtubBasin': "bathtub basin",
    'ButterKnife': "butter knife",
    'CoffeeMachine': "coffee machine",
    'CreditCard': "credit card",
    'DeskLamp': "desk lamp",
    'DishSponge': "dish sponge",
    'FloorLamp': "floor lamp",
    'GarbageCan': "garbage can",
    'Glassbottle': "glass bottle",
    'HandTowel': "hand towel",
    'HandTowelHolder': "towel holder",
    'LaundryHamper': "laundry hamper",
    'LaundryHamperLid': "laundry hamper lid",
    'LightSwitch': "light switch",
    'PaperTowel': "paper towel",
    'PaperTowelRoll': "paper towel roll",
    'PepperShaker': "pepper shaker",
    'RemoteControl': "remote control",
    'SaltShaker': "salt shaker",
    'ScrubBrush': "scrub brush",
    'ShowerDoor': "shower door",
    'ShowerGlass': "shower glass",
    'SinkBasin': "sink",
    'SoapBar': "soap bar",
    'SoapBottle': "soap bottle",
    'SprayBottle': "spray bottle",
    'StoveKnob': "stove knob",
    'DiningTable': "dining table",
    'CoffeeTable': "coffee table",
    'SideTable': "side table",
    'TennisRacket': "tennis racket",
    'TissueBox': "tissue box",
    'ToiletPaper': "toilet paper",
    'ToiletPaperHanger': "toilet paper holder",
    'ToiletPaperRoll': "toilet paper roll",
    'TowelHolder': "towel holder",
    'TVStand': "tv stand",
    'WateringCan': "watering can",
    'WineBottle': "wine bottle",
    "ShelvingUnit": "shelving unit",
}

OBJ_SLICED_MAP = {
    "Tomato": "TomatoSliced",
    "Potato": "PotatoSliced",
    "Lettuce": "LettuceSliced",
    "Bread": "BreadSliced",
    "Egg": "EggCracked",
    "Apple": "AppleSliced"
}

OBJ_UNSLICED_MAP = {
    "TomatoSliced": "Tomato",
    "PotatoSliced": "Potato",
    "LettuceSliced": "Lettuce",
    "BreadSliced": "Bread",
    "AppleSliced": "Apple",
    "EggCracked": "Egg"
}

SOUND_PATH = {
    "Toggle on StoveBurner": "toggle-on-stoveburner.wav",
    "Toggle on Toaster": "toggle-on-toaster.wav",
    "Toggle on Faucet": "toggle-on-faucet.wav",
    "Toggle on Television": "toggle-on-television.wav",
    "Open Fridge": "open-fridge.wav",
    "Close Fridge": "close-fridge.wav",
    "Close Laptop": "close-laptop.wav",
    "Drop Knife": "drop-knife.wav",
    "Drop Bowl": "drop-plastic-bowl.wav",
    "Drop RemoteControl": "drop-plastic-bowl.wav",
    "Drop Egg": "drop-egg.wav",
    "Drop Pot": "drop-pot.wav",
    "Drop Mug": "drop-plastic-bowl.wav",
    "Drop Plate": "drop-plate.wav",
    "Slice Bread": "slice-bread.wav",
    "Crack Egg": "crack-egg.wav",
    "Open Microwave": "open-microwave.wav",
    "Close Microwave": "close-microwave.wav",
    "Toggle on Microwave": "toggle-on-microwave.wav",
    "Toggle on CoffeeMachine": "toggle-on-coffeemachine.wav",
    "Pour water into Mug": "pour-water-in-mug.wav",
    "Pour water into Sink": "pour-water-in-sink.wav",
}

TASK_DICT = {
    0: '',
    1: 'boilWater',
    2: 'toastBread',
    3: 'cookEgg',
    4: 'heatPotato',
    5: 'makeCoffee',
    6: 'waterPlant',
    7: 'storeEgg',
    8: 'makeSalad',
    9: 'switchDevices',
    10: 'warmWater'
}

########################################################################################################################
# Unity Hyperparameters
########################################################################################################################

BUILD_PATH = None
X_DISPLAY = '0'

AGENT_STEP_SIZE = 0.25
AGENT_HORIZON_ADJ = 15
AGENT_ROTATE_ADJ = 90
CAMERA_HEIGHT_OFFSET = 0.75
VISIBILITY_DISTANCE = 1.5
HORIZON_GRANULARITY = 15

RENDER_IMAGE = True
RENDER_DEPTH_IMAGE = True
RENDER_CLASS_IMAGE = True
RENDER_OBJECT_IMAGE = True

MAX_DEPTH = 5000
STEPS_AHEAD = 5
SCENE_PADDING = STEPS_AHEAD * 3
SCREEN_WIDTH = DETECTION_SCREEN_WIDTH = 300
SCREEN_HEIGHT = DETECTION_SCREEN_HEIGHT = 300
MIN_VISIBLE_PIXELS = 10

# (400) / (600*600) ~ 0.13% area of image
# int(MIN_VISIBLE_RATIO * float(DETECTION_SCREEN_WIDTH) * float(DETECTION_SCREEN_HEIGHT))
# MIN_VISIBLE_PIXELS = int(MIN_VISIBLE_RATIO * float(DETECTION_SCREEN_WIDTH) * float(
#    DETECTION_SCREEN_HEIGHT))  # (400) / (600*600) ~ 0.13% area of image

########################################################################################################################
# Scenes and Objects
########################################################################################################################

TRAIN_SCENE_NUMBERS = list(range(7, 31))           # Train Kitchens (24/30)
TRAIN_SCENE_NUMBERS.extend(list(range(207, 231)))  # Train Living Rooms (24/30)
TRAIN_SCENE_NUMBERS.extend(list(range(307, 331)))  # Train Bedrooms (24/30)
TRAIN_SCENE_NUMBERS.extend(list(range(407, 431)))  # Train Bathrooms (24/30)

TEST_SCENE_NUMBERS = list(range(1, 7))             # Test Kitchens (6/30)
TEST_SCENE_NUMBERS.extend(list(range(201, 207)))   # Test Living Rooms (6/30)
TEST_SCENE_NUMBERS.extend(list(range(301, 307)))   # Test Bedrooms (6/30)
TEST_SCENE_NUMBERS.extend(list(range(401, 407)))   # Test Bathrooms (6/30)

SCENE_NUMBERS = TRAIN_SCENE_NUMBERS + TEST_SCENE_NUMBERS

# Scene types.
SCENE_TYPE = {"Kitchen": range(1, 31),
              "LivingRoom": range(201, 231),
              "Bedroom": range(301, 331),
              "Bathroom": range(401, 431)}

OBJECTS = [
    'AlarmClock',
    'Apple',
    'ArmChair',
    'BaseballBat',
    'BasketBall',
    'Bathtub',
    'BathtubBasin',
    'Bed',
    'Blinds',
    'Book',
    'Boots',
    'Bowl',
    'Box',
    'Bread',
    'ButterKnife',
    'Cabinet',
    'Candle',
    'Cart',
    'CD',
    'CellPhone',
    'Chair',
    'Cloth',
    'CoffeeMachine',
    'CounterTop',
    'CreditCard',
    'Cup',
    'Curtains',
    'Desk',
    'DeskLamp',
    'DishSponge',
    'Drawer',
    'Dresser',
    'Egg',
    'FloorLamp',
    'Footstool',
    'Fork',
    'Fridge',
    'GarbageCan',
    'Glassbottle',
    'HandTowel',
    'HandTowelHolder',
    'HousePlant',
    'Kettle',
    'KeyChain',
    'Knife',
    'Ladle',
    'Laptop',
    'LaundryHamper',
    'LaundryHamperLid',
    'Lettuce',
    'LightSwitch',
    'Microwave',
    'Mirror',
    'Mug',
    'Newspaper',
    'Ottoman',
    'Painting',
    'Pan',
    'PaperTowel',
    'PaperTowelRoll',
    'Pen',
    'Pencil',
    'PepperShaker',
    'Pillow',
    'Plate',
    'Plunger',
    'Poster',
    'Pot',
    'Potato',
    'RemoteControl',
    'Safe',
    'SaltShaker',
    'ScrubBrush',
    'Shelf',
    'ShowerDoor',
    'ShowerGlass',
    'Sink',
    'SinkBasin',
    'SoapBar',
    'SoapBottle',
    'Sofa',
    'Spatula',
    'Spoon',
    'SprayBottle',
    'Statue',
    'StoveBurner',
    'StoveKnob',
    'DiningTable',
    'CoffeeTable',
    'SideTable',
    'TeddyBear',
    'Television',
    'TennisRacket',
    'TissueBox',
    'Toaster',
    'Toilet',
    'ToiletPaper',
    'ToiletPaperHanger',
    'ToiletPaperRoll',
    'Tomato',
    'Towel',
    'TowelHolder',
    'TVStand',
    'Vase',
    'Watch',
    'WateringCan',
    'Window',
    'WineBottle',
]

OBJECTS_LOWER_TO_UPPER = {obj.lower(): obj for obj in OBJECTS}

OBJECTS_SINGULAR = [
    'alarmclock',
    'apple',
    'armchair',
    'baseballbat',
    'basketball',
    'bathtub',
    'bathtubbasin',
    'bed',
    'blinds',
    'book',
    'boots',
    'bowl',
    'box',
    'bread',
    'butterknife',
    'cabinet',
    'candle',
    'cart',
    'cd',
    'cellphone',
    'chair',
    'cloth',
    'coffeemachine',
    'countertop',
    'creditcard',
    'cup',
    'curtains',
    'desk',
    'desklamp',
    'dishsponge',
    'drawer',
    'dresser',
    'egg',
    'floorlamp',
    'footstool',
    'fork',
    'fridge',
    'garbagecan',
    'glassbottle',
    'handtowel',
    'handtowelholder',
    'houseplant',
    'kettle',
    'keychain',
    'knife',
    'ladle',
    'laptop',
    'laundryhamper',
    'laundryhamperlid',
    'lettuce',
    'lightswitch',
    'microwave',
    'mirror',
    'mug',
    'newspaper',
    'ottoman',
    'painting',
    'pan',
    'papertowel',
    'papertowelroll',
    'pen',
    'pencil',
    'peppershaker',
    'pillow',
    'plate',
    'plunger',
    'poster',
    'pot',
    'potato',
    'remotecontrol',
    'safe',
    'saltshaker',
    'scrubbrush',
    'shelf',
    'showerdoor',
    'showerglass',
    'sink',
    'sinkbasin',
    'soapbar',
    'soapbottle',
    'sofa',
    'spatula',
    'spoon',
    'spraybottle',
    'statue',
    'stoveburner',
    'stoveknob',
    'diningtable',
    'coffeetable',
    'sidetable'
    'teddybear',
    'television',
    'tennisracket',
    'tissuebox',
    'toaster',
    'toilet',
    'toiletpaper',
    'toiletpaperhanger',
    'toiletpaperroll',
    'tomato',
    'towel',
    'towelholder',
    'tvstand',
    'vase',
    'watch',
    'wateringcan',
    'window',
    'winebottle',
]

OBJECTS_PLURAL = [
    'alarmclocks',
    'apples',
    'armchairs',
    'baseballbats',
    'basketballs',
    'bathtubs',
    'bathtubbasins',
    'beds',
    'blinds',
    'books',
    'boots',
    'bottles',
    'bowls',
    'boxes',
    'bread',
    'butterknives',
    'cabinets',
    'candles',
    'carts',
    'cds',
    'cellphones',
    'chairs',
    'cloths',
    'coffeemachines',
    'countertops',
    'creditcards',
    'cups',
    'curtains',
    'desks',
    'desklamps',
    'dishsponges',
    'drawers',
    'dressers',
    'eggs',
    'floorlamps',
    'footstools',
    'forks',
    'fridges',
    'garbagecans',
    'glassbottles',
    'handtowels',
    'handtowelholders',
    'houseplants',
    'kettles',
    'keychains',
    'knives',
    'ladles',
    'laptops',
    'laundryhampers',
    'laundryhamperlids',
    'lettuces',
    'lightswitches',
    'microwaves',
    'mirrors',
    'mugs',
    'newspapers',
    'ottomans',
    'paintings',
    'pans',
    'papertowels',
    'papertowelrolls',
    'pens',
    'pencils',
    'peppershakers',
    'pillows',
    'plates',
    'plungers',
    'posters',
    'pots',
    'potatoes',
    'remotecontrollers',
    'safes',
    'saltshakers',
    'scrubbrushes',
    'shelves',
    'showerdoors',
    'showerglassess',
    'sinks',
    'sinkbasins',
    'soapbars',
    'soapbottles',
    'sofas',
    'spatulas',
    'spoons',
    'spraybottles',
    'statues',
    'stoveburners',
    'stoveknobs',
    'diningtables',
    'coffeetables',
    'sidetable',
    'teddybears',
    'televisions',
    'tennisrackets',
    'tissueboxes',
    'toasters',
    'toilets',
    'toiletpapers',
    'toiletpaperhangers',
    'toiletpaperrolls',
    'tomatoes',
    'towels',
    'towelholders',
    'tvstands',
    'vases',
    'watches',
    'wateringcans',
    'windows',
    'winebottles',
]

_RECEPTACLE_OBJECTS = [
    'BathtubBasin',
    'Bowl',
    'Cup',
    'Drawer',
    'Mug',
    'Plate',
    'Shelf',
    'Sink',
    'SinkBasin',
    'Box',
    'Cabinet',
    'CoffeeMachine',
    'CounterTop',
    'Fridge',
    'GarbageCan',
    'HandTowelHolder',
    'Microwave',
    'PaintingHanger',
    'Pan',
    'Pot',
    'StoveBurner',
    'DiningTable',
    'CoffeeTable',
    'SideTable',
    'ToiletPaperHanger',
    'TowelHolder',
    'Safe',
    'BathtubBasin',
    'ArmChair',
    'Toilet',
    'Sofa',
    'Ottoman',
    'Dresser',
    'LaundryHamper',
    'Desk',
    'Bed',
    'Cart',
    'TVStand',
    'Toaster',
]

_MOVABLE_RECEPTACLES = [
    'Bowl',
    'Box',
    'Cup',
    'Mug',
    'Plate',
    'Pan',
    'Pot',
]

_INTERACTIVE_OBJECTS = [
    'AlarmClock',
    'Apple',
    'ArmChair',
    'BaseballBat',
    'BasketBall',
    'Bathtub',
    'BathtubBasin',
    'Bed',
    'Blinds',
    'Book',
    'Boots',
    'Bowl',
    'Box',
    'Bread',
    'BreadSliced',
    'ButterKnife',
    'Cabinet',
    'Candle',
    'Cart',
    'CD',
    'CellPhone',
    'Chair',
    'Cloth',
    'CoffeeMachine',
    'CounterTop',
    'CreditCard',
    'Cup',
    'Curtains',
    'Desk',
    'DeskLamp',
    'DishSponge',
    'Drawer',
    'Dresser',
    'Egg',
    'EggCracked',
    'FloorLamp',
    'Footstool',
    'Fork',
    'Fridge',
    'GarbageCan',
    'Glassbottle',
    'HandTowel',
    'HandTowelHolder',
    'HousePlant',
    'Kettle',
    'KeyChain',
    'Knife',
    'Ladle',
    'Laptop',
    'LaundryHamper',
    'LaundryHamperLid',
    'Lettuce',
    'LettuceSliced',
    'LightSwitch',
    'Microwave',
    'Mirror',
    'Mug',
    'Newspaper',
    'Ottoman',
    'Painting',
    'Pan',
    'PaperTowel',
    'PaperTowelRoll',
    'Pen',
    'Pencil',
    'PepperShaker',
    'Pillow',
    'Plate',
    'Plunger',
    'Poster',
    'Pot',
    'Potato',
    'PotatoSliced',
    'RemoteControl',
    'Safe',
    'SaltShaker',
    'ScrubBrush',
    'Shelf',
    'ShowerDoor',
    'ShowerGlass',
    'Sink',
    'SinkBasin',
    'SoapBar',
    'SoapBottle',
    'Sofa',
    'Spatula',
    'Spoon',
    'SprayBottle',
    'Statue',
    'StoveBurner',
    'StoveKnob',
    'DiningTable',
    'CoffeeTable',
    'SideTable',
    'TeddyBear',
    'Television',
    'TennisRacket',
    'TissueBox',
    'Toaster',
    'Toilet',
    'ToiletPaper',
    'ToiletPaperHanger',
    'ToiletPaperRoll',
    'Tomato',
    'TomatoSliced',
    'Towel',
    'TowelHolder',
    'TVStand',
    'Vase',
    'Watch',
    'WateringCan',
    'Window',
    'WineBottle',
]

_TABLETOP_OBJECTS = [
    'AlarmClock',
    'Apple',
    'BaseballBat',
    'BasketBall',
    'Book',
    'Boots',
    'Bowl',
    'Box',
    'Bread',
    'ButterKnife',
    'Cabinet',
    'Candle',
    'Cart',
    'CD',
    'CellPhone',
    'Cloth',
    'CoffeeMachine',
    'CreditCard',
    'Cup',
    'DeskLamp',
    'DishSponge',
    'Egg',
    'FloorLamp',
    'Footstool',
    'Fork',
    'GarbageCan',
    'Glassbottle',
    'HandTowel',
    'HandTowelHolder',
    'Kettle',
    'KeyChain',
    'Knife',
    'Ladle',
    'Laptop',
    'Lettuce',
    'Microwave',
    'Mug',
    'Newspaper',
    'Pan',
    'PaperTowel',
    'PaperTowelRoll',
    'Pen',
    'Pencil',
    'PepperShaker',
    'Pillow',
    'Plate',
    'Plunger',
    'Pot',
    'Potato',
    'RemoteControl',
    'Safe',
    'SaltShaker',
    'ScrubBrush',
    'Sink',
    'SinkBasin',
    'SoapBar',
    'SoapBottle',
    'Spatula',
    'Spoon',
    'SprayBottle',
    'Statue',
    'TeddyBear',
    'TennisRacket',
    'TissueBox',
    'Toaster',
    'ToiletPaper',
    'ToiletPaperHanger',
    'ToiletPaperRoll',
    'Tomato',
    'Towel',
    'TowelHolder',
    'Vase',
    'Watch',
    'WateringCan',
    'WineBottle',
]

_STRUCTURAL_OBJECTS = [
    "Books",
    "Ceiling",
    "Door",
    "Floor",
    "KitchenIsland",
    "LightFixture",
    "Rug",
    "Wall",
    "StandardWallSize",
    "Faucet",
    "Bottle",
    "Bag",
    "Cube",
    "Room",
]

MOVABLE_RECEPTACLES_SET = set(_MOVABLE_RECEPTACLES)
OBJECTS_SET = set(OBJECTS) | MOVABLE_RECEPTACLES_SET

OBJECT_CLASS_TO_ID = {obj: ii for (ii, obj) in enumerate(OBJECTS)}

_OPENABLES = ['Fridge', 'Cabinet', 'Microwave', 'Drawer', 'Safe', 'Box', 'Kettle', 'Toilet', 'Laptop']

_SLICEABLES = [
    "Apple", 
    "Bread",
    "Egg"
    "Lettuce",
    "Potato",
    "Tomato",
]

_FILLABLE = [
    "Bottle",
    "Bowl",
    "Cup",
    "HousePlant",
    "Kettle",
    "Mug",
    "Pot",
    "Sink",
    "WateringCan",
    "WineBottle",
]

_TOGGLABLES = [
    "DeskLamp", 
    "FloorLamp",
    "Microwave", 
    "Candle", 
    "CellPhone", 
    "CoffeeMachine", 
    "Faucet", 
    "Laptop", 
    "LightSwitch",
    "ShowerHead",
    "StoveBurner",
    "StoveKnob",
    "Television",
    "Toaster"]

_DIRTYABLE = [
    "Lettuce",
    "LettuceSliced",
    "Tomato",
    "TomatoSliced",
    "Potato",
    "PotatoSliced",
    "Bread",
    "BreadSliced"
]

_BREAKABLE = [
    "Bottle",
    "Bowl",
    "CellPhone",
    "Cup",
    "Egg",
    "Laptop",
    "Mirror",
    "Mug",
    "Plate",
    "ShowerDoor",
    "ShowerGlass",
    "Statue",
    "Television",
    "Vase",
    "Window",
    "WineBottle"
]

_CRACKABLE = [
    "Egg"
]

_FLAT_RECEPT = [
    "CounterTop",
    "StoveBurner",
    "TVStand",
    "DiningTable",
    "SinkBasin"
]

INVENTORY_OBJECT_STR = "<InventoryObject>"
_EXTRA_TOKENS = [
    INVENTORY_OBJECT_STR
]

_PICKABLES = [s for s in _INTERACTIVE_OBJECTS if (
        (s not in _RECEPTACLE_OBJECTS) and
        (s not in _OPENABLES) and
        (s not in _TOGGLABLES)) or (s in _MOVABLE_RECEPTACLES)
    ]

OBJECT_CLASSES = _STRUCTURAL_OBJECTS + _INTERACTIVE_OBJECTS + _EXTRA_TOKENS

# Mappings between integers and strings
OBJECT_INT_TO_STR = {i: o for i, o in enumerate(OBJECT_CLASSES)}
OBJECT_STR_TO_INT = {o: i for i, o in enumerate(OBJECT_CLASSES)}
UNK_OBJ_INT = len(OBJECT_CLASSES)
UNK_OBJ_STR = "Unknown"
#COLOR_OTHERS = (255, 0, 0)
COLOR_OTHERS = (100, 100, 100)

# -------------------------------------------------------------
# Public API:
# -------------------------------------------------------------
# Simple mappings

def get_all_interactive_objects():
    return list(iter(_INTERACTIVE_OBJECTS))

def get_receptacle_ids():
    return [object_string_to_intid(s) for s in _RECEPTACLE_OBJECTS]

def get_pickable_ids():
    return [object_string_to_intid(s) for s in _PICKABLES]

def get_togglable_ids():
    return [object_string_to_intid(s) for s in _TOGGLABLES]

def get_openable_ids():
    return [object_string_to_intid(s) for s in _OPENABLES]

def get_sliceable_ids():
    return [object_string_to_intid(s) for s in _SLICEABLES]

def get_ground_ids():
    return [object_string_to_intid(s) for s in ["Rug", "Floor"]]

def get_num_objects():
    return len(OBJECT_CLASSES) + 1

def object_color_to_intid(color: Tuple[int, int, int]) -> int:
    global OBJECT_COLOR_TO_INTID
    return OBJECT_COLOR_TO_INTID[color]

def object_intid_to_color(intid: int) -> Tuple[int, int, int]:
    global OBJECT_INTID_TO_COLOR
    return OBJECT_INTID_TO_COLOR[intid]

def object_string_to_intid(object_str) -> int:
    global OBJECT_STR_TO_INT, UNK_OBJ_INT
    # Remove the part about object instance location
    object_str = object_str.split("|")[0].split(":")[-1].split(".")[0]
    if object_str in OBJECT_STR_TO_INT:
        return OBJECT_STR_TO_INT[object_str]
    else:
        return UNK_OBJ_INT

INTERACTIVE_OBJECT_IDS = [object_string_to_intid(o) for o in _INTERACTIVE_OBJECTS]
STRUCTURAL_OBJECT_IDS = [object_string_to_intid(o) for o in _STRUCTURAL_OBJECTS]
TABLETOP_OBJECT_IDS = [object_string_to_intid(o) for o in _TABLETOP_OBJECTS]

def object_intid_to_string(intid: int) -> str:
    global OBJECT_INT_TO_STR, UNK_OBJ_STR
    if intid in OBJECT_INT_TO_STR:
        return OBJECT_INT_TO_STR[intid]
    else:
        return UNK_OBJ_STR

def object_string_to_color(object_str : str) -> Tuple[int, int, int]:
    return object_intid_to_color(object_string_to_intid((object_str)))

def object_color_to_string(color: Tuple[int, int, int]) -> str:
    return object_intid_to_string(object_color_to_intid(color))

# object parents
OBJ_PARENTS = {obj: obj for obj in OBJECTS}
OBJ_PARENTS['AppleSliced'] = 'Apple'
OBJ_PARENTS['BreadSliced'] = 'Bread'
OBJ_PARENTS['LettuceSliced'] = 'Lettuce'
OBJ_PARENTS['PotatoSliced'] = 'Potato'
OBJ_PARENTS['TomatoSliced'] = 'Tomato'

# force a different horizon view for objects of (type, location). If the location is None, force this horizon for all
# objects of that type.
FORCED_HORIZON_OBJS = {
    ('FloorLamp', None): 0,
    ('Fridge', 18): 30,
    ('Toilet', None): 15,
}

# openable objects with fixed states for transport.
FORCED_OPEN_STATE_ON_PICKUP = {
    'Laptop': False,
}

# list of openable classes.
OPENABLE_CLASS_LIST = ['Fridge', 'Cabinet', 'Microwave', 'Drawer', 'Safe', 'Box']
OPENABLE_CLASS_SET = set(OPENABLE_CLASS_LIST)

########################################################################################################################
# Actions
########################################################################################################################

# actions
IDX_TO_ACTION_TYPE = {
    0: "RotateLeft",
    1: "RotateRight",
    2: "MoveAhead",
    3: "LookUp",
    4: "LookDown",
    5: "OpenObject",
    6: "CloseObject",
    7: "PickupObject",
    8: "PutObject",
    9: "ToggleObjectOn",
    10: "ToggleObjectOff",
    11: "SliceObject",
}

ACTION_TYPE_TO_IDX = {v: k for k, v in IDX_TO_ACTION_TYPE.items()}
ACTION_TYPES = [IDX_TO_ACTION_TYPE[i] for i in range(len(IDX_TO_ACTION_TYPE))]

NAV_ACTION_TYPES = [
    "RotateLeft",
    "RotateRight",
    "MoveAhead",
    "LookUp",
    "LookDown"
]

INTERACT_ACTION_TYPES = [
    "OpenObject",
    "CloseObject",
    "PickupObject",
    "PutObject",
    "ToggleObjectOn",
    "ToggleObjectOff",
    "SliceObject"
]



File: main/data.py

import os
import pickle

def get_object_list_from_actions(actions):
    object_list = set()
    for action in actions:
        params = action[1:-1].split(", ")
        for obj in params[1:]:
            if "CounterTop" not in obj: # ignore CounterTop
                object_list.add(obj.split("-")[0])

    return list(object_list)

def load_data(task_path, task):
    object_list = get_object_list_from_actions(task['actions']) # task['object_list']

    if os.path.exists(task_path):
        num_events = len([f for f in os.listdir(os.path.join(task_path, "events")) if f.endswith(".pickle")])
        events = []
        for event_idx in range(num_events):
            with open(os.path.join(task_path, "events", "step_{}.pickle".format(event_idx+1)), 'rb') as f:
                event = pickle.load(f)
                events.append(event)
        with open(os.path.join(task_path, "interact_actions.pickle"), 'rb') as f:
            interact_actions = pickle.load(f)
        with open(os.path.join(task_path, "nav_actions.pickle"), 'rb') as f:
            nav_actions = pickle.load(f)

        print("interact_actions: ", interact_actions)
        print("nav_actions: ", nav_actions)
        print("length of events: ", len(events))

        return events, task, object_list, interact_actions, nav_actions
    else:
        raise ValueError("Queried folder does not exist.")



File: main/execute_replan.py

import os
import pickle
import json
from ai2thor.controller import Controller
from ai2thor.platform import CloudRendering

from utils import *
from task_utils import *
from constants import *
from action_primitives import *


def execute_correction_plan(task_idx, f_name, taskUtil):
    with open(f'state_summary/{f_name}/replan.json', 'r') as f:
        plan = json.load(f)["plan"]
        for instr in plan:
            lis = instr.split(',')
            lis = [item.strip("() ") for item in lis]
            action = lis[0]
            params = lis[1:]

            taskUtil.chosen_failure = "blocking" if taskUtil.chosen_failure == "blocking" else None
            print("action, params: ", action, params)
            func = globals()[action]
            func(taskUtil, *params, fail_execution=False, replan=True)

    is_success = check_task_success(task_idx, taskUtil.controller.last_event)
    print("Task success :-)" if is_success else "Task fail :-(")
    return is_success


def run_correction(data_path, f_name):
    with open(f'thor_tasks/{f_name}/task.json') as f:
        task = json.load(f)
    controller = Controller(
        agentMode="default",
        massThreshold=None,
        scene=task['scene'],
        visibilityDistance=1.5,
        gridSize=0.25,
        renderDepthImage=True,
        renderInstanceSegmentation=True,
        width=960,
        height=960,
        fieldOfView=60,
        platform=CloudRendering
    )

    events_path = 'thor_tasks/' + f_name + '/events/'
    l = os.listdir(events_path)
    lsorted = sorted(l, key=lambda x: int(os.path.splitext(x)[0].split('_')[-1]))
    last_frame = int(lsorted[-1].split('_')[-1].split('.')[0])
    with open(events_path + lsorted[-1], 'rb') as f:
        final_event = pickle.load(f)
    objs = [obj for obj in final_event.metadata['objects']]
    final_agent = final_event.metadata['agent']
    # print("final_agent: ", final_agent)

    e = controller.step(
        action="Teleport",
        position=final_agent['position'],
        rotation=final_agent['rotation'],
        horizon=final_agent['cameraHorizon'],
        standing=final_agent['isStanding'],
        forceAction=True
    )
    # print("Setting agent pose: ", e)

    # ---------- Resetting state to final state of the failed execution ----------
    objectPoses = []
    dropped_obj_type = ""
    
    # restore dropped object
    if "Dropped" == task['gt_failure_reason'].split(" ")[0]:
        dropped_obj_type = task['gt_failure_reason'].split(" ")[1]
        dropped_step = convert_timestep_to_step(task['gt_failure_step'], video_fps=1)
        with open(f'thor_tasks/{f_name}/events/step_{dropped_step}.pickle', 'rb') as f:
            dropped_event = pickle.load(f)
            dropped_obj = next(o for o in dropped_event.metadata["objects"] if o["objectType"] == dropped_obj_type)
            temp_dict = {'objectName': dropped_obj['name'], 'position': dropped_obj['position'], 'rotation': dropped_obj['rotation']}
            objectPoses.append(temp_dict)

    # restore object states and object poses
    for obj in objs:
        if 'Sliced' in obj['objectType']:  # e.g. BreadSliced
            index = obj['objectType'].find('Sliced')
            org_obj_type = obj['objectType'][:index]  # e.g. Bread
            
            org_obj = next(o for o in controller.last_event.metadata["objects"] if o["objectType"] == org_obj_type)
            if not org_obj['isSliced']:
                e = controller.step(
                action="SliceObject",
                objectId=org_obj['objectId'],
                forceAction=True
            )

        elif 'Cracked' in obj['objectType']:
            index = obj['objectType'].find('Cracked')
            org_obj_type = obj['objectType'][:index]

            org_obj = next(o for o in controller.last_event.metadata["objects"] if o["objectType"] == org_obj_type)
            if not org_obj['isBroken']:
                e = controller.step(
                action="BreakObject",
                objectId=org_obj['objectId'],
                forceAction=True
            )

    for obj in objs:
        org_obj = next(o for o in controller.last_event.metadata["objects"] if o["name"] == obj['name'])
        if obj['isOpen']:
            e = controller.step(
                action="OpenObject",
                objectId=org_obj['objectId'],
                forceAction=True
            )
        else:
            e = controller.step(
                    action="CloseObject",
                    objectId=org_obj['objectId'],
                    forceAction=True
                ) 
        if obj['isToggled']:
            e = controller.step(
                action="ToggleObjectOn",
                objectId=org_obj['objectId'],
                forceAction=True
            )
        else:
            e = controller.step(
                action="ToggleObjectOff",
                objectId=org_obj['objectId'],
                forceAction=True
            )
        if obj['isFilledWithLiquid']:
            e = controller.step(
                action="FillObjectWithLiquid",
                objectId=org_obj['objectId'],
                fillLiquid=obj['fillLiquid'],
                forceAction=True
            )
        if obj['isDirty']:
            e = controller.step(
                action="DirtyObject",
                objectId=org_obj['objectId'],
                forceAction=True
            )
        if obj['objectType'] != dropped_obj_type:
            obj_name = obj['name']
            pos = obj['position']
            rot = obj['rotation']
            if not obj['pickupable'] and not obj['moveable']:
                continue
            temp_dict = {'objectName': obj_name, 'position': pos, 'rotation': rot}
            objectPoses.append(temp_dict)

    e = controller.step(
        action='SetObjectPoses',
        objectPoses=objectPoses
    )
    # print("SetObjectPoses: ", e)
    controller.step(action="Done")

    # restore 'in robot gripper' relation
    for obj in objs:
        if obj['isPickedUp']:
            org_obj = next(o for o in controller.last_event.metadata["objects"] if o["name"] == obj['name'])
            e = controller.step(
                action="PickupObject",
                objectId=org_obj['objectId'],
                forceAction=True
            )
    # ----------------------------------------------------------------------------

    reachable_positions = controller.step(action="GetReachablePositions").metadata["actionReturn"]
    if 'chosen_failure' in task:
        chosen_failure = task['chosen_failure']
    else:
        chosen_failure = None
    if 'failure_injection_params' in task:
        failure_injection_params = task['failure_injection_params']
    else:
        failure_injection_params = None
    taskUtil = TaskUtil(folder_name=f_name,
                    controller=controller,
                    reachable_positions=reachable_positions,
                    failure_injection=False,
                    index=0,
                    repo_path=data_path,
                    chosen_failure=chosen_failure,
                    failure_injection_params=failure_injection_params,
                    counter=last_frame,
                    replan=True)
    is_success = execute_correction_plan(task['task_idx'], f_name, taskUtil)

    with open(f'state_summary/{f_name}/replan.json', 'r') as f:
        replan_json = json.load(f)
    replan_json["success"] = is_success
    with open(f'state_summary/{f_name}/replan.json', 'w') as f:
        json.dump(replan_json, f)

    generate_video(taskUtil, recovery_video=True)
    controller.stop()



File: main/exp.py

import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"
import json
import pickle
from constants import *
from scene_graph import SceneGraph
from scene_graph import Node as SceneGraphNode
from get_local_sg import get_scene_graph, save_pcd
import numpy as np
from data import *
from utils import *
from clip_utils import *
from audio import process_sound, audio2label
from point_cloud_utils import *

def run_sound_module(folder_name, object_list):
    detected_sounds = []
    try:
        detected_sounds = process_sound(f'thor_tasks/{folder_name}', object_list)
        print("detected sounds:", detected_sounds)
    except Exception as e:
        print(e)
    return detected_sounds


def get_scene_text(scene_graph):
    output = ""
    visited = []
    for node in set(scene_graph.nodes):
        output += (node.get_name() + ", ")

    if len(output) != 0:
        output = output[:-2] + ". "
    for edge_key, edge in scene_graph.edges.items():
        start_name, end_name = edge_key
        edge_key_2 = (end_name, start_name)
        if (edge_key not in visited and edge_key_2 not in visited):
            output += edge.start.name + " is " + edge.edge_type + " " + edge.end.name
            output += ". "
        visited.append(edge_key)
    output = output[:-1]

    return output


def get_held_object(folder_name, step_idx):
    found = False
    while not found:
        if os.path.exists(f'state_summary/{folder_name}/local_graphs/local_sg_{step_idx}.pkl'):
            found = True
            with open(f'state_summary/{folder_name}/local_graphs/local_sg_{step_idx}.pkl', 'rb') as f:
                local_sg = pickle.load(f)
            for key in local_sg.edges:
                if "robot gripper" in key and key[0] != "nothing":
                    return key[0]
        else:
            step_idx -= 1


def generate_scene_graphs(folder_name, events, object_list, nav_actions, interact_actions, WITH_AUDIO, detected_sounds):
    with open(f'thor_tasks/{folder_name}/task.json') as f:
        task = json.load(f)

    if not os.path.exists(f'state_summary/{folder_name}/global_sg.pkl'):
        # sensory-input summary
        os.system(f'mkdir -p state_summary/{folder_name}/local_graphs')
        # os.system("mkdir -p scene/{}".format(folder_name))
        key_frames = []
        prev_graph = SceneGraph(event=None, task=task)
        total_points_dict, bbox3d_dict = {}, {}
        obj_held_prev = None
        cnt, interval = 0, 2
        nav_actions_end_indices = [idx[1] for idx in nav_actions.keys()]
        for step_idx, event in enumerate(events):
            # uniformly drop intermediate navigation frames with no sound
            if (step_idx+1) not in interact_actions and ((step_idx+1) not in nav_actions_end_indices):
                cnt += 1
                if WITH_AUDIO == 1:
                    if step_idx not in detected_sounds and cnt % interval == 0:
                        continue
                elif WITH_AUDIO == 0:
                    if str(step_idx) not in task['sounds'] and cnt % interval == 0:
                        continue

            print("[Frame] " + str(step_idx+1))

            local_sg, total_points_dict, obj_held_prev, bbox3d_dict = get_scene_graph(step_idx, event, object_list, 
                                                                                      total_points_dict, bbox3d_dict, 
                                                                                      obj_held_prev, task)
            print("========================[Current Graph]=====================")
            print(local_sg)

            # 1. Select keyframe based on scene graph difference
            if local_sg != prev_graph:
                if (step_idx+1) not in key_frames:
                    key_frames.append(step_idx+1)
                    prev_graph = local_sg

            # 2. Select keyframe based on actions
            if (step_idx+1) in interact_actions or (step_idx+1) in nav_actions_end_indices:
                if (step_idx+1) not in key_frames:
                    key_frames.append(step_idx+1)

            # 3. Select keyframe based on audio
            if WITH_AUDIO == 0:
                if str(step_idx) in task['sounds']:
                    if (step_idx+1) not in key_frames:
                        key_frames.append(step_idx+1)
            elif WITH_AUDIO == 1:
                if step_idx in detected_sounds:
                    if (step_idx+1) not in key_frames:
                        key_frames.append(step_idx+1)

            with open(f'state_summary/{folder_name}/local_graphs/local_sg_{step_idx}.pkl', 'wb') as f:
                pickle.dump(local_sg, f)

        with open('state_summary/{}/L1_key_frames.txt'.format(folder_name), 'w') as f:
            for frame in key_frames:
                f.write("%i\n" % frame)

        # save_pcd(folder_name, total_points_dict)

        # ======================Get global graph========================
        global_sg = SceneGraph(events[-1], task)
        for label in total_points_dict.keys():
            name = get_label_from_object_id(label, events, task)
            if name is not None:
                new_node = SceneGraphNode(name=name, object_id=label, pos3d=bbox3d_dict[label].get_center(), 
                        corner_pts=np.array(bbox3d_dict[label].get_box_points()),
                        pcd=total_points_dict[label], global_node=True)
                global_sg.add_node_wo_edge(new_node)

        for label in total_points_dict.keys():
            object_name = label.split("|")[0]
            if object_name in object_list:
                name = get_label_from_object_id(label, events, task)
                if name is not None:
                    for node in global_sg.total_nodes:
                        if node.name == name:
                            global_sg.add_node(node)
            
        global_sg.add_agent()
        with open(f'state_summary/{folder_name}/global_sg.pkl', 'wb') as f:
            pickle.dump(global_sg, f)
        # ===============================================================


def generate_summary(folder_name, events, nav_actions, interact_actions, WITH_AUDIO, detected_sounds):
    with open(f'thor_tasks/{folder_name}/task.json') as f:
        task = json.load(f)

    key_frames = []
    with open('state_summary/{}/L1_key_frames.txt'.format(folder_name), 'r') as f:
        frames = f.readlines()
        key_frames = [int(frame) for frame in frames]

    # event-based summary
    if not os.path.exists(f'state_summary/{folder_name}/state_summary_L1.txt'):
    # if True:
        print("[INFO] Start generating event-based summary")
        state_summary_L1 = ""
        L1_captions = []
        for step_idx, event in enumerate(events):
            if not os.path.exists(f'state_summary/{folder_name}/local_graphs/local_sg_{step_idx}.pkl'):
                continue
            if (step_idx+1) in key_frames:
                caption = ""

                # add action
                if (step_idx+1) in interact_actions:
                    caption += f"{convert_step_to_timestep(step=step_idx+1, video_fps=1)}. Action: {interact_actions[step_idx+1]}."
                    # action = interact_actions[step_idx+1]
                else:
                    for key in nav_actions:
                        min_step, max_step = key
                        if min_step <= (step_idx+1) <= max_step:
                            caption += f"{convert_step_to_timestep(step=step_idx+1, video_fps=1)}. Action: {nav_actions[key]}."
                            # action = nav_actions[key]

                if len(caption) == 0:
                    continue
                
                with open(f'state_summary/{folder_name}/local_graphs/local_sg_{step_idx}.pkl', 'rb') as f:
                    local_sg = pickle.load(f)
                    scene_text = get_scene_text(local_sg)
                    caption += f" Visual observation: {scene_text}"

                # Add audio info.
                if WITH_AUDIO == 0:
                    if str(step_idx) in task['sounds']:
                        if 'drop' in task['sounds'][str(step_idx)] and get_held_object(folder_name, step_idx-1) is not None:
                            caption += f" Auditory observation: something drops."
                        else:
                            caption += f" Auditory observation: {audio2label[task['sounds'][str(step_idx)]]}."
                elif WITH_AUDIO == 1:
                    if (step_idx+1) in detected_sounds:
                        caption += f" Auditory observation: {detected_sounds[step_idx+1]}."

                caption += "\n"

                state_summary_L1 += caption
                L1_captions.append(caption)
        with open('state_summary/{}/state_summary_L1.txt'.format(folder_name), 'w') as f:
            f.write(state_summary_L1)
            print("[INFO] Write event-based summary")
    else:
        print("[INFO] Event-based summary already generated")
        L1_captions = []
        state_summary_L1 = ""
        with open('state_summary/{}/state_summary_L1.txt'.format(folder_name), 'r') as f:
            L1_captions = f.readlines()
        state_summary_L1 = "".join(L1_captions)

    # subgoal-based summary
    if not os.path.exists(f'state_summary/{folder_name}/state_summary_L2.txt'):
    # if True:
        print("[INFO] Start generating subgoal-based summary")
        L2_captions = []
        for caption in L1_captions:
            step_num = convert_timestep_to_step(caption.split(".")[0], video_fps=1)
            if step_num in interact_actions:
                L2_captions.append(caption.replace("Action", "Goal"))

        state_summary_L2 = "".join(L2_captions)
        with open('state_summary/{}/state_summary_L2.txt'.format(folder_name), 'w') as f:
            f.write(state_summary_L2)
            print("[INFO] Write subgoal-based summary")
    else:
        print("[INFO] Subgoal-based summary already generated")
        L2_captions = []
        L2_file_name = 'state_summary/{}/state_summary_L2.txt'.format(folder_name)
        if os.path.exists(L2_file_name):
            with open(L2_file_name, 'r') as f:
                L2_captions = f.readlines()
            state_summary_L2 = "".join(L2_captions)


def run_reasoning(folder_name, llm_prompter, global_sg):
    with open(f'thor_tasks/{folder_name}/task.json') as f:
        task = json.load(f)
    
    if os.path.exists(f'state_summary/{folder_name}/reasoning.json'):
        print("[INFO] Reasoning already generated")
        with open(f'state_summary/{folder_name}/reasoning.json', 'r') as f:
            reasoning_dict = json.load(f)
        return
    else:
        reasoning_dict = {}

    save_dir = f'../LLM/{folder_name}'
    # os.system("mkdir -p {}".format(save_dir))

    with open('../LLM/prompts.json', 'r') as f:
        prompt_info = json.load(f)

    # Load L2 captions from state_summary_L2.txt
    with open('state_summary/{}/state_summary_L2.txt'.format(folder_name), 'r') as f:
        L2_captions = f.readlines()

    # Load L1 captions from state_summary_L1.txt
    with open('state_summary/{}/state_summary_L1.txt'.format(folder_name), 'r') as f:
        L1_captions = f.readlines()
    
    # Loop through each subgoal and check for post-condition
    print(">>> Run step-by-step subgoal-level analysis...")
    selected_caption = ""
    prompt = {}

    for caption in L2_captions:
        print(">>> Verify subgoal...")
        subgoal = caption.split(". ")[1].split(": ")[1].lower()

        prompt['system'] = prompt_info['subgoal-verifier']['template-system']
        prompt['user'] = prompt_info['subgoal-verifier']['template-user'].replace("[SUBGOAL]", subgoal).replace("[OBSERVATION]", caption[caption.find("Visual observation"):])

        ans, _  = llm_prompter.query(prompt=prompt, sampling_params=prompt_info['subgoal-verifier']['params'], 
                                    save=prompt_info['subgoal-verifier']['save'], save_dir=save_dir)
        is_success = int(ans.split(", ")[0] == "Yes")
        if is_success == 0:
            selected_caption = caption
            print(f"[INFO] Failure identified in subgoal [{subgoal}] at {caption.split('.')[0]}")
            break
        else:
            print(f"[INFO] Subgoal [{subgoal}] succeeded!")

    if len(selected_caption) != 0:
            print(">>> Get detailed reasoning from L1...")
            step_name = selected_caption.split(".")[0]
            for _, caption in enumerate(L1_captions):
                if step_name in caption:
                    action = caption.split(". ")[1].split(": ")[1].lower()
                    prev_observations = get_robot_plan(folder_name, step=step_name, with_obs=True)
                    if len(prev_observations) != 0:
                        prompt_name = 'reasoning-execution'
                    else:
                        prompt_name = 'reasoning-execution-no-history'
                    prompt['system'] = prompt_info[prompt_name]['template-system']
                    prompt['user'] = prompt_info[prompt_name]['template-user'].replace("[ACTION]", action)
                    prompt['user'] = prompt['user'].replace("[TASK_NAME]", task['name'])
                    prompt['user'] = prompt['user'].replace("[STEP]", step_name)
                    prompt['user'] = prompt['user'].replace("[SUMMARY]", prev_observations)
                    prompt['user'] = prompt['user'].replace("[OBSERVATION]", caption[caption.find("Action"):])
                    ans, _  = llm_prompter.query(prompt=prompt, sampling_params=prompt_info[prompt_name]['params'], 
                                                save=prompt_info[prompt_name]['save'], save_dir=save_dir)

                    print("[INFO] Predicted failure reason:", ans)
                    reasoning_dict['pred_failure_reason'] = ans

                    prompt = {}
                    prompt['system'] = prompt_info['reasoning-execution-steps']['template-system']
                    prompt['user'] = prompt_info['reasoning-execution-steps']['template-user'].replace("[FAILURE_REASON]", ans)
                    time_steps, _ = llm_prompter.query(prompt=prompt, sampling_params=prompt_info['reasoning-execution-steps']['params'],
                                                            save=prompt_info['reasoning-execution-steps']['save'], save_dir=save_dir)
                    
                    print("[INFO] Predicted failure time steps:", time_steps, time_steps.split(", "))
                    reasoning_dict['pred_failure_step'] = [time_step.replace(",", "") for time_step in time_steps.split(", ")]
                    break
    else:
        print(">>> All actions are executed successfully, run plan-level analysis...")

        prompt['system'] = prompt_info['reasoning-plan']['template-system']
        prompt['user'] = prompt_info['reasoning-plan']['template-user'].replace("[TASK_NAME]", task['name'])
        prompt['user'] = prompt['user'].replace("[SUCCESS_CONDITION]", task['success_condition'])
        prompt['user'] = prompt['user'].replace("[CURRENT_STATE]", get_scene_text(global_sg))
        prompt['user'] = prompt['user'].replace("[OBSERVATION]", get_robot_plan(folder_name, step=None, with_obs=False))
        ans, _ = llm_prompter.query(prompt=prompt, sampling_params=prompt_info['reasoning-plan']['params'], 
                                    save=prompt_info['reasoning-plan']['save'], save_dir=save_dir)
        
        print("[INFO] Predicted failure reason:", ans)
        reasoning_dict['pred_failure_reason'] = ans

        prompt['system'] = prompt_info['reasoning-plan-steps']['template-system']
        prompt['user'] = prompt_info['reasoning-plan-steps']['template-user'].replace("[PREV_PROMPT]", prompt['user'] + " " + ans)
        step, _ = llm_prompter.query(prompt=prompt, sampling_params=prompt_info['reasoning-plan-steps']['params'], 
                                    save=prompt_info['reasoning-plan-steps']['save'], save_dir=save_dir)
        step_str = step.split(" ")[0]
        if step_str[-1] == '.' or step_str[-1] == ',':
            step_str = step_str[:-1]

        print("[INFO] Predicted failure time steps:", step_str)
        reasoning_dict['pred_failure_step'] = step_str

    reasoning_dict['gt_failure_reason'] = task['gt_failure_reason']
    reasoning_dict['gt_failure_step'] = task['gt_failure_step']
    
    with open('state_summary/{}/{}'.format(folder_name, 'reasoning.json'), 'w') as f:
        json.dump(reasoning_dict, f)


def generate_replan(folder_name, llm_prompter, global_sg, last_event, task_object_list):
    with open(f'thor_tasks/{folder_name}/task.json') as f:
        task = json.load(f)
    curr_state = get_scene_text(global_sg)
    print("[INFO] Current state:", curr_state)
    global_object_list = list(set([obj["objectType"] for obj in last_event.metadata["objects"]]) | set(task_object_list))

    with open('state_summary/{}/reasoning.json'.format(folder_name), 'r') as f:
        data = json.load(f)
        reason = data["pred_failure_reason"]

    if os.path.exists(f'state_summary/{folder_name}/replan.json'):
        print("[INFO] Skipping replan generation")
        with open(f'state_summary/{folder_name}/replan.json', 'r') as f:
            plan = json.load(f)["original_plan"]
            plan = "\n".join(plan)
    else:
        with open('../LLM/prompts.json', 'r') as f:
            prompt_info = json.load(f)

        prompt = {}
        prompt['system'] = prompt_info['correction']['template-system'].replace("[PREFIX]", get_replan_prefix())
        prompt['user'] = prompt_info['correction']['template-user'].replace("[TASK_NAME]", task['name']).replace("[PLAN]", get_initial_plan(task['actions']))
        prompt['user'] = prompt['user'].replace("[FAILURE_REASON]", reason)
        prompt['user'] = prompt['user'].replace("[CURRENT_STATE]", curr_state).replace("[SUCCESS_CONDITION]", task['success_condition'])
    
        # print("=====================RE-PLAN PROMPT START========================")
        # print(prompt['system'])
        # print(prompt['user'])
        # print("=====================RE-PLAN PROMPT END==========================")

        plan, _ = llm_prompter.query(prompt=prompt, sampling_params=prompt_info['correction']['params'], 
                                save=prompt_info['correction']['save'], save_dir=f'../LLM/{folder_name}')

    translated_plan = translate_plan(plan, global_object_list, last_event)
    print("========================Translated plan===========================")
    print(translated_plan)

    replan_dict = {}

    with open(f'state_summary/{folder_name}/replan.json', 'w') as f:
        replan_dict["original_plan"] = plan.split("\n")
        replan_dict["plan"] = translated_plan.split("\n")[:-1]
        replan_dict["num_steps"] = len(replan_dict["plan"])
        json_object = json.dumps(replan_dict, indent=4)
        f.write(json_object)



File: main/gen_data.py

import os
import json
import numpy as np
import random
import pickle
from ai2thor.controller import Controller
from ai2thor.platform import CloudRendering
from action_primitives import *
from task_utils import *
from constants import *
from utils import *

def flatten_list(lis):
    output = []
    for item in lis:
        if isinstance(item, list):
            output.extend(item)
        else:
            output.append(item)
    return output

def get_failure_injection_idx(taskUtil, actions, task, action_idxs, nav_idxs, interact_cnt=0, nav_cnt=0):
    counter = 0
    print("[INFO] Injected failures:", taskUtil.failures_already_injected)
    try: 
        while True:
            if taskUtil.chosen_failure == 'missing_step':
                if "specified_missing_steps" in task:
                    cnt = 0
                    for f in taskUtil.failures_already_injected:
                        if f[0] == 'missing_step':
                            cnt += 1
                    if cnt < len(task['specified_missing_steps']):
                        failure_injection_idx = task['specified_missing_steps'][cnt] # can contain multiple indices
                        return failure_injection_idx
                
                failure_injection_idx = np.random.choice(action_idxs[interact_cnt:])
                if "toggle_off" in actions[failure_injection_idx] or "close_obj" in actions[failure_injection_idx]:
                    continue
                if len(taskUtil.failures_already_injected) == 0 or \
                    failure_injection_idx not in flatten_list([f[1] for f in taskUtil.failures_already_injected]):
                    return failure_injection_idx
            elif taskUtil.chosen_failure == 'failed_action':
                failure_injection_idx = np.random.choice(action_idxs[interact_cnt:])
                if "toggle_off" in actions[failure_injection_idx] or "close_obj" in actions[failure_injection_idx]:
                    continue
                if len(taskUtil.failures_already_injected) == 0 or \
                    failure_injection_idx not in flatten_list([f[1] for f in taskUtil.failures_already_injected]):
                    return failure_injection_idx
            elif taskUtil.chosen_failure == 'drop':
                failure_injection_idx = np.random.choice(nav_idxs[nav_cnt:])
                return failure_injection_idx

            if counter > 20:
                print(f"[INFO] Unable to inject a novel failure for failure type: {taskUtil.chosen_failure}. Choosing a new failuire type")            
                taskUtil.chosen_failure = np.random.choice(taskUtil.failures)
            if counter > 60:
                print("[INFO] Unable to inject a novel failure. Skipping this round. Maybe out of failures to inject.")
                return -1
            counter += 1
    except Exception as e:
        print("[INFO] Unable to inject a novel failure. Skipping this round. Maybe out of failures to inject:", e)
        return -1
    
def run_data_gen(data_path, task):
    np.random.seed(91)
    random.seed(91)

    # to avoid repetition in failure injection
    os.system("mkdir -p {}".format('thor_tasks/' + TASK_DICT[task["task_idx"]]))
    with open(f'thor_tasks/{TASK_DICT[task["task_idx"]]}/{task["folder_name"]}.pickle', 'wb') as handle:
        pickle.dump([], handle, protocol=pickle.HIGHEST_PROTOCOL)

    for i in range(int(task['num_samples'])):
        controller = Controller(
            agentMode="default",
            massThreshold=None,
            scene=task['scene'],
            visibilityDistance=1.5,
            gridSize=0.25,
            renderDepthImage=True,
            renderInstanceSegmentation=True,
            width=960,
            height=960,
            fieldOfView=60,
            platform=CloudRendering
        )
        # print("controller.last_event.metadata: ", controller.last_event.metadata['agent'])
        reachable_positions = controller.step(action="GetReachablePositions").metadata["actionReturn"]
        if 'chosen_failure' in task:
            chosen_failure = task['chosen_failure']
        else:
            chosen_failure = None
        reachable_positions = controller.step(action="GetReachablePositions").metadata["actionReturn"]
        if 'failure_injection_params' in task:
            failure_injection_params = task['failure_injection_params']
        else:
            failure_injection_params = None
        taskUtil = TaskUtil(folder_name=os.path.join(TASK_DICT[task["task_idx"]], task['folder_name']),
                            controller=controller,
                            reachable_positions=reachable_positions,
                            failure_injection=task['failure_injection'],
                            index=i,
                            repo_path=data_path,
                            chosen_failure=chosen_failure,
                            failure_injection_params=failure_injection_params)

        # for injecting blocking failure, explicitly changing the location of relevant objects
        if taskUtil.chosen_failure in ['blocking', 'occupied', 'occupied_put'] and 'failure_injection_params' in task:
            place_obj(taskUtil, task['failure_injection_params'])
        if taskUtil.chosen_failure == 'wrong perception' and 'disp_x' in taskUtil.failure_injection_params:
            place_obj(taskUtil, task['failure_injection_params'])

        # Add preaction steps (e.g. set mug to be dirty)
        if "preactions" in task:
            for preaction_instr in task['preactions']:
                lis = preaction_instr.split(',')
                lis = [item.strip("() ") for item in lis]
                preaction = lis[0]
                params = lis[1:]
                func = globals()[preaction]
                retval = func(taskUtil, *params)

        instrs, new_instrs = [], []
        action_idxs, nav_idxs = [], []
        for i, instr in enumerate(task['actions']):
            instrs.append(instr)
            lis = instr.split(',')
            lis = [item.strip("() ") for item in lis]
            action = lis[0]
            if action in taskUtil.interact_action_primitives:
                action_idxs.append(i)
            if 'navigate_to_obj' == action:
                nav_idxs.append(i)

        if task['failure_injection']:
            failure_injection_idx = get_failure_injection_idx(taskUtil, instrs, task, action_idxs, nav_idxs)
            if failure_injection_idx == -1:
                continue
            print("failure_injection_idx: ", failure_injection_idx)

        nav_counter = 0
        interact_counter = 0
        for i, instr in enumerate(instrs):
            lis = instr.split(',')
            lis = [item.strip("() ") for item in lis]
            action = lis[0]
            params = lis[1:]
            # print("action, params: ", action, params)
            func = globals()[action]
            if action in taskUtil.interact_action_primitives:
                interact_counter += 1
            if 'navigate_to_obj' == action:
                nav_counter += 1
            
            # dropping injection
            to_drop = False
            if not taskUtil.failure_added and taskUtil.chosen_failure == 'drop' and i == failure_injection_idx:
                to_drop = True
                params.append(to_drop)
                params.append(failure_injection_idx)

            # missing step(s) injection (remove from interact action)
            if not taskUtil.failure_added and taskUtil.chosen_failure == 'missing_step' and \
                action in taskUtil.interact_action_primitives:
                if not isinstance(failure_injection_idx, list):
                    failure_injection_idx = [failure_injection_idx]
                if i in failure_injection_idx:
                    if 'gt_failure_reason' in taskUtil.gt_failure:
                        taskUtil.gt_failure['gt_failure_reason'] += ', ' + instr
                    else:
                        taskUtil.gt_failure['gt_failure_reason'] = 'Missing ' + instr
                    taskUtil.gt_failure['gt_failure_step'] = taskUtil.counter + 1
                    if i == failure_injection_idx[-1]:
                        taskUtil.failure_added = True
                        taskUtil.failures_already_injected.append([taskUtil.chosen_failure, failure_injection_idx])
                    else:
                        taskUtil.failure_added = False
                    continue

            # failed execution injection 
            fail_execution = False
            if not taskUtil.failure_added and taskUtil.chosen_failure == 'failed_action' and \
                action in taskUtil.interact_action_primitives and i == failure_injection_idx:
                    print("Injecting failed action...")
                    fail_execution = True
                    taskUtil.gt_failure['gt_failure_reason'] = 'Failed to successfully execute ' + instr
                    taskUtil.gt_failure['gt_failure_step'] = taskUtil.counter + 1
                    taskUtil.failures_already_injected.append([taskUtil.chosen_failure, failure_injection_idx])
                    taskUtil.failure_added = True
                    params.append(fail_execution)

            new_instrs.append(instr)
            do_action = True
            if do_action:
                retval = func(taskUtil, *params)
            else:
                retval = func(taskUtil, *params, fail_execution=True)

            # if dropped failure was not successfully injected, find a different failure instance
            if retval == False:
                failure_injection_idx = get_failure_injection_idx(taskUtil, instrs, task, action_idxs, nav_idxs, 
                                                                    interact_cnt=interact_counter, nav_cnt=nav_counter)
                if failure_injection_idx == -1:
                    break

        # adding two buffer frames in the end
        for _ in range(2):
            e = controller.step(action="Done")
            save_data(taskUtil, e)

        print("[INFO] interact_actions:", taskUtil.interact_actions)
        print("[INFO] nav_actions:", taskUtil.nav_actions)

        with open(f'thor_tasks/{taskUtil.specific_folder_name}/interact_actions.pickle', 'wb') as handle:
            pickle.dump(taskUtil.interact_actions, handle, protocol=pickle.HIGHEST_PROTOCOL)

        with open(f'thor_tasks/{taskUtil.specific_folder_name}/nav_actions.pickle', 'wb') as handle:
            pickle.dump(taskUtil.nav_actions, handle, protocol=pickle.HIGHEST_PROTOCOL)

        with open(f'thor_tasks/{TASK_DICT[task["task_idx"]]}/{task["folder_name"]}.pickle', 'wb') as handle:
            pickle.dump(taskUtil.failures_already_injected, handle, protocol=pickle.HIGHEST_PROTOCOL)

        updated_task = task.copy()
        updated_task['specific_folder_name'] = taskUtil.specific_folder_name
        # in case no failure was added
        if 'gt_failure_reason' not in taskUtil.gt_failure:
            taskUtil.gt_failure['gt_failure_reason'] = 'No failure added'
            taskUtil.gt_failure['gt_failure_step'] = 0
        
        if 'gt_failure_reason' not in updated_task:
            updated_task['gt_failure_reason'] = taskUtil.gt_failure['gt_failure_reason']
            updated_task['gt_failure_step'] = convert_step_to_timestep(taskUtil.gt_failure['gt_failure_step'], video_fps=1)
        updated_task['unity_name_map'] = taskUtil.unity_name_map
        updated_task['sounds'] = taskUtil.sounds
        updated_task['actions'] = new_instrs
        with open(f'thor_tasks/{taskUtil.specific_folder_name}/task.json', 'w') as f:
            json.dump(updated_task, f)
        
        # print("[INFO] interact actions: ", taskUtil.interact_actions)
        # print("[INFO] nav_actions: ", taskUtil.nav_actions)

        # save video of the executed task
        generate_video(taskUtil, recovery_video=False)

        # end of task steps
        controller.stop()
        


File: main/get_local_sg.py

import os
import math
import numpy as np
import torch
torch.set_grad_enabled(False)
torch.manual_seed(0)

from point_cloud_utils import *
import open3d as o3d
from PIL import Image
from scene_graph import *
from utils import *

COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],
          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]


def get_scene_graph(step_idx, event, object_list, total_points_dict, bbox3d_dict, obj_held_prev, task):
    pcd_dict, depth_dict  = {}, {}
    height, width, channel = event.frame.shape
    camera_space_xyz = depth_frame_to_camera_space_xyz(
            depth_frame=torch.as_tensor(event.depth_frame.copy()), mask=None, fov=event.metadata['fov'])
    x = event.metadata['agent']['position']['x']
    y = event.metadata['agent']['position']['y']
    z = event.metadata['agent']['position']['z']

    if not event.metadata['agent']['isStanding']:
        y = y - 0.22

    world_points = camera_space_xyz_to_world_xyz(
        camera_space_xyzs=camera_space_xyz,
        camera_world_xyz=torch.as_tensor([x, y, z]),
        rotation=event.metadata['agent']['rotation']['y'],
        horizon=event.metadata['agent']['cameraHorizon'],
    ).reshape(channel, height, width).permute(1, 2, 0)

    sinkbasin_pts = None
    for object_id in event.instance_masks:
        if object_id.split("|")[0] in ["Window", "Floor", "Wall", "Ceiling", "Cabinet"]:
            continue
        label = object_id

        # register this box in 3D
        mask = event.instance_masks[object_id].reshape(height, width)
        obj_points = torch.as_tensor(world_points[mask])
        
        if len(obj_points) < 700:
            continue
        
        depth_dict[label] = event.depth_frame[mask]
        # obj_colors = torch.as_tensor(world_colors[mask])

        # downsample point cloud
        obj_pcd = o3d.geometry.PointCloud()
        obj_pcd.points = o3d.utility.Vector3dVector(obj_points)
        voxel_down_pcd = obj_pcd.voxel_down_sample(voxel_size=0.01)

        # denoise point cloud
        if "Pan" == label.split("|")[0] or "EggCracked" == label.split("|")[0] or "Bowl" == label.split("|")[0] or "Pot" == label.split("|")[0]:
            _, ind = voxel_down_pcd.remove_radius_outlier(nb_points=30, radius=0.03)
            inlier = voxel_down_pcd.select_by_index(ind)
            pcd_dict[label] = torch.tensor(np.array(inlier.points))
        elif "CounterTop" == label.split("|")[0]:
            _, ind = voxel_down_pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=0.1)
            inlier = voxel_down_pcd.select_by_index(ind)
            pcd_dict[label] = torch.tensor(np.array(inlier.points))
        elif "SinkBasin" in label:
            sinkbasin_pts = torch.tensor(np.array(voxel_down_pcd.points))
        else:
            pcd_dict[label] = torch.tensor(np.array(voxel_down_pcd.points))
            # assert points[label].shape == colors[label].shape
    #==============================================================

    for label in pcd_dict.keys():
        for keyword in ["Sliced", "Cracked"]:
            if keyword in label:
                tmp = ""
                for key in total_points_dict.keys():
                    if len(key.split("|")) == 4 and key.split("|")[0] == label.split("|")[0]:
                        tmp = key
                if len(tmp)!=0 and (keyword not in tmp) and (tmp in total_points_dict):
                    print("remove object:", tmp)
                    del total_points_dict[tmp]

        if label not in total_points_dict:
            total_points_dict[label] = pcd_dict[label]

        if is_receptacle(label, event):
            if is_moving(label, event) or is_picked_up(label, event) or obj_held_prev == label:
                total_points_dict[label] = pcd_dict[label]
            else:
                total_points_dict[label] = torch.unique(torch.cat((total_points_dict[label], pcd_dict[label]), 0), dim=0)
        else:
            total_points_dict[label] = pcd_dict[label]

        if label.split("|")[0] == "Sink" and sinkbasin_pts is not None:
            total_points_dict[label] = torch.unique(torch.cat((total_points_dict[label], sinkbasin_pts), 0), dim=0)
    
    # remove dropped object 
    if obj_held_prev not in pcd_dict.keys():
        if obj_held_prev in total_points_dict.keys():
            print("remove object:", obj_held_prev)
            del total_points_dict[obj_held_prev]

    for _, label in enumerate(total_points_dict.keys()):
        boxes3d_pts = o3d.utility.Vector3dVector(total_points_dict[label])
        box = o3d.geometry.AxisAlignedBoundingBox.create_from_points(boxes3d_pts)
        bbox3d_dict[label] = box

    # Generate local scene graph
    local_sg = SceneGraph(event, task)
    for label in pcd_dict.keys():
        name = get_label_from_object_id(label, [event], task)
        bbox = get_2d_bbox_from_3d_pcd(event, label, total_points_dict)
        if name is not None and bbox is not None:
            node = Node(name, 
                        object_id=label, 
                        pos3d=bbox3d_dict[label].get_center(), 
                        corner_pts=np.array(bbox3d_dict[label].get_box_points()), 
                        bbox2d=bbox, 
                        pcd=total_points_dict[label],
                        depth=depth_dict[label])
            local_sg.add_node_wo_edge(node)

    # check if need to add a node and its edges for each object in the instance segmentation
    for label in pcd_dict.keys():
        object_name = label.split("|")[0]
        if object_name in object_list:
            node = next((node for node in local_sg.total_nodes if node.object_id == label), None)
            if node is not None:
                local_sg.add_node(node)

    obj_held_prev = local_sg.add_agent()
    
    return local_sg, total_points_dict, obj_held_prev, bbox3d_dict


def get_2d_bbox_from_3d_pcd(event, label, total_points_dict):
    x = event.metadata['agent']['position']['x']
    y = event.metadata['agent']['position']['y']
    z = event.metadata['agent']['position']['z']

    gt_mask = event.instance_masks[label].reshape(event.metadata["screenHeight"], event.metadata["screenWidth"])
    gt_mask_indices = np.where(gt_mask==1)

    mask_img = np.zeros_like(event.frame)
    pred_mask = np.array(world_space_xyz_to_2d_pixel(
        world_space_xyzs=total_points_dict[label].permute(1, 0),
        camera_world_xyz=torch.as_tensor([x, y, z]),
        rotation=event.metadata['agent']['rotation']['y'],
        horizon=event.metadata['agent']['cameraHorizon'],
        fov=event.metadata["fov"], 
        width=event.metadata["screenWidth"], 
        height=event.metadata["screenHeight"],
    ))
    
    try:
        mask_img[gt_mask_indices[0], gt_mask_indices[1]] = (0, 255, 0)
        pred_mask[1, :] = -pred_mask[1, :] + (event.metadata["screenHeight"]-1)

        valid_pixel = np.logical_and(pred_mask[0, :] >= 0,
            np.logical_and(pred_mask[0, :] < event.metadata["screenWidth"],
            np.logical_and(pred_mask[1, :] >= 0,
            pred_mask[1, :] < event.metadata["screenHeight"])))
        
        filtered_pred_mask = pred_mask[:, valid_pixel]

        # gt_bbox = (np.min(gt_mask_indices[0]), np.min(gt_mask_indices[1]), np.max(gt_mask_indices[0]), np.max(gt_mask_indices[1]))
        bbox = (np.min(filtered_pred_mask[1, :]), np.min(filtered_pred_mask[0, :]), np.max(filtered_pred_mask[1, :]), np.max(filtered_pred_mask[0, :]))
    except ValueError:
        return None
    return bbox


def save_pcd(folder_name, total_points_dict, camera_coord=False):
    total_points, total_colors = None, None
    os.system("mkdir -p scene/{}".format(folder_name))
    for i, label in enumerate(total_points_dict.keys()):
        if total_points is None:
            total_points = total_points_dict[label]
            c = torch.tensor(COLORS[i%len(COLORS)])
            total_colors = c.repeat(len(total_points_dict[label]), 1)
        else:
            total_points = torch.cat((total_points, total_points_dict[label]), 0)
            c = torch.tensor(COLORS[i%len(COLORS)])
            total_colors = torch.cat((total_colors, c.repeat(len(total_points_dict[label]), 1)), 0)

    # save pcd to file
    if total_points is not None:
        saved_pcd = o3d.geometry.PointCloud()
        saved_pcd.points = o3d.utility.Vector3dVector(total_points)
        saved_pcd.colors = o3d.utility.Vector3dVector(total_colors)
        if camera_coord:
            o3d.io.write_point_cloud("scene/{}/scene-cam.ply".format(folder_name), saved_pcd)
        else:
            o3d.io.write_point_cloud("scene/{}/scene.ply".format(folder_name), saved_pcd)



File: main/point_cloud_utils.py

# MIT License
#
# Original Copyright (c) 2020 Devendra Chaplot
#
# Modified work Copyright (c) 2021 Allen Institute for Artificial Intelligence
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import math
import numpy as np
import torch
from typing import Optional, Sequence, cast
from projection.projection_ops import project_3d_camera_points_to_2d_pixels

def camera_space_xyz_to_world_xyz(
    camera_space_xyzs: torch.Tensor,
    camera_world_xyz: torch.Tensor,
    rotation: float,
    horizon: float,
) -> torch.Tensor:
    """Transforms xyz coordinates in the camera's coordinate frame to world-
    space (global) xyz frame.

    This code has been adapted from https://github.com/devendrachaplot/Neural-SLAM.

    **IMPORTANT:** We use the conventions from the Unity game engine. In particular:

    * A rotation of 0 corresponds to facing north.
    * Positive rotations correspond to CLOCKWISE rotations. That is a rotation of 90 degrees corresponds
        to facing east. **THIS IS THE OPPOSITE CONVENTION OF THE ONE GENERALLY USED IN MATHEMATICS.**
    * When facing NORTH (rotation==0) moving ahead by 1 meter results in the the z coordinate
        increasing by 1. Moving to the right by 1 meter corresponds to increasing the x coordinate by 1.
         Finally moving upwards by 1 meter corresponds to increasing the y coordinate by 1.
         **Having x,z as the ground plane in this way is common in computer graphics but is different than
         the usual mathematical convention of having z be "up".**
    * The horizon corresponds to how far below the horizontal the camera is facing. I.e. a horizon
        of 30 corresponds to the camera being angled downwards at an angle of 30 degrees.

    # Parameters
    camera_space_xyzs : A 3xN matrix of xyz coordinates in the camera's reference frame.
        Here `x, y, z = camera_space_xyzs[:, i]` should equal the xyz coordinates for the ith point.
    camera_world_xyz : The camera's xyz position in the world reference frame.
    rotation : The world-space rotation (in degrees) of the camera.
    horizon : The horizon (in degrees) of the camera.

    # Returns
    3xN tensor with entry [:, i] is the xyz world-space coordinate corresponding to the camera-space
    coordinate camera_space_xyzs[:, i]
    """
    # Adapted from https://github.com/devendrachaplot/Neural-SLAM.

    # First compute the transformation that points undergo
    # due to the camera's horizon
    psi = -horizon * np.pi / 180
    cos_psi = np.cos(psi)
    sin_psi = np.sin(psi)
    # fmt: off
    horizon_transform = camera_space_xyzs.new(
        [
            [1, 0, 0], # unchanged
            [0, cos_psi, sin_psi],
            [0, -sin_psi, cos_psi,],
        ],
    )
    # fmt: on

    # Next compute the transformation that points undergo
    # due to the agent's rotation about the y-axis
    phi = -rotation * np.pi / 180
    cos_phi = np.cos(phi)
    sin_phi = np.sin(phi)
    # fmt: off
    rotation_transform = camera_space_xyzs.new(
        [
            [cos_phi, 0, -sin_phi],
            [0, 1, 0], # unchanged
            [sin_phi, 0, cos_phi],],
    )
    # fmt: on

    # Apply the above transformations
    view_points = (rotation_transform @ horizon_transform) @ camera_space_xyzs

    # Translate the points w.r.t. the camera's position in world space.
    world_points = view_points + camera_world_xyz[:, None]
    return world_points

def world_space_xyz_to_camera_space_xyz(world_space_xyzs, camera_world_xyz, rotation, horizon):
    psi = -horizon * np.pi / 180
    cos_psi = np.cos(psi)
    sin_psi = np.sin(psi)
    horizon_transform = world_space_xyzs.new(
        [
            [1, 0, 0],
            [0, cos_psi, sin_psi],
            [0, -sin_psi, cos_psi,],
        ],
    )
    phi = -rotation * np.pi / 180
    cos_phi = np.cos(phi)
    sin_phi = np.sin(phi)
    rotation_transform = world_space_xyzs.new(
        [
            [cos_phi, 0, -sin_phi],
            [0, 1, 0],
            [sin_phi, 0, cos_phi],],
    )

    world2cam_mat = torch.inverse(rotation_transform @ horizon_transform)
    # camera_space_xyzs: 3 x N
    camera_space_xyzs = (world2cam_mat @ (world_space_xyzs - camera_world_xyz[:, None])).float()

    return camera_space_xyzs

def world_space_xyz_to_2d_pixel(world_space_xyzs, camera_world_xyz, rotation, horizon, fov, width, height):
    camera_space_xyzs = world_space_xyz_to_camera_space_xyz(world_space_xyzs, camera_world_xyz, rotation, horizon)
    camera_space_xyzs = camera_space_xyzs.reshape(1, camera_space_xyzs.shape[0], camera_space_xyzs.shape[1])
    points_px_camera, _ = project_3d_camera_points_to_2d_pixels(height, width, fov, camera_space_xyzs)
    return (points_px_camera[0]).long()


def depth_frame_to_camera_space_xyz(
    depth_frame: torch.Tensor, mask: Optional[torch.Tensor], fov: float = 90
) -> torch.Tensor:
    """Transforms a input depth map into a collection of xyz points (i.e. a
    point cloud) in the camera's coordinate frame.

    # Parameters
    depth_frame : A square depth map, i.e. an MxM matrix with entry `depth_frame[i, j]` equaling
        the distance from the camera to nearest surface at pixel (i,j).
    mask : An optional boolean mask of the same size (MxM) as the input depth. Only values
        where this mask are true will be included in the returned matrix of xyz coordinates. If
        `None` then no pixels will be masked out (so the returned matrix of xyz points will have
        dimension 3x(M*M)
    fov: The field of view of the camera.

    # Returns

    A 3xN matrix with entry [:, i] equalling a the xyz coordinates (in the camera's coordinate
    frame) of a point in the point cloud corresponding to the input depth frame.
    """
    assert (
        len(depth_frame.shape) == 2 and depth_frame.shape[0] == depth_frame.shape[1]
    ), f"depth has shape {depth_frame.shape}, we only support (N, N) shapes for now."

    resolution = depth_frame.shape[0]
    if mask is None:
        mask = torch.ones_like(depth_frame, dtype=torch.bool)

    # pixel centers
    camera_space_yx_offsets = (
        torch.stack(torch.where(mask))
        + 0.5  # Offset by 0.5 so that we are in the middle of the pixel
    )

    # Subtract center
    camera_space_yx_offsets -= resolution / 2.0

    # Make "up" in y be positive
    camera_space_yx_offsets[0, :] *= -1

    # Put points on the clipping plane
    camera_space_yx_offsets *= (2.0 / resolution) * math.tan((fov / 2) / 180 * math.pi)

    # noinspection PyArgumentList
    camera_space_xyz = torch.cat(
        [
            camera_space_yx_offsets[1:, :],  # This is x
            camera_space_yx_offsets[:1, :],  # This is y
            torch.ones_like(camera_space_yx_offsets[:1, :]),
        ],
        axis=0,
    )

    return camera_space_xyz * depth_frame[mask][None, :]


def depth_frame_to_world_space_xyz(
    depth_frame: torch.Tensor,
    camera_world_xyz: torch.Tensor,
    rotation: float,
    horizon: float,
    fov: float,
):
    """Transforms a input depth map into a collection of xyz points (i.e. a
    point cloud) in the world-space coordinate frame.

    **IMPORTANT:** We use the conventions from the Unity game engine. In particular:

    * A rotation of 0 corresponds to facing north.
    * Positive rotations correspond to CLOCKWISE rotations. That is a rotation of 90 degrees corresponds
        to facing east. **THIS IS THE OPPOSITE CONVENTION OF THE ONE GENERALLY USED IN MATHEMATICS.**
    * When facing NORTH (rotation==0) moving ahead by 1 meter results in the the z coordinate
        increasing by 1. Moving to the right by 1 meter corresponds to increasing the x coordinate by 1.
         Finally moving upwards by 1 meter corresponds to increasing the y coordinate by 1.
         **Having x,z as the ground plane in this way is common in computer graphics but is different than
         the usual mathematical convention of having z be "up".**
    * The horizon corresponds to how far below the horizontal the camera is facing. I.e. a horizon
        of 30 corresponds to the camera being angled downwards at an angle of 30 degrees.

    # Parameters
    depth_frame : A square depth map, i.e. an MxM matrix with entry `depth_frame[i, j]` equaling
        the distance from the camera to nearest surface at pixel (i,j).
    mask : An optional boolean mask of the same size (MxM) as the input depth. Only values
        where this mask are true will be included in the returned matrix of xyz coordinates. If
        `None` then no pixels will be masked out (so the returned matrix of xyz points will have
        dimension 3x(M*M)
    camera_space_xyzs : A 3xN matrix of xyz coordinates in the camera's reference frame.
        Here `x, y, z = camera_space_xyzs[:, i]` should equal the xyz coordinates for the ith point.
    camera_world_xyz : The camera's xyz position in the world reference frame.
    rotation : The world-space rotation (in degrees) of the camera.
    horizon : The horizon (in degrees) of the camera.
    fov: The field of view of the camera.

    # Returns

    A 3xN matrix with entry [:, i] equalling a the xyz coordinates (in the world coordinate
    frame) of a point in the point cloud corresponding to the input depth frame.
    """

    camera_space_xyz = depth_frame_to_camera_space_xyz(
        depth_frame=depth_frame, mask=None, fov=fov
    )

    world_points = camera_space_xyz_to_world_xyz(
        camera_space_xyzs=camera_space_xyz,
        camera_world_xyz=camera_world_xyz,
        rotation=rotation,
        horizon=horizon,
    )

    return world_points.view(3, *depth_frame.shape).permute(1, 2, 0)


def project_point_cloud_to_map(
    xyz_points: torch.Tensor,
    bin_axis: str,
    bins: Sequence[float],
    map_size: int,
    resolution_in_cm: int,
    flip_row_col: bool,
):
    """Bins an input point cloud into a map tensor with the bins equaling the
    channels.

    This code has been adapted from https://github.com/devendrachaplot/Neural-SLAM.

    # Parameters
    xyz_points : (x,y,z) pointcloud(s) as a torch.Tensor of shape (... x height x width x 3).
        All operations are vectorized across the `...` dimensions.
    bin_axis : Either "x", "y", or "z", the axis which should be binned by the values in `bins`.
        If you have generated your point clouds with any of the other functions in the `point_cloud_utils`
        module you almost certainly want this to be "y" as this is the default upwards dimension.
    bins: The values by which to bin along `bin_axis`, see the `bins` parameter of `np.digitize`
        for more info.
    map_size : The axes not specified by `bin_axis` will be be divided by `resolution_in_cm / 100`
        and then rounded to the nearest integer. They are then expected to have their values
        within the interval [0, ..., map_size - 1].
    resolution_in_cm: The resolution_in_cm, in cm, of the map output from this function. Every
        grid square of the map corresponds to a (`resolution_in_cm`x`resolution_in_cm`) square
        in space.
    flip_row_col: Should the rows/cols of the map be flipped? See the 'Returns' section below for more
        info.

    # Returns
    A collection of maps of shape (... x map_size x map_size x (len(bins)+1)), note that bin_axis
    has been moved to the last index of this returned map, the other two axes stay in their original
    order unless `flip_row_col` has been called in which case they are reversed (useful as often
    rows should correspond to y or z instead of x).
    """
    bin_dim = ["x", "y", "z"].index(bin_axis)

    start_shape = xyz_points.shape
    xyz_points = xyz_points.reshape([-1, *start_shape[-3:]])
    num_clouds, h, w, _ = xyz_points.shape

    if not flip_row_col:
        new_order = [i for i in [0, 1, 2] if i != bin_dim] + [bin_dim]
    else:
        new_order = [i for i in [2, 1, 0] if i != bin_dim] + [bin_dim]

    uvw_points = cast(
        torch.Tensor, torch.stack([xyz_points[..., i] for i in new_order], dim=-1)
    )

    num_bins = len(bins) + 1

    isnotnan = ~torch.isnan(xyz_points[..., 0])

    uvw_points_binned: torch.Tensor = torch.cat(
        (
            torch.round(100 * uvw_points[..., :-1] / resolution_in_cm).long(),
            torch.bucketize(
                uvw_points[..., -1:].contiguous(), boundaries=uvw_points.new(bins)
            ),
        ),
        dim=-1,
    )

    maxes = (
        xyz_points.new()
        .long()
        .new([map_size, map_size, num_bins])
        .reshape((1, 1, 1, 3))
    )

    isvalid = torch.logical_and(
        torch.logical_and(
            (uvw_points_binned >= 0).all(-1), (uvw_points_binned < maxes).all(-1),
        ),
        isnotnan,
    )

    uvw_points_binned_with_index_mat = torch.cat(
        (
            torch.repeat_interleave(
                torch.arange(0, num_clouds).to(xyz_points.device), h * w
            ).reshape(-1, 1),
            uvw_points_binned.reshape(-1, 3),
        ),
        dim=1,
    )

    uvw_points_binned_with_index_mat[~isvalid.reshape(-1), :] = 0
    ind = (
        uvw_points_binned_with_index_mat[:, 0] * (map_size * map_size * num_bins)
        + uvw_points_binned_with_index_mat[:, 1] * (map_size * num_bins)
        + uvw_points_binned_with_index_mat[:, 2] * num_bins
        + uvw_points_binned_with_index_mat[:, 3]
    )
    ind[~isvalid.reshape(-1)] = 0
    count = torch.bincount(
        ind.view(-1),
        isvalid.view(-1).long(),
        minlength=num_clouds * map_size * map_size * num_bins,
    )

    return count.view(*start_shape[:-3], map_size, map_size, num_bins)


################
# FOR DEBUGGNG #
################
# The below functions are versions of the above which, because of their reliance on
# numpy functions, cannot use GPU acceleration. These are possibly useful for debugging,
# performance comparisons, or for validating that the above GPU variants work properly.


def _cpu_only_camera_space_xyz_to_world_xyz(
    camera_space_xyzs: np.ndarray,
    camera_world_xyz: np.ndarray,
    rotation: float,
    horizon: float,
):
    # Adapted from https://github.com/devendrachaplot/Neural-SLAM.

    # view_position = 3, world_points = 3 x N
    # NOTE: camera_position is not equal to agent_position!!

    # First compute the transformation that points undergo
    # due to the camera's horizon
    psi = -horizon * np.pi / 180
    cos_psi = np.cos(psi)
    sin_psi = np.sin(psi)
    # fmt: off
    horizon_transform = np.array(
        [
            [1, 0, 0], # unchanged
            [0, cos_psi, sin_psi],
            [0, -sin_psi, cos_psi,],
        ],
        np.float64,
    )
    # fmt: on

    # Next compute the transformation that points undergo
    # due to the agent's rotation about the y-axis
    phi = -rotation * np.pi / 180
    cos_phi = np.cos(phi)
    sin_phi = np.sin(phi)
    # fmt: off
    rotation_transform = np.array(
        [
            [cos_phi, 0, -sin_phi],
            [0, 1, 0], # unchanged
            [sin_phi, 0, cos_phi],],
        np.float64,
    )
    # fmt: on

    # Apply the above transformations
    view_points = (rotation_transform @ horizon_transform) @ camera_space_xyzs

    # Translate the points w.r.t. the camera's position in world space.
    world_points = view_points + camera_world_xyz[:, None]
    return world_points


def _cpu_only_depth_frame_to_camera_space_xyz(
    depth_frame: np.ndarray, mask: Optional[np.ndarray], fov: float = 90
):
    """"""
    assert (
        len(depth_frame.shape) == 2 and depth_frame.shape[0] == depth_frame.shape[1]
    ), f"depth has shape {depth_frame.shape}, we only support (N, N) shapes for now."

    resolution = depth_frame.shape[0]
    if mask is None:
        mask = np.ones(depth_frame.shape, dtype=bool)

    # pixel centers
    camera_space_yx_offsets = (
        np.stack(np.where(mask))
        + 0.5  # Offset by 0.5 so that we are in the middle of the pixel
    )

    # Subtract center
    camera_space_yx_offsets -= resolution / 2.0

    # Make "up" in y be positive
    camera_space_yx_offsets[0, :] *= -1

    # Put points on the clipping plane
    camera_space_yx_offsets *= (2.0 / resolution) * math.tan((fov / 2) / 180 * math.pi)

    camera_space_xyz = np.concatenate(
        [
            camera_space_yx_offsets[1:, :],  # This is x
            camera_space_yx_offsets[:1, :],  # This is y
            np.ones_like(camera_space_yx_offsets[:1, :]),
        ],
        axis=0,
    )

    return camera_space_xyz * depth_frame[mask][None, :]


def _cpu_only_depth_frame_to_world_space_xyz(
    depth_frame: np.ndarray,
    camera_world_xyz: np.ndarray,
    rotation: float,
    horizon: float,
    fov: float,
):
    camera_space_xyz = _cpu_only_depth_frame_to_camera_space_xyz(
        depth_frame=depth_frame, mask=None, fov=fov
    )

    world_points = _cpu_only_camera_space_xyz_to_world_xyz(
        camera_space_xyzs=camera_space_xyz,
        camera_world_xyz=camera_world_xyz,
        rotation=rotation,
        horizon=horizon,
    )

    return world_points.reshape((3, *depth_frame.shape)).transpose((1, 2, 0))


def _cpu_only_project_point_cloud_to_map(
    xyz_points: np.ndarray,
    bin_axis: str,
    bins: Sequence[float],
    map_size: int,
    resolution_in_cm: int,
    flip_row_col: bool,
):
    """Bins points into  bins.

    Adapted from https://github.com/devendrachaplot/Neural-SLAM.

    # Parameters
    xyz_points : (x,y,z) point clouds as a np.ndarray of shape (... x height x width x 3). (x,y,z)
        should be coordinates specified in meters.
    bin_axis : Either "x", "y", or "z", the axis which should be binned by the values in `bins`
    bins: The values by which to bin along `bin_axis`, see the `bins` parameter of `np.digitize`
        for more info.
    map_size : The axes not specified by `bin_axis` will be be divided by `resolution_in_cm / 100`
        and then rounded to the nearest integer. They are then expected to have their values
        within the interval [0, ..., map_size - 1].
    resolution_in_cm: The resolution_in_cm, in cm, of the map output from this function. Every
        grid square of the map corresponds to a (`resolution_in_cm`x`resolution_in_cm`) square
        in space.
    flip_row_col: Should the rows/cols of the map be flipped

    # Returns
    A collection of maps of shape (... x map_size x map_size x (len(bins)+1)), note that bin_axis
    has been moved to the last index of this returned map, the other two axes stay in their original
    order unless `flip_row_col` has been called in which case they are reversed (useful if you give
    points as often rows should correspond to y or z instead of x).
    """
    bin_dim = ["x", "y", "z"].index(bin_axis)

    start_shape = xyz_points.shape
    xyz_points = xyz_points.reshape([-1, *start_shape[-3:]])
    num_clouds, h, w, _ = xyz_points.shape

    if not flip_row_col:
        new_order = [i for i in [0, 1, 2] if i != bin_dim] + [bin_dim]
    else:
        new_order = [i for i in [2, 1, 0] if i != bin_dim] + [bin_dim]

    uvw_points: np.ndarray = np.stack([xyz_points[..., i] for i in new_order], axis=-1)

    num_bins = len(bins) + 1

    isnotnan = ~np.isnan(xyz_points[..., 0])

    uvw_points_binned = np.concatenate(
        (
            np.round(100 * uvw_points[..., :-1] / resolution_in_cm).astype(np.int32),
            np.digitize(uvw_points[..., -1:], bins=bins).astype(np.int32),
        ),
        axis=-1,
    )

    maxes = np.array([map_size, map_size, num_bins]).reshape((1, 1, 1, 3))

    isvalid = np.logical_and.reduce(
        (
            (uvw_points_binned >= 0).all(-1),
            (uvw_points_binned < maxes).all(-1),
            isnotnan,
        )
    )

    uvw_points_binned_with_index_mat = np.concatenate(
        (
            np.repeat(np.arange(0, num_clouds), h * w).reshape(-1, 1),
            uvw_points_binned.reshape(-1, 3),
        ),
        axis=1,
    )

    uvw_points_binned_with_index_mat[~isvalid.reshape(-1), :] = 0
    ind = np.ravel_multi_index(
        uvw_points_binned_with_index_mat.transpose(),
        (num_clouds, map_size, map_size, num_bins),
    )
    ind[~isvalid.reshape(-1)] = 0
    count = np.bincount(
        ind.ravel(),
        isvalid.ravel().astype(np.int32),
        minlength=num_clouds * map_size * map_size * num_bins,
    )

    return count.reshape([*start_shape[:-3], map_size, map_size, num_bins])



File: main/projection/__init__.py




File: main/projection/constants.py

# ROUNDING_OFFSET = 0.5
ROUNDING_OFFSET = 0.0


File: main/projection/image_to_pointcloud.py

import os
import numpy as np
import torch
import torch.nn as nn
import imageio

from kornia.geometry.camera import PinholeCamera
from kornia.geometry.depth import depth_to_3d
from projection.utils import make_pinhole_camera_matrix

# from lgp.utils.utils import standardize_image
# from lgp.paths import get_artifact_output_path

# import lgp.utils.render3d as r3d
# from lgp.utils.utils import save_gif


class ImageToPointcloud(nn.Module):
    """
    Projects a first-person image to a PointCloud
    """
    def __init__(self):
        super().__init__()

    def forward(self, camera_image, depth_image, extrinsics4f, hfov_deg, min_depth=0.7):
        batch_size = camera_image.shape[0]
        dev = camera_image.device
        b, c, h, w = camera_image.shape

        intrinsics = make_pinhole_camera_matrix(
            height_px=h,
            width_px=w,
            hfov_deg = hfov_deg
        )
        intrinsics = intrinsics.to(dev)
        intrinsics = intrinsics[None, :, :].repeat((batch_size, 1, 1))
        # Extrinsics project world points to camera
        extrinsics = extrinsics4f.to(dev).float()
        # Inverse extrinsics project camera points to the world
        inverse_extrinsics = extrinsics.inverse()
        # Repeat over batch if needed
        if inverse_extrinsics.shape[0] == 1 and b > 1:
            inverse_extrinsics = inverse_extrinsics.repeat((b, 1, 1))

        # Points3D - 1 x 3 x H x W grid of coordinates
        points_3d_wrt_camera = depth_to_3d(depth=depth_image,
                                           camera_matrix=intrinsics,
                                           normalize_points=True)

        has_depth = depth_image > min_depth

        # Project to world reference frame by applying the extrinsic homogeneous transformation matrix
        homo_ones = torch.ones_like(points_3d_wrt_camera[:, 0:1, :, :])
        homo_points_3d_wrt_camera = torch.cat([points_3d_wrt_camera, homo_ones], dim=1)
        homo_points_3d_wrt_world = torch.einsum("bxhw,byx->byhw", homo_points_3d_wrt_camera, inverse_extrinsics)
        points_3d_wrt_world = homo_points_3d_wrt_world[:, :3, :, :]

        # Plot to sanity check
        if False:
            imgid = 26
            outdir = os.path.join(get_artifact_output_path(), "3dviz")
            os.makedirs(outdir, exist_ok=True)
            img_w = r3d.render_aligned_point_cloud(points_3d_wrt_world, camera_image, animate=True)
            img_c = r3d.render_aligned_point_cloud(points_3d_wrt_camera, camera_image, animate=True)
            save_gif(img_w, os.path.join(outdir, f"pointcloud_test_global_{imgid}.gif"))
            save_gif(img_c, os.path.join(outdir, f"pointcloud_test_camera_{imgid}.gif"))
            #imageio.imsave(os.path.join(outdir, f"pointcloud_test_global_{imgid}.gif"), img_w)
            #imageio.imsave(os.path.join(outdir, f"pointcloud_test_camera_{imgid}.gif"), img_c)

            imageio.imsave(os.path.join(outdir, f"pointcloud_test_scene_{imgid}.png"), standardize_image(camera_image[0]))
            imageio.imsave(os.path.join(outdir, f"pointcloud_test_depth_{imgid}.png"), standardize_image(depth_image[0]))

            # o3d.visualization.draw_geometries([pcd])

        # points_3d_wrt_world_with_attrs = torch.cat([points_3d_wrt_world, camera_image], dim=1)
        # B x (3 + C) x H x W  : tensor of 3D points with color/feature attributes

        # Zero out the points that are at the camera location
        # points_3d_wrt_world_with_attrs = points_3d_wrt_world_with_attrs * has_depth

        points_3d_wrt_world = points_3d_wrt_world * has_depth
        camera_image = camera_image * has_depth

        return points_3d_wrt_world, camera_image


File: main/projection/image_to_voxels.py

import torch.nn as nn
import torch

from projection.image_to_pointcloud import ImageToPointcloud
from projection.pointcloud_to_voxelgrid import PointcloudToVoxels


class ImageToVoxels(nn.Module):

    def __init__(self):
        super().__init__()
        self.image_to_pointcloud = ImageToPointcloud()
        self.pointcloud_to_voxels = PointcloudToVoxels()

    def forward(self, scene, depth, extrinsics4f, hfov_deg, mark_agent=False):
        # CPU doesn't support most of the half-precision operations.
        if scene.device == "cpu":
            scene = scene.float()
            depth = depth.float()
        point_coords, img = self.image_to_pointcloud(scene, depth, extrinsics4f, hfov_deg)
        voxel_grid = self.pointcloud_to_voxels(point_coords, img)
        return voxel_grid


File: main/projection/pointcloud_to_voxelgrid.py

import torch
import torch.nn as nn
from voxel_grid import VoxelGrid
from projection.projection_ops import scatter_add_and_pool
from projection.constants import ROUNDING_OFFSET

# This many points need to land within a voxel for that voxel to be considered "occupied".
# Too low, and noisy depth readings can generate obstacles.
# Too high, and objects far away don't register in the map
MIN_POINTS_PER_VOXEL = 10


class PointcloudToVoxels(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, point_coordinates : torch.Tensor, point_attributes : torch.Tensor):

        dtype = torch.float32 if "cpu" == str(point_coordinates.device) else torch.half
        point_attributes = point_attributes.type(dtype)

        voxelgrid = VoxelGrid.create_empty(batch_size=point_coordinates.shape[0],
                                           channels=point_attributes.shape[1],
                                           device=point_coordinates.device,
                                           data_dtype=dtype)
        b, c, w, l, h = voxelgrid.data.shape

        # Compute which voxel coordinates (integer) each point falls within
        point_in_voxel_coords_f = (point_coordinates - voxelgrid.origin[:, :, None, None]) / voxelgrid.voxel_size

        point_in_voxel_coords = (point_in_voxel_coords_f + ROUNDING_OFFSET).long()

        # Compute a mask of which points land within voxel grid bounds
        min_bounds, max_bounds = voxelgrid.get_integer_bounds()
        point_in_bounds_mask = torch.logical_and(point_in_voxel_coords >= min_bounds[None, :, None, None],
                                                 point_in_voxel_coords < max_bounds[None, :, None, None])
        point_in_bounds_mask = point_in_bounds_mask.min(dim=1, keepdim=True).values  # And across all coordinates
        num_oob_points = (point_in_bounds_mask.int() == 0).int().sum().detach().cpu().item()
        print(num_oob_points)
        if num_oob_points > 20000:
            print(f"Number of OOB points: {num_oob_points}")

        # Convert coordinates into a flattened voxel grid
        point_in_voxel_flat_coords = point_in_voxel_coords[:, 0] * l * h + point_in_voxel_coords[:, 1] * h + point_in_voxel_coords[:, 2]

        # Flatten spatial coordinates so that we can run the scatter operation
        voxeldata_flat = voxelgrid.data.view([b, c, -1])
        point_data_flat = point_attributes.view([b, c, -1])
        point_in_voxel_flat_coords = point_in_voxel_flat_coords.view([b, 1, -1])
        point_in_bounds_mask_flat = point_in_bounds_mask.view([b, 1, -1])

        voxeldata_new_pooled, voxeloccupancy_new_pooled = scatter_add_and_pool(
            voxeldata_flat,
            point_data_flat,
            point_in_bounds_mask_flat,
            point_in_voxel_flat_coords,
            pool="max",
            occupancy_threshold=MIN_POINTS_PER_VOXEL
        )

        # Convert dtype to save space
        voxeloccupancy_new_pooled = voxeloccupancy_new_pooled.type(dtype)

        # Unflatten the results
        voxeldata_new_pooled = voxeldata_new_pooled.view([b, c, w, l, h])
        voxeloccupancy_new_pooled = voxeloccupancy_new_pooled.view([b, 1, w, l, h])

        voxelgrid_new = VoxelGrid(voxeldata_new_pooled, voxeloccupancy_new_pooled, voxelgrid.voxel_size, voxelgrid.origin)
        return voxelgrid_new



File: main/projection/pointcloud_voxelgrid_intersection.py

import torch
import torch.nn as nn
from lgp.models.alfred.voxel_grid import VoxelGrid

from lgp.models.alfred.projection.constants import ROUNDING_OFFSET


class PointcloudVoxelgridIntersection(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, point_coordinates : torch.Tensor, voxelgrid: VoxelGrid):
        """
        Given a pointcloud arranged in a 2D spatial grid (B x (3 + C) x H x W), where the first 3 channels are 3D coordinates,
        and given a 3D voxel grid mask compute a mask of size Bx1xHxW indicating which points in the pointcloud intersect with
        voxels of value > 0.5.
        """
        #point_coordinates = pointcloud[0]#[:, :3, :, :]
        #point_attributes = pointcloud[1]#[:, 3:, :, :]
        b, c, w, l, h = voxelgrid.data.shape
        bp, _, ih, iw = point_coordinates.shape

        if b == 1 and bp > 1:
            voxelgrid__data = voxelgrid.data.repeat((bp, 1, 1, 1, 1))
            b = bp
        else:
            voxelgrid__data = voxelgrid.data

        # Compute which voxel coordinates (integer) each point falls within
        point_in_voxel_coords_f = (point_coordinates - voxelgrid.origin[:, :, None, None]) / voxelgrid.voxel_size
        point_in_voxel_coords = (point_in_voxel_coords_f + ROUNDING_OFFSET).long()

        # Compute a mask of which points land within voxel grid bounds
        min_bounds, max_bounds = voxelgrid.get_integer_bounds()
        point_in_bounds_mask = torch.logical_and(point_in_voxel_coords >= min_bounds[None, :, None, None],
                                                 point_in_voxel_coords < max_bounds[None, :, None, None])
        point_in_bounds_mask = point_in_bounds_mask.min(dim=1, keepdim=True).values  # And across all coordinates
        num_oob_points = (point_in_bounds_mask.int() == 0).int().sum().detach().cpu().item()
        #print(f"Number of OOB points: {num_oob_points}")

        # Mask out voxels that land out of hte map
        point_in_voxel_coords = point_in_voxel_coords * point_in_bounds_mask

        # Convert coordinates into a flattened voxel grid
        point_in_voxel_flat_coords = point_in_voxel_coords[:, 0] * l * h + point_in_voxel_coords[:, 1] * h + point_in_voxel_coords[:, 2]

        # Flatten spatial coordinates so that we can run the scatter operation
        voxeldata_flat = voxelgrid__data.view([b, c, -1])

        # Gather voxel data corresponding to each pointcloud point
        point_mask_flat = torch.gather(voxeldata_flat, dim=2, index=point_in_voxel_flat_coords.view([b, 1, -1]))
        point_mask = point_mask_flat.view([b, 1, ih, iw])

        return point_mask



File: main/projection/pose.py

import uuid
import numpy as np

from transforms3d import quaternions
from transforms3d import euler
from transforms3d import affines


class Pose:
    def __init__(self, position, orientation,
                 frame="world", name=None):
        self.frame = frame
        self.name = name if name is not None else uuid.uuid4()
        self.position = position
        self.orientation = orientation

    @classmethod
    def make_identity_pose(cls, frame="world"):
        position = np.asarray([0, 0, 0])
        orientation = np.asarray([0, 0, 0, 1])
        return cls(position, orientation, frame)

    @classmethod
    def from_matrix_4f(cls, matrix4f, frame="world", name=None):
        position = matrix4f[0:3, 3].numpy()
        orientation = quaternions.mat2quat(matrix4f[:3, :3].numpy())
        return cls(position, orientation, frame, name)

    def to_matrix_4f(self):
        rotation = quaternions.quat2mat(self.orientation)
        mat = affines.compose(self.position, rotation, [1, 1, 1])
        return mat


File: main/projection/projection_ops.py

import torch
from projection.utils import make_pinhole_camera_matrix


def project_3d_points(mat4f, points):
    sdf = "wlhabcd"[0:len(points.shape) - 2]
    homo_ones = torch.ones_like(points[:, 0:1])
    points_projected = (
        torch.einsum(f"by{sdf},bxy->bx{sdf}",
                     torch.cat([points, homo_ones], dim=1),
                     mat4f)[:, 0:3])
    return points_projected


def project_3d_camera_points_to_2d_pixels(img_h, img_w, hfov_deg, points_3d_camera):
    batch_size = points_3d_camera.shape[0]
    device = points_3d_camera.device

    intrinsics = make_pinhole_camera_matrix(
        height_px=img_h,
        width_px=img_w,
        hfov_deg=hfov_deg
    )
    intrinsics = intrinsics.to(device)
    intrinsics = intrinsics[None, :, :].repeat((batch_size, 1, 1))

    # Spatial dimension string
    sdf = "wlhabcd"[0:len(points_3d_camera.shape)-2]

    tmp = torch.einsum(f"by{sdf},bxy->bx{sdf}", points_3d_camera, intrinsics)
    points_px_camera = tmp[:, 0:2] / (tmp[:, 2:3] + 1e-10)
    point_px_depths = tmp[:, 2:3]
    return points_px_camera, point_px_depths


def project_3d_world_points_to_2d_pixels(extrinsics4f, img_h, img_w, hfov_deg, points_3d_world):
    """
    Args:
        extrinsics4f: 4x4 camera extrinsics matrix that projects 3D points in world coordinates
            to 3d points in camera coordinates
        img_h, img_w, hfov_deg: image height and width in pixels, horizontal field of view in degrees
        points_3d_world: Bx3xAx(B)x(C) dimensional tensor of 3D coordinates, where A and optionally B, C are spatial dimensions
    Returns:
        Bx2xAx(B)x(C) tensor of 2D image pixel coordinates corresponding to each 3D point
    """
    points_3d_camera = project_3d_points(extrinsics4f, points_3d_world)
    points_px_camera, point_px_depths = project_3d_camera_points_to_2d_pixels(img_h, img_w, hfov_deg, points_3d_camera)
    return points_px_camera, point_px_depths


def project_2d_depth_to_3d():
    ...


def scatter_add_and_pool(dest_tensor, source_data, source_mask, src_dst_mapping, pool="max", occupancy_threshold=1.0):
    """
    dest_tensor: BxCxM tensor - destination tensor to add the information to
    source_data: BxCxN tensor - containing source data that will be scattered
    source_mask: Bx1xN tensor - indicating which source elements to copy
    src_dst_mapping: Bx1xN tensor - for each source data point,
        indicates the coordinate in dest_tensor where this data point should be added.
    """
    b, c, m = dest_tensor.shape
    _, _, n = source_data.shape

    # Destination tensor - voxel grid. Add a "counter" layer.
    dest_with_counters = torch.cat([dest_tensor,
                                    torch.zeros_like(dest_tensor[:, 0:1, :])], dim=1)

    # Source tensor - point cloud data. Add a "counter" layer
    src_with_counters = torch.cat([source_data,
                                   torch.ones_like(source_data[:, 0:1, :])], dim=1)

    # Mapping from source tensor to destination tensor.
    # Repeat across channels to specify that the same mapping is used for each channel.
    src_dst_mapping = src_dst_mapping.repeat((1, c + 1, 1))
    source_mask = source_mask.repeat((1, c + 1, 1)).int()

    # Mask out the points that shouldn't be projected
    src_dst_mapping = src_dst_mapping * source_mask
    src_with_counters = src_with_counters * source_mask

    # Project point cloud data onto the destination tensor, adding the features in case of collisions
    dest_with_counters = torch.scatter_add(
        dest_with_counters, dim=2, index=src_dst_mapping, src=src_with_counters)

    # Slice off the layer that counts how many pixels were projected on this voxel
    # ... and average voxel representations that landed on the voxel
    dest_data = dest_with_counters[:, 0:c, :]
    dest_counters = dest_with_counters[:, c:, :]

    if pool == "max":
        # Maximum of all objects within a voxel
        dest_data_pooled = dest_data.clamp(0, 1)
    else:
        # Average of all objects within a voxel
        dest_data_pooled = dest_data / (dest_counters + 1e-10)

    dest_occupancy_pooled = (dest_counters >= occupancy_threshold).int().float()
    return dest_data_pooled, dest_occupancy_pooled



File: main/projection/utils.py

import math
import torch


def make_pinhole_camera_matrix(hfov_deg, width_px, height_px):
    f_x = width_px * 0.5 / math.tan(math.radians(hfov_deg) * 0.5)
    f_y = f_x
    K = torch.tensor([
        [f_y, 0, height_px / 2],
        [0, f_x, width_px / 2],
        [0, 0, 1]], dtype=torch.float32)
    return K


def make_pinhole_camera_matrix_4f(hfov_deg, width_px, height_px):
    f_x = width_px * 0.5 / math.tan(hfov_deg * 0.5 * math.pi / 180)
    f_y = f_x
    K = torch.tensor([
        [f_y, 0, 0, height_px / 2],
        [0, f_x, 0, width_px / 2],
        [0, 0, 1, 0],
        [0, 0, 0, 1]
    ], dtype=torch.float32)
    return K


File: main/projection/voxel_3d_observability.py

import torch
import torch.nn as nn

from lgp.models.alfred.projection.projection_ops import project_3d_camera_points_to_2d_pixels, project_3d_points
from lgp.models.alfred.voxel_grid import VoxelGrid


class Voxel3DObservability(nn.Module):
    """
    Computes which voxels in the voxel grid are visible, by projecting each voxel back into the image.
    """
    def __init__(self):
        super().__init__()

    def forward(self,
                voxel_grid : VoxelGrid,
                extrinsics4f : torch.tensor, # B x 1 x 4 x 4
                depth_image : torch.tensor, # B x 1 x H x W
                hfov_deg : float):
        b, c, ih, iw = depth_image.shape
        _, _, w, l, h = voxel_grid.data.shape
        device = depth_image.device
        dtype = dtype = torch.float32 if "cpu" == str(voxel_grid.data.device) else torch.half
        extrinsics4f = extrinsics4f.to(device).float() # Projects from world coordinates to camera

        # Represent voxel grid by a point cloud of voxel centers
        voxel_coordinates_3d_world_meters = voxel_grid.get_centroid_coord_grid()

        # Project points into camera pixels
        voxel_coordinates_3d_cam_meters = project_3d_points(
            extrinsics4f, voxel_coordinates_3d_world_meters)
        voxel_coordinates_cam_pixels, pixel_z = project_3d_camera_points_to_2d_pixels(
            ih, iw, hfov_deg, voxel_coordinates_3d_cam_meters)

        # Compute a mask indicating which voxels are within camera FOV and in front of the camera
        voxels_in_image_bounds = (
            (voxel_coordinates_cam_pixels[:, 0:1, :, :, :] > 0) *
            (voxel_coordinates_cam_pixels[:, 0:1, :, :, :] < ih) *
            (voxel_coordinates_cam_pixels[:, 1:2, :, :, :] > 0) *
            (voxel_coordinates_cam_pixels[:, 1:2, :, :, :] < iw) *
            (pixel_z > 0)
        )

        # Map all out-of-bounds pixels to pixel (0, 0) in the image (just as a dummy value that's in-bounds)
        voxel_coordinates_cam_pixels = (voxel_coordinates_cam_pixels * voxels_in_image_bounds.int()).int()
        voxel_coordinates_cam_pixels = voxel_coordinates_cam_pixels[:, 0:2, :, :, :] # drop the z coordinate

        # Compute coordinates of each voxel into a 1D flattened image
        voxel_coordinates_in_flat_cam_pixels = voxel_coordinates_cam_pixels[:, 1:2, :, :, :] * iw + voxel_coordinates_cam_pixels[:, 0:1, :, :, :]
        flat_voxel_coordinates_in_flat_cam_pixels = voxel_coordinates_in_flat_cam_pixels.view([b, -1]).long()
        flat_depth_image = depth_image.view([b, -1])
        # Gather the depth image values corresponding to each "voxel"
        flat_voxel_ray_bounce_depths = torch.stack([torch.index_select(flat_depth_image[i], dim=0, index=flat_voxel_coordinates_in_flat_cam_pixels[i]) for i in range(b)])

        # Depth where a ray cast from the camera to this voxel hits an object in the depth image
        voxel_ray_bounce_depths = flat_voxel_ray_bounce_depths.view([b, 1, w, l, h]) # Unflatten
        # Depth of the voxel itself
        voxel_depths = voxel_coordinates_3d_cam_meters[:, 2:3, :, :, :]

        # Compute which voxels are observed by this camera taking depth image into account:
        #       All voxels along camera rays that hit an obstacle are considered observed
        #       Consider a voxel that's immediately behind an observed point as observed
        #       ... to make sure that we consider the voxels that contain objects as observed
        depth_bleed_tolerance = voxel_grid.voxel_size / 2
        voxel_observability_mask = torch.logical_and(
            voxel_depths <= voxel_ray_bounce_depths + depth_bleed_tolerance,
            voxels_in_image_bounds).long()

        # Consider all voxels that contain stuff as observed
        voxel_observability_mask = torch.max(voxel_observability_mask, (voxel_grid.data.max(1, keepdim=True).values > 0.2).long())
        voxel_observability_mask = voxel_observability_mask.long()

        # output
        observability_grid = VoxelGrid(voxel_observability_mask.type(dtype), # Observability
                                       voxel_observability_mask.type(dtype), # Occupancy (same as observability - consider observed voxels to be occupied)
                                       voxel_grid.voxel_size,
                                       voxel_grid.origin)

        return observability_grid, voxel_ray_bounce_depths


File: main/projection/voxel_centroids_to_image.py

from typing import Iterable

import torch
import torch.nn as nn

from lgp.models.alfred.voxel_grid import VoxelGrid
from lgp.models.alfred.projection.projection_ops import project_3d_camera_points_to_2d_pixels, project_3d_points, scatter_add_and_pool


class VoxelsToImage(nn.Module):
    """
    Projects a first-person image to a PointCloud
    """
    def __init__(self):
        super().__init__()

    def forward(self,
                voxel_grid : VoxelGrid,
                extrinsics4f : torch.tensor, # B x 1 x 4 x 4
                image_shape : Iterable,
                hfov_deg : float):

        b, c, ih, iw = image_shape
        _, _, w, l, h = voxel_grid.data.shape
        device = voxel_grid.data.device
        extrinsics4f = extrinsics4f.to(device).float() # Projects from world coordinates to camera

        # Represent voxel grid by a point cloud of voxel centers
        voxel_coordinates_3d_world_meters = voxel_grid.get_centroid_coord_grid()

        # Project points into camera pixels
        voxel_coordinates_3d_cam_meters = project_3d_points(
            extrinsics4f, voxel_coordinates_3d_world_meters)
        voxel_coordinates_cam_pixels, pixel_z = project_3d_camera_points_to_2d_pixels(
            ih, iw, hfov_deg, voxel_coordinates_3d_cam_meters)

        # Compute a mask indicating which voxels are within camera FOV and in front of the camera
        voxels_in_image_bounds = (
            (voxel_coordinates_cam_pixels[:, 0:1, :, :, :] > 0) *
            (voxel_coordinates_cam_pixels[:, 0:1, :, :, :] < ih) *
            (voxel_coordinates_cam_pixels[:, 1:2, :, :, :] > 0) *
            (voxel_coordinates_cam_pixels[:, 1:2, :, :, :] < iw) *
            (pixel_z > 0)
        )

        # Map all out-of-bounds pixels to pixel 0, 0 in the image (just as a dummy value that's in-bounds)
        voxel_coordinates_cam_pixels = (voxel_coordinates_cam_pixels * voxels_in_image_bounds.int()).int()
        voxel_coordinates_cam_pixels = voxel_coordinates_cam_pixels[:, 0:2, :, :, :] # drop the z coordinate

        voxel_coordinates_in_flat_cam_pixels = voxel_coordinates_cam_pixels[:, 1:2, :, :, :] * iw + voxel_coordinates_cam_pixels[:, 0:1, :, :, :]
        flat_voxel_coordinates_in_flat_cam_pixels = voxel_coordinates_in_flat_cam_pixels.view([b, -1]).long()

        # Project all voxels that are "occupied", i.e. exist within the voxel grid, and are visible from the camera
        included_voxel_mask = voxels_in_image_bounds * voxel_grid.occupancy

        # Flatten spatial coordinates so that we can run the scatter operation
        image_data_flat = torch.zeros([b, c, ih * iw], dtype=torch.float32, device=device)
        voxel_data_flat = voxel_grid.data.view([b, c, -1])
        point_in_voxel_flat_coords = flat_voxel_coordinates_in_flat_cam_pixels.view([b, 1, -1])
        included_voxel_mask_flat = included_voxel_mask.view([b, 1, -1])

        image_data_pooled_flat, occupancy_pooled_flat = scatter_add_and_pool(
            image_data_flat,
            voxel_data_flat,
            included_voxel_mask_flat,
            point_in_voxel_flat_coords,
            pool="max"
        )
        image_data_pooled = image_data_pooled_flat.view([b, c, ih, iw])
        return image_data_pooled


File: main/projection/voxel_mask_to_image_mask.py

from typing import Iterable

import torch
import torch.nn as nn

from lgp.models.alfred.voxel_grid import VoxelGrid
from lgp.models.alfred.projection.projection_ops import project_3d_camera_points_to_2d_pixels, project_3d_points, scatter_add_and_pool
from lgp.models.alfred.projection.image_to_pointcloud import ImageToPointcloud
from lgp.models.alfred.projection.pointcloud_voxelgrid_intersection import PointcloudVoxelgridIntersection

from lgp.ops.depth_estimate import DepthEstimate


DISTR = True


class VoxelMaskToImageMask(nn.Module):
    """
    Projects a first-person image to a PointCloud
    """
    def __init__(self):
        super().__init__()
        self.image_to_pointcloud = ImageToPointcloud()
        self.pointcloud_voxel_intersection = PointcloudVoxelgridIntersection()

    def forward(self,
                voxel_grid : VoxelGrid,
                extrinsics4f : torch.tensor, # B x 1 x 4 x 4
                depth_image : torch.tensor,
                hfov_deg : float):

        if DISTR and isinstance(depth_image, DepthEstimate):
            domain = depth_image.domain_image()#res=100)
            b, c, h, w = domain.shape
            domain_b = domain.reshape((b*c, 1, h, w))
            domain_pc_b, domain_pa_b = self.image_to_pointcloud(
                    domain_b, domain_b, extrinsics4f, hfov_deg, min_depth=0.5)

            masks = self.pointcloud_voxel_intersection(domain_pc_b, voxel_grid)
            masks = masks.reshape((b, c, h, w))

            # Don't look at the depth to increase recall at expense of precision
            point_mask = masks.sum(dim=1, keepdims=True).clamp(0, 1)
        else:
            depth_images = [depth_image]

            # For each percentile depth, compute a corresponding pointcloud
            all_img_point_coords = []
            for dimg in depth_images:
                point_coordinates, point_attributes = self.image_to_pointcloud(
                    dimg, dimg, extrinsics4f, hfov_deg, min_depth=0.5)
                all_img_point_coords.append(point_coordinates)

            # Generate a fpv mask for each depth image
            point_masks = [self.pointcloud_voxel_intersection(pc, voxel_grid) for pc in all_img_point_coords]

            # Or the masks together
            point_mask = torch.stack(point_masks).max(dim=0).values

        return point_mask


File: main/scene_graph.py

import numpy as np
from clip_utils import *
from utils import *
from constants import *
from point_cloud_utils import *


# =========  Parameters for spatial relation heuristics ============
IN_CONTACT_DISTANCE = 0.1
CLOSE_DISTANCE = 0.4
INSIDE_THRESH = 0.5
ON_TOP_OF_THRESH = 0.7
NORM_THRESH_FRONT_BACK = 0.9
NORM_THRESH_UP_DOWN = 0.9
NORM_THRESH_LEFT_RIGHT = 0.8
OCCLUDE_RATIO_THRESH = 0.5
DEPTH_THRESH = 0.9
# ==================================================================


state_dict = {
    "Fridge": ["open", "closed"],
    "Faucet": ["turned on", "turned off"],
    "Pot": ["filled with coffee", "filled with water", "filled with wine" "empty", "dirty", "clean"],
    "StoveBurner": ["turned on", "turned off"],
    "Egg": ['cracked', 'not cracked'],
    "Bread": ['sliced', 'not sliced'],
    "Toaster": ["turned on", "turned off"],
    "CoffeeMachine": ["turned on", "turned off"],
    "Mug": ["filled with coffee", "filled with water", "filled with wine" "empty", "dirty", "clean"],
    "HousePlant": ["watered", "not watered"],
    "Microwave": ["open", "closed", "turned on", "turned off"],
    "Television": ["turned on", "turned off"],
    "Laptop": ["open", "closed", "turned on", "turned off"],
    "Pan": ["clean", "dirty"],
    "Plate": ["clean", "dirty"]
}

# ref: https://github.com/Treesfive/calculate-iou/blob/master/get_iou.py
def get_iou(pred_box, gt_box):
    """
    pred_box : the coordinate for predict bounding box
    gt_box :   the coordinate for ground truth bounding box
    return :   the iou score
    the  left-down coordinate of  pred_box:(pred_box[0], pred_box[1])
    the  right-up coordinate of  pred_box:(pred_box[2], pred_box[3])
    """
    # 1. get the coordinate of inters
    ixmin = max(pred_box[0], gt_box[0])
    ixmax = min(pred_box[2], gt_box[2])
    iymin = max(pred_box[1], gt_box[1])
    iymax = min(pred_box[3], gt_box[3])

    iw = np.maximum(ixmax-ixmin, 0.)
    ih = np.maximum(iymax-iymin, 0.)

    # 2. calculate the area of inters
    inters = iw*ih

    # 3. calculate the area of union
    uni = ((pred_box[2]-pred_box[0]) * (pred_box[3]-pred_box[1]) +
           (gt_box[2] - gt_box[0]) * (gt_box[3] - gt_box[1]) -
           inters)

    # 4. calculate the overlaps between pred_box and gt_box
    iou = inters / uni
    return iou, inters

def get_node_dist(pts_A, pts_B):
    pcd_A = o3d.geometry.PointCloud()
    pcd_A.points = o3d.utility.Vector3dVector(pts_A)
    pcd_B = o3d.geometry.PointCloud()
    pcd_B.points = o3d.utility.Vector3dVector(pts_B)

    dists = pcd_A.compute_point_cloud_distance(pcd_B)
    dist = np.min(np.array(dists))
    return dist

def get_object_state(node_name, img):
    object_name = node_name.split("|")[0]
    if object_name in state_dict:
        states = state_dict[object_name]

        # rank based on current image
        img_feats = get_img_feats(img)
        state_feats = get_text_feats(states)
        sorted_states, sorted_scores = get_nn_text(states, state_feats, img_feats)
        return sorted_states[0]
    return None

def get_gt_object_state(node_name, event):
    object_name = node_name.split("|")[0]
    if object_name in state_dict and len(node_name.split("|")) == 4:
        gt_states = []
        for obj in event.metadata["objects"]:
            if node_name == obj["objectId"]:
                if obj["openable"]:
                    if obj["isOpen"]:
                        gt_states.append("open")
                    else:
                        gt_states.append("closed")
                if obj["sliceable"]:
                    if not obj["isSliced"]:
                        gt_states.append("not sliced")
                if node_name.split("|")[0] == "Egg":
                    if not obj['isBroken']:
                        gt_states.append("not cracked")
                if obj["canFillWithLiquid"]:
                    if obj["isFilledWithLiquid"]:
                            gt_states.append(f"filled with {obj['fillLiquid']}")
                    else:
                        gt_states.append("empty")
                if obj["toggleable"]:
                    if obj["isToggled"]:
                        gt_states.append("turned on")
                    else:
                        gt_states.append("turned off")
                if obj["dirtyable"]:
                    if obj["isDirty"]:
                        gt_states.append("dirty")
                    else:
                        gt_states.append("clean")
            elif obj["controlledObjects"] is not None and node_name in obj["controlledObjects"]:
                # print("ControlledObject: ", node_name, obj['objectId'])
                if obj["isToggled"]:
                    gt_states.append("turned on")
                else:
                    gt_states.append("turned off")
            
        if len(gt_states) > 0:
            return " and ".join(gt_states)
    
    return None

class Node(object):
    def __init__(self, name, object_id=None, pos3d=None, corner_pts=None, bbox2d=None, pcd=None, depth=None, global_node=False):
        self.name = name
        self.object_id = object_id # ai2thor object_id
        self.bbox2d = bbox2d # 2d bounding box (4x1)
        self.pos3d = pos3d # object position
        self.corner_pts = corner_pts # corner points of 3d bbox (8x3)
        self.pcd = pcd # point cloud (px3)
        self.depth = depth
        self.name_w_state = None
        self.global_node = global_node

    def set_state(self, state):
        self.name_w_state = state

    def __str__(self):
        return self.get_name()

    def __hash__(self):
        return hash(self.get_name())

    def __eq__(self, other):
        return True if self.get_name() == other.get_name() else False

    def get_name(self):
        if self.name_w_state is not None:
            return self.name_w_state
        else:
            return self.name


class Edge(object):
    def __init__(self, start_node, end_node, edge_type="none"):
        self.start = start_node
        self.end = end_node
        self.edge_type = edge_type
    
    def __hash__(self):
        return hash((self.start, self.end, self.edge_type))

    def __eq__(self, other):
        if self.start == other.start and self.end == other.end and self.edge_type == other.edge_type:
            return True
        else:
            return False

    def __str__(self):
        return str(self.start) + "->" + self.edge_type + "->" + str(self.end)


class SceneGraph(object):
    """
    Create a spatial scene graph
    """
    def __init__(self, event, task):
        self.event = event
        self.nodes = []
        self.total_nodes = []
        self.edges = {}
        self.task = task

        if event is not None:
            x = event.metadata['agent']['position']['x']
            y = event.metadata['agent']['position']['y']
            z = event.metadata['agent']['position']['z']
            self.camera_world_xyz = torch.as_tensor([x, y, z])
            self.rotation = event.metadata['agent']['rotation']['y']
            self.horizon = event.metadata['agent']['cameraHorizon']

    def add_node_wo_edge(self, node):
        self.total_nodes.append(node)

    def add_node(self, new_node):
        merge = False
        for idx, node in enumerate(self.nodes):
            if new_node.name == node.name:
                merge, dist = is_merge(np.array(new_node.pcd), np.array(node.pcd))
                if merge:
                    node.pcd = torch.cat((node.pcd, new_node.pcd), 0)
                    self.nodes[idx] = node
                    self.add_object_state(new_node, self.event, mode="gt")
                    return new_node
        if not merge:
            if new_node.object_id != self.in_robot_gripper():
                for node in self.total_nodes:
                    if node.name != new_node.name:
                        self.add_edge(node, new_node)
                        self.add_edge(new_node, node)
            self.nodes.append(new_node)
            self.add_object_state(new_node, self.event, mode="gt")
        return new_node

    def add_edge(self, node, new_node):
        pos_A = world_space_xyz_to_camera_space_xyz(torch.tensor(np.array([node.pos3d])).reshape(3, 1), 
            self.camera_world_xyz, self.rotation, self.horizon).flatten()
        pos_B = world_space_xyz_to_camera_space_xyz(torch.tensor(np.array([new_node.pos3d])).reshape(3, 1), 
            self.camera_world_xyz, self.rotation, self.horizon).flatten()
        cam_arr = pos_B - pos_A
        norm_vector = cam_arr / np.linalg.norm(cam_arr)

        box_A, box_B = np.array(node.corner_pts), np.array(new_node.corner_pts)
        if len(node.pcd) == 0 or len(new_node.pcd) == 0:
            return
        else:
            dist = get_pcd_dist(node.pcd, new_node.pcd)
        
        box_A_pts, box_B_pts = np.array(node.pcd), np.array(new_node.pcd)

        if new_node.object_id == self.in_robot_gripper(): # skip spatial relations if object is in robot gripper
            return

        # IN CONTACT
        if dist < IN_CONTACT_DISTANCE:
            if new_node.name not in BULKY_OBJECTS:
                if is_inside(src_pts=box_B_pts, target_pts=box_A_pts, thresh=INSIDE_THRESH):
                    if "countertop" in node.name or "stove burner" in node.name: # address the "inside countertop" issue
                        self.edges[(new_node.name, node.name)] = Edge(new_node, node, "on top of")
                    else:
                        self.edges[(new_node.name, node.name)] = Edge(new_node, node, "inside")
                elif len(np.where((box_B_pts[:, 0] < box_A[4, 0]) & (box_B_pts[:, 0] > box_A[0, 0]) & 
                        (box_B_pts[:, 2] < box_A[4, 2]) & (box_B_pts[:, 2] > box_A[0, 2]))[0]) > len(box_B_pts) * ON_TOP_OF_THRESH:
                    if len(np.where(box_B_pts[:, 1] > box_A[4, 1])[0]) > len(box_B_pts) * ON_TOP_OF_THRESH:
                        # thor specific - fruits unrealistically stay upright in the bowl leading to "on top of bowl" relation instead of "inside bowl" relation
                        if 'slice' in new_node.name and node.name == 'bowl':
                            self.edges[(new_node.name, node.name)] = Edge(new_node, node, "inside")
                        else:
                            self.edges[(new_node.name, node.name)] = Edge(new_node, node, "on top of")
                    elif len(np.where(box_A_pts[:, 1] > box_B[4, 1])[0]) > len(box_A_pts) * ON_TOP_OF_THRESH:
                        if node.name not in BULKY_OBJECTS:
                            # thor specific - fruits unrealistically stay upright in the bowl leading to "on top of bowl" relation instead of "inside bowl" relation 
                            if 'slice' in node.name and new_node.name == 'bowl':
                                self.edges[(node.name, new_node.name)] = Edge(node, new_node, "inside")
                            else:
                                self.edges[(node.name, new_node.name)] = Edge(node, new_node, "on top of")

        # CLOSE TO
        if dist < CLOSE_DISTANCE and (new_node.name, node.name) not in self.edges and (not new_node.global_node):
            if node.name not in BULKY_OBJECTS and new_node.name not in BULKY_OBJECTS:
                if abs(norm_vector[1]) > NORM_THRESH_UP_DOWN:
                    if norm_vector[1] > 0:
                        self.edges[(new_node.name, node.name)] = Edge(new_node, node, "above")
                    else:
                        self.edges[(new_node.name, node.name)] = Edge(new_node, node, "below")
                elif abs(norm_vector[0]) > NORM_THRESH_LEFT_RIGHT:
                    if norm_vector[0] > 0:
                        self.edges[(new_node.name, node.name)] = Edge(new_node, node, "on the right of")
                    else:
                        self.edges[(new_node.name, node.name)] = Edge(new_node, node, "on the left of")
                elif abs(norm_vector[2]) > NORM_THRESH_FRONT_BACK and new_node.bbox2d is not None and node.bbox2d is not None and new_node.depth is not None and node.depth is not None:
                    iou, inters = get_iou(new_node.bbox2d, node.bbox2d)
                    occlude_ratio = inters / ((node.bbox2d[2]-node.bbox2d[0]) * (node.bbox2d[3]-node.bbox2d[1]))
                    # print("new_node, node: ", new_node.name, node.name)
                    # print("iou, occlude_ratio: ", iou, occlude_ratio)
                    if occlude_ratio > OCCLUDE_RATIO_THRESH and len(np.where(new_node.depth <= np.min(node.depth))[0]) > len(new_node.depth) * DEPTH_THRESH:
                        self.edges[(new_node.name, node.name)] = Edge(new_node, node, "blocking")
    
    def add_object_state(self, node, event, mode="gt"):
        if mode == "gt":
            state = get_gt_object_state(node.object_id, event)
        elif mode == "clip":
            state = get_object_state(node.object_id, event.frame)
        if state is not None:
            node.set_state(f"{node.name} ({state})")
        return node

    def add_agent(self):
        object_id = self.in_robot_gripper()
        if object_id:
            name = get_label_from_object_id(object_id, [self.event], self.task)
            self.edges[(name, "robot gripper")] = Edge(Node(name), Node("robot gripper"), "inside")
        else:
            self.edges[("nothing", "robot gripper")] = Edge(Node("nothing"), Node("robot gripper"), "inside")
        return object_id

    def in_robot_gripper(self):
        for obj in self.event.metadata["objects"]:
            if obj["isPickedUp"]:
                return obj['objectId']
        return None

    def __eq__(self, other):
        if (set(self.nodes) == set(other.nodes)) and (set(self.edges.values()) == set(other.edges.values())):
            return True
        else:
            return False

    def __str__(self):
        visited = []
        res = "[Nodes]:\n"
        for node in set(self.nodes):
            res += node.get_name()
            res += "\n"
        res += "\n"
        res += "[Edges]:\n"
        for edge_key, edge in self.edges.items():
            name_1, name_2 = edge_key
            edge_key_reversed = (name_2, name_1)
            if (edge_key not in visited and edge_key_reversed not in visited) or edge.edge_type in ['on top of', 'inside', 'occluding']:
                res += str(edge)
                res += "\n"
            visited.append(edge_key)
        return res



File: main/state_summary/boilWater/boilWater-1/reasoning.json

{"pred_failure_reason": "The failure at 00:41 was caused by the robot attempting to put the pot on the fourth stove burner without having picked up the pot from the sink first. As a result, the robot's gripper was empty, and it could not complete the task of boiling water.", "pred_failure_step": ["00:41"], "gt_failure_reason": "The robot did not pick up the pot from sink before moving to stove burner.", "gt_failure_step": [["00:31", "00:41"]]}


File: main/state_summary/boilWater/boilWater-1/replan.json

{"original_plan": ["1. (pick_up, pot)", "2. (put_on, pot, fourth stove burner)", "3. (toggle_on, fourth stove burner)"], "plan": ["(pick_up, Pot)", "(put_on, Pot, StoveBurner-4)", "(toggle_on, StoveBurner-4)"], "num_steps": 3, "success": true}


File: main/state_summary/makeCoffee/makeCoffee-1/reasoning.json

{"pred_failure_reason": "The failure at 00:51 occurred because the robot attempted to place the mug inside the coffee machine while there was already a cup inside it. The robot should have removed the existing cup from the coffee machine before attempting to place the mug inside.", "pred_failure_step": ["00:51"], "gt_failure_reason": "The robot failed to put the mug inside the coffee machine because there was already a cup inside it, occupying the space.", "gt_failure_step": "00:51"}


File: main/state_summary/makeCoffee/makeCoffee-1/replan.json

{"original_plan": ["1. (pick_up, cup from coffee machine)", "2. (put_on, cup, countertop)", "3. (pick_up, mug)", "4. (put_in, mug, coffee machine)", "5. (toggle_on, coffee machine)", "6. (toggle_off, coffee machine)", "7. (pick_up, mug)", "8. (put_on, mug, countertop)"], "plan": ["(pick_up, Cup)", "(put_on, Cup, CounterTop)", "(pick_up, Mug)", "(put_in, Mug, CoffeeMachine)", "(toggle_on, CoffeeMachine)", "(toggle_off, CoffeeMachine)", "(pick_up, Mug)", "(put_on, Mug, CounterTop)"], "num_steps": 8, "success": true}


File: main/task_utils.py

import os
import PIL
import json
import pickle
import imageio
import numpy as np
import matplotlib.pyplot as plt

from typing import Dict, List
from collections import deque
from moviepy.editor import VideoFileClip, AudioFileClip, CompositeAudioClip
from constants import *
from point_cloud_utils import *

class TaskUtil:
    def __init__(self, folder_name,
                controller, 
                reachable_positions, 
                failure_injection, 
                index, 
                repo_path, 
                chosen_failure,
                failure_injection_params, 
                counter=0, 
                replan=False):
        self.counter = counter
        self.repo_path = repo_path
        self.folder_name = folder_name
        if failure_injection:
            self.specific_folder_name = self.get_folder_name(folder_name, index+1)
        else:
            self.specific_folder_name = folder_name
        self.controller = controller
        self.grid = self.createGraph()
        self.interact_actions = {}
        self.nav_actions = {}
        self.reachable_positions = reachable_positions
        self.reachable_points = self.get_2d_reachable_points()
        self.failure_added = False
        self.objs_w_unk_loc = []
        self.failures = ['drop', 'failed_action', 'missing_step']
        self.unity_name_map = self.get_unity_name_map()
        self.sounds = {}
        self.failure_injection_params = failure_injection_params
        if failure_injection and chosen_failure is None:
            i = index % len(self.failures)
            self.chosen_failure = self.failures[i]
            print("[INFO] chosen failure:", self.chosen_failure)
        else:
            self.chosen_failure = chosen_failure
        self.interact_action_primitives = ['put_on', 'put_in', 'pick_up', 'slice_obj', 'toggle_on', 'toggle_off', 'open_obj', 'close_obj', 'pour', 'crack_obj']
        self.gt_failure = {}
        if os.path.exists(f'{self.repo_path}/thor_tasks/{folder_name.split("/")[0]}/{folder_name.split("/")[1]}.pickle'):
            with open(f'{self.repo_path}/thor_tasks/{folder_name.split("/")[0]}/{folder_name.split("/")[1]}.pickle', 'rb') as handle:
                self.failures_already_injected = pickle.load(handle)

    def get_unity_name_map(self):
        obj_list = ['CounterTop', 'StoveBurner', 'Cabinet', 'Faucet', 'Sink']
        obj_rep_map = {}
        for obj in self.controller.last_event.metadata["objects"]:
            if obj["objectType"] in obj_list:
                if obj["objectType"] in obj_rep_map:
                    obj_rep_map[obj["objectType"]] += 1
                else:
                    obj_rep_map[obj["objectType"]] = 1
        for key in obj_rep_map.keys():
            if obj_rep_map[key] == 1:
                obj_list.remove(key)
        unity_name_map = {}
        for obj_type in obj_list:
            counter = 0
            for obj in self.controller.last_event.metadata["objects"]:
                if obj["objectType"] == obj_type:
                    counter += 1
                    unity_name_map[obj['name']] = obj_type + '-' + str(counter)
        # print("unity_name_map: ", unity_name_map)
        return unity_name_map

    def get_folder_name(self, folder_name, folder_idx):
        final_folder_name = folder_name + '-' + str(folder_idx)
        return final_folder_name

    def createGraph(self, gridSize=0.25, min=-5, max=5.1):
        grid = np.mgrid[min:max:gridSize, min:max:gridSize].transpose(1,2,0)
        return grid

    def get_2d_reachable_points(self):
        reachable_points = []
        for p in self.reachable_positions:
            reachable_points.append([p['x'], p['z']])
        reachable_points = np.array(reachable_points)
        return reachable_points
    
    def get_reasoning_json(self):
        with open(f'state_summary/{self.specific_folder_name}/reasoning.json') as f:
            reasoning_json = json.load(f)
        return reasoning_json


def closest_position(
    object_position: Dict[str, float],
    reachable_positions: List[Dict[str, float]]
) -> Dict[str, float]:
    out = reachable_positions[0]
    min_distance = float('inf')
    for pos in reachable_positions:
        # NOTE: y is the vertical direction, so only care about the x/z ground positions
        dist = sum([(pos[key] - object_position[key]) ** 2 for key in ["x", "z"]])
        if dist < min_distance:
            min_distance = dist
            out = pos
    return out

# A queue node used in BFS
class Node:
    # (x, y) represents coordinates of a cell in the matrix
    # maintain a parent node for the printing path
    def __init__(self, x, y, parent=None):
        self.x = x
        self.y = y
        self.parent = parent
 
    def __repr__(self):
        return str((self.x, self.y))
 
    def __eq__(self, other):
        return self.x == other.x and self.y == other.y
 
# Below lists detail all four possible movements from a cell
ROW = [-1, 0, 0, 1]
COL = [0, -1, 1, 0]
 
# The function returns false if (x, y) is not a valid position
def isValid(x, y, N, reachable_points, grid):
    # If cell lies out of bounds
    if (x < 0 or y < 0 or x >= N or y >= N):
        return False
    
    # Check if cell is in reachable cells
    val = grid[x][y]
    if val not in reachable_points.tolist():    
        return False
    
    return True
 
# Utility function to find path from source to destination
def getPath(node, path=[]):
    if node:
        getPath(node.parent, path)
        path.append(node)
 
# Find the shortest route in a matrix from source cell (x, y) to
# destination cell (N-1, N-1)
def findPath(matrix, x=0, y=0, target_pos=None, reachable_points=None):
    matrix = matrix.tolist()

    # `N × N` matrix
    N = len(matrix)
 
    # create a queue and enqueue the first node
    q = deque()
    src = Node(x, y)
    q.append(src)
 
    # set to check if the matrix cell is visited before or not
    visited = set()
 
    key = (src.x, src.y)
    visited.add(key)
 
    # loop till queue is empty
    while q:
 
        # dequeue front node and process it
        curr = q.popleft()
        i = curr.x
        j = curr.y
 
        # return if the destination is found
        if i == target_pos[0] and j == target_pos[1]:
            path = []
            getPath(curr, path)
            return path
 
        # check all four possible movements from the current cell
        # and recur for each valid movement
        for k in range(len(ROW)):
            # get next position coordinates using the value of the current cell
            x = i + ROW[k]
            y = j + COL[k]
 
            # check if it is possible to go to the next position
            # from the current position
            if isValid(x, y, N, reachable_points, matrix):
                # construct the next cell node
                next = Node(x, y, curr)
                key = (next.x, next.y)
 
                # if it isn't visited yet
                if key not in visited:
                    # enqueue it and mark it as visited
                    q.append(next)
                    visited.add(key)
 
    # return None if the path is not possible
    return


def obj_is_blocked(taskUtil, src_obj_type):
    target_obj_type = taskUtil.failure_injection_params['target_obj_type']
    src_obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["objectType"] == src_obj_type)
    src_obj_loc = src_obj['position']
    src_obj_loc = np.array([src_obj_loc['x'], src_obj_loc['y'], src_obj_loc['z']])
    target_obj = next(obj for obj in taskUtil.controller.last_event.metadata["objects"] if obj["objectType"] == target_obj_type)
    target_obj_loc = target_obj['position']
    target_obj_loc = np.array([target_obj_loc['x'], target_obj_loc['y'], target_obj_loc['z']])
    dist = np.linalg.norm(src_obj_loc - target_obj_loc)
    # print("dist: ", dist)
    if dist < 0.3:
        e = taskUtil.controller.last_event
        x = e.metadata['agent']['position']['x']
        y = e.metadata['agent']['position']['y']
        z = e.metadata['agent']['position']['z']
        camera_world_xyz = torch.as_tensor([x, y, z])
        rotation = e.metadata['agent']['rotation']['y']
        horizon = e.metadata['agent']['cameraHorizon']
        pos_A = world_space_xyz_to_camera_space_xyz(torch.tensor(np.expand_dims(target_obj_loc, 0)).reshape(3, 1),
                                                    camera_world_xyz, rotation, horizon).flatten()
        pos_B = world_space_xyz_to_camera_space_xyz(torch.tensor(np.expand_dims(src_obj_loc, 0)).reshape(3, 1),
                                                    camera_world_xyz, rotation, horizon).flatten()
        cam_arr = pos_B - pos_A
        norm_vector = cam_arr / np.linalg.norm(cam_arr)
        # print("norm_vector: ", norm_vector)
        if norm_vector[2] > 0.75:
            print(f"[INFO] {src_obj_type} is blocked.")
            return True
        else:
            print(f"[INFO] {src_obj_type} is not blocked.")
            return False

def save_data(task, e, replan=False):
    task.counter += 1
    if replan:
        folder = 'recovery'
    else:
        folder = 'thor_tasks'
    os.system(f"mkdir -p {task.repo_path}/{folder}/{task.specific_folder_name}/events")
    os.system(f"mkdir -p {task.repo_path}/{folder}/{task.specific_folder_name}/ego_img")
    
    with open(f'{task.repo_path}/{folder}/{task.specific_folder_name}/events/step_' + str(task.counter) + '.pickle', 'wb') as handle:
        pickle.dump(e, handle, protocol=pickle.HIGHEST_PROTOCOL)
    rgb = e.frame
    plt.imsave(f'{task.repo_path}/{folder}/{task.specific_folder_name}/ego_img/img_step_' + str(task.counter) + '.png', np.asarray(rgb, order='C'))


def generate_video(taskUtil, recovery_video):
    if recovery_video:
        folder = 'recovery'
    else:
        folder = 'thor_tasks'
    img_path = f"{taskUtil.repo_path}/{folder}/{taskUtil.specific_folder_name}/ego_img/"
    save_path = f"{taskUtil.repo_path}/{folder}/{taskUtil.specific_folder_name}/"
    SOUND_PATH = f"{taskUtil.repo_path}/assets/sounds/"
    l = os.listdir(img_path)
    lsorted = sorted(l, key=lambda x: int(os.path.splitext(x)[0].split('_')[-1]))
    
    if len(lsorted) > 0 and not recovery_video:
        video_path = f'{save_path}original-video.mp4'
        with imageio.get_writer(video_path, mode='I', fps=1) as writer:
            for filename in lsorted:
                img = PIL.Image.open(img_path + filename).convert('RGB')
                writer.append_data(np.array(img))
        # Add sound
        sounds = taskUtil.sounds
        print("sounds: ", sounds)
        sound_lis = []
        for key, val in sounds.items():
            start_time = key
            audio = AudioFileClip(SOUND_PATH + val)
            sound_lis.append(audio.set_start(start_time))
        if len(sound_lis) > 0:
            clip = VideoFileClip(save_path + "original-video.mp4")
            audio = CompositeAudioClip(sound_lis)
            audio = audio.set_duration(clip.duration)
            clip.audio = audio
            os.system(f"rm {save_path}original-video.mp4")
            clip.write_videofile(save_path + 'original-video.mp4', audio_codec='aac')
    
    if len(lsorted) > 0 and recovery_video:
        recover_video_path = f'{save_path}recovery-video.mp4'
        with imageio.get_writer(recover_video_path, mode='I', fps=1) as writer:
            for filename in lsorted:
                img = PIL.Image.open(img_path + filename).convert('RGB')
                writer.append_data(np.array(img))
        
        sounds = taskUtil.sounds
        print("sounds: ", sounds)
        sound_lis = []
        for key, val in sounds.items():
            start_time = key
            audio = AudioFileClip(SOUND_PATH + val)
            sound_lis.append(audio.set_start(start_time))
        if len(sound_lis) > 0:
            clip = VideoFileClip(save_path + "recovery-video.mp4")
            audio = CompositeAudioClip(sound_lis)
            audio = audio.set_duration(clip.duration)
            clip.audio = audio
            os.system(f"rm {save_path}recovery-video.mp4")
            clip.write_videofile(save_path + 'recovery-video.mp4', audio_codec='aac')



File: main/tasks.json

{
    "Task 1": {
        "name": "make coffee",
        "task_idx": 5,
        "num_samples": 1,
        "failure_injection": false,
        "folder_name": "makeCoffee-1",
        "scene": "FloorPlan16",
        "chosen_failure": "occupied",
        "gt_failure_reason": "The robot failed to put the mug inside the coffee machine because there was already a cup inside it, occupying the space.",
        "gt_failure_step": "00:51",
        "preactions": [
            "(dirty_obj, Mug)"
        ],
        "failure_injection_params" : {
            "src_obj_type": "Cup",
            "target_obj_type": "CoffeeMachine",
            "disp_x": 0.0,
            "disp_z": 0.05,
            "disp_y": 0.02
        },
        "actions": [
            "(navigate_to_obj, Mug)",
            "(pick_up, Mug)",
            "(navigate_to_obj, Sink)",
            "(put_on, Mug, SinkBasin)",
            "(toggle_on, Faucet)",
            "(toggle_off, Faucet)",
            "(pick_up, Mug)",
            "(pour, Mug, Sink)",
            "(navigate_to_obj, CoffeeMachine)",
            "(put_in, Mug, CoffeeMachine)",
            "(toggle_on, CoffeeMachine)",
            "(toggle_off, CoffeeMachine)",
            "(pick_up, Mug)",
            "(put_on, Mug, CounterTop)"
        ],
        "success_condition": "a clean mug is filled with coffee and on top of the countertop."
    },
    "Task 2": {
        "name": "boil water",
        "task_idx": 1,
        "num_samples": 1,
        "failure_injection": false,
        "folder_name": "boilWater-1",
        "scene": "FloorPlan1",
        "gt_failure_reason": "The robot did not pick up the pot from sink before moving to stove burner.",
        "gt_failure_step": [["00:31", "00:41"]],
        "actions": [
            "(navigate_to_obj, Pot)",
            "(pick_up, Pot)",
            "(navigate_to_obj, Sink)",
            "(put_in, Pot, Sink)",
            "(toggle_on, Faucet)",
            "(toggle_off, Faucet)",
            "(navigate_to_obj, StoveBurner-4)",
            "(put_on, Pot, StoveBurner-4)",
            "(toggle_on, StoveBurner-4)"
        ],
        "success_condition": "a pot is filled with water, the pot is on top of a stove burner that is turned on."
    },
    "Task 3": {
        "name": "heat potato",
        "task_idx": 4,
        "num_samples": 1,
        "failure_injection": false,
        "folder_name": "heatPotato-1",
        "scene": "FloorPlan16",
        "gt_failure_reason": "The robot should not toggle on the microwave before trying to open it. As a result, the robot cannot open a microwave that is turned on.",
        "gt_failure_step": [["01:33", "01:36"]],
        "actions": [
            "(navigate_to_obj, Potato)",
            "(pick_up, Potato)",
            "(put_on, Potato, Plate)",
            "(pick_up, Plate)",
            "(navigate_to_obj, Microwave)",
            "(put_on, Plate, CounterTop)",
            "(toggle_on, Microwave)",
            "(open_obj, Microwave)",
            "(pick_up, Plate)",
            "(put_in, Plate, Microwave)",
            "(close_obj, Microwave)"
        ],
        "success_condition": "a potato is on a plate and inside a microwave that is turned on."
    },
    "Task 4": {
        "name": "heat potato",
        "task_idx": 4,
        "failure_injection": false,
        "num_samples": 1,
        "folder_name": "heatPotato-2",
        "chosen_failure": "occupied_put",
        "scene": "FloorPlan16",
        "gt_failure_reason": "A bowl is already inside the microwave, as a result, the plate cannot be put inside the microwave due to limited space.",
        "gt_failure_step": "01:57",
        "failure_injection_params" : {
            "src_obj_type": "Bowl",
            "target_obj_type": "Microwave"
        },
        "actions": [
            "(navigate_to_obj, Potato)",
            "(pick_up, Potato)",
            "(put_on, Potato, Plate)",
            "(pick_up, Plate)",
            "(navigate_to_obj, Microwave)",
            "(put_on, Plate, CounterTop)",
            "(open_obj, Microwave)",
            "(pick_up, Plate)",
            "(put_in, Plate, Microwave)",
            "(close_obj, Microwave)",
            "(toggle_on, Microwave)"
        ],
        "success_condition": "a potato is on a plate and inside a microwave that is turned on."
    },
    "Task 5": {
        "name": "boil water",
        "task_idx": 1,
        "num_samples": 1,
        "failure_injection": false,
        "chosen_failure": "ambiguous_plan",
        "folder_name": "boilWater-2",
        "scene": "FloorPlan1",
        "gt_failure_reason": "The robot put the pot on the fourth stove burner but toggled on the second stove burner (instead of the fourth stove burner).",
        "gt_failure_step": "00:46",
        "failure_injection_params": {
            "ambi_obj_type": "StoveBurner"
        },
        "actions": [
            "(navigate_to_obj, Pot)",
            "(pick_up, Pot)",
            "(navigate_to_obj, Sink)",
            "(put_in, Pot, Sink)",
            "(toggle_on, Faucet)",
            "(toggle_off, Faucet)",
            "(pick_up, Pot)",
            "(navigate_to_obj, StoveBurner)",
            "(put_on, Pot, StoveBurner-4)",
            "(toggle_on, StoveBurner-2)"
        ],
        "success_condition": "a pot is filled with water, the pot is on top of a stove burner that is turned on."
    },
    "Task 6": {
        "name": "make coffee",
        "task_idx": 5,
        "num_samples": 2,
        "failure_injection": true,
        "chosen_failure": "drop",
        "folder_name": "R-makeCoffee",
        "scene": "FloorPlan16",
        "preactions": [
            "(dirty_obj, Mug)"
        ],
        "actions": [
            "(navigate_to_obj, Mug)",
            "(pick_up, Mug)",
            "(navigate_to_obj, Sink)",
            "(put_on, Mug, SinkBasin)",
            "(toggle_on, Faucet)",
            "(toggle_off, Faucet)",
            "(pick_up, Mug)",
            "(pour, Mug, Sink)",
            "(navigate_to_obj, CoffeeMachine)",
            "(put_in, Mug, CoffeeMachine)",
            "(toggle_on, CoffeeMachine)",
            "(toggle_off, CoffeeMachine)",
            "(pick_up, Mug)",
            "(put_on, Mug, CounterTop)"
        ],
        "success_condition": "a clean mug is filled with coffee and on top of the countertop."
    },
    "Task 7": {
        "name": "boil water",
        "task_idx": 1,
        "num_samples": 1,
        "failure_injection": false,
        "folder_name": "boilWater-3",
        "chosen_failure": "wrong_perception",
        "scene": "FloorPlan1",
        "gt_failure_reason": "The robot mis-identified pan as pot, and picked up the pan instead of the pot.",
        "gt_failure_step": "00:24",
        "failure_injection_params" : {
            "correct_obj_type": "Pot",
            "wrong_obj_type": "Pan"
        },
        "actions": [
            "(navigate_to_obj, Pot)",
            "(pick_up, Pot)",
            "(navigate_to_obj, Sink)",
            "(put_in, Pot, Sink)",
            "(toggle_on, Faucet)",
            "(toggle_off, Faucet)",
            "(pick_up, Pot)",
            "(navigate_to_obj, StoveBurner-4)",
            "(put_on, Pot, StoveBurner-4)",
            "(toggle_on, StoveBurner-4)"
        ],
        "success_condition": "a pot is filled with water, the pot is on top of a stove burner that is turned on."
    },
    "Task 8": {
        "name": "toast bread",
        "task_idx": 2,
        "num_samples": 1,
        "failure_injection": false,
        "folder_name": "toastBread-2",
        "scene": "FloorPlan2",
        "chosen_failure": "blocking",
        "gt_failure_reason": "The robot cannot pick up knife due to the pot occluding the knife.",
        "gt_failure_step": "00:23",
        "failure_injection_params" : {
            "blocked_obj_type": "Knife",
            "src_obj_type": "Knife",
            "target_obj_type": "Pot",
            "disp_x": 0.05,
            "disp_z": -0.25,
            "disp_y": 0.05
        },
        "actions": [
            "(navigate_to_obj, Knife)",
            "(pick_up, Knife)",
            "(navigate_to_obj, Bread)",
            "(slice_obj, Bread)",
            "(put_on, Knife, CounterTop-2)",
            "(pick_up, BreadSliced)",
            "(navigate_to_obj, Toaster)",
            "(put_in, BreadSliced, Toaster)",
            "(toggle_on, Toaster)"
        ],
        "success_condition": "bread slice is inside the toaster and toaster is turned on."
    }
}


File: main/tasks_real_world.json

{
    "Task 1": {
        "task_idx": 8,
        "name": "put apple in bowl",
        "general_folder_name": "putAppleBowl1",
        "gt_failure_reason": "Apple is placed on top of the bowl instead of inside the bowl due to the bowl being upside down.",
        "gt_failure_step": ["01:00", "01:09"],
        "object_list": [
            "red apple",
            "dark blue bowl without object on top"
        ],
        "actions": [
            "Pick up apple",
            "Put apple inside bowl"
        ],
        "success_condition": "apple is inside bowl."
    },
    "Task 2": {
        "task_idx": 8,
        "name": "put apple in bowl",
        "general_folder_name": "putAppleBowl2",
        "gt_failure_reason": "Apple was dropped from robot gripper when executing the action 'put apple in bowl'.",
        "gt_failure_step": "01:03",
        "object_list": [
            "red apple",
            "dark blue bowl"
        ],
        "actions": [
            "Pick up apple",
            "Put apple inside bowl"
        ],
        "success_condition": "apple is inside bowl."
    },
    "Task 3": {
        "task_idx": 8,
        "name": "put apple in bowl",
        "general_folder_name": "putAppleBowl3",
        "gt_failure_reason": "The robot cannot put the apple inside the bowl because there's a knife on top of the bowl, blocking the space.",
        "gt_failure_step": "01:06",
        "object_list": [
            "knife with blue handle",
            "red apple",
            "dark blue bowl"
        ],
        "actions": [
            "Pick up apple",
            "Put apple inside bowl"
        ],
        "success_condition": "apple is inside bowl."
    },
    "Task 4": {
        "task_idx": 1,
        "name": "boil water in a pot",
        "general_folder_name": "boilWater1",
        "gt_failure_reason": "A bowl is put on top of the stove burner instead of a pot.",
        "gt_failure_step": ["01:58", "02:22"],
        "object_list": ["steel pot", "dark blue bowl", "faucet", "sink with a white color", "stove burner"],
        "actions": [
            "Pick up pot",
            "Put pot in sink",
            "Toggle on faucet",
            "Ignore",
            "Toggle off faucet",
            "Pick up bowl",
            "Put bowl on stove burner",
            "Toggle on stove burner",
            "Terminate"
        ],
        "success_condition": "a pot is filled with water, the pot is on top of a stove burner that is turned on."
    },
    "Task 5": {
        "task_idx": 1,
        "name": "boil water in a pot",
        "general_folder_name": "boilWater2",
        "gt_failure_reason": "The robot never toggled on and off the faucet to fill the pot with water.", 
        "gt_failure_step": ["01:00", "01:41"],
        "object_list": ["steel pot", "faucet", "sink with a white color", "stove burner"],
        "actions": [
            "Pick up pot",
            "Put pot in sink",
            "Pick up pot",
            "Put pot on stove burner",
            "Toggle on stove burner",
            "Terminate"
        ],
        "success_condition": "a pot is filled with water, the pot is on top of a stove burner that is turned on."
    },
    "Task 6": {
        "task_idx": 1,
        "name": "boil water in a pot",
        "general_folder_name": "boilWater3",
        "gt_failure_reason": "All stove burners are already occupied (by pan and red kettle), as a result, the pot cannot be put on stove burner.",
        "gt_failure_step": "03:08",
        "object_list": ["steel pot", "faucet", "sink with a white color", "stove burner with black stove knobs", "red kettle", "pan"],
        "actions": [
            "Pick up pot",
            "Put pot in sink",
            "Toggle on faucet",
            "Ignore",
            "Toggle off faucet",
            "Pick up pot",
            "Put pot on stove burner",
            "Toggle on stove burner",
            "Terminate"
        ],
        "success_condition": "a pot is filled with water, the pot is on top of a stove burner that is turned on."
    },
    "Task 7": {
        "task_idx": 1,
        "name": "boil water in a pot",
        "general_folder_name": "boilWater4",
        "gt_failure_reason": "The robot accidentally dropped the pot when moving to stove burner.",
        "gt_failure_step": "02:43",
        "object_list": ["steel pot", "faucet", "sink with a white color", "stove burner"],
        "actions": [
            "Pick up pot",
            "Put pot in sink",
            "Toggle on faucet",
            "Ignore",
            "Toggle off faucet",
            "Pick up pot",
            "Put pot on stove burner",
            "Toggle on stove burner",
            "Terminate"
        ],
        "success_condition": "a pot is filled with water, the pot is on top of a stove burner that is turned on."
    },
    "Task 8": {
        "task_idx": 9,
        "name": "cut carrot",
        "general_folder_name": "cutCarrot1",
        "gt_failure_reason": "The robot failed to slice the carrot because knife orientation is wrong.",
        "gt_failure_step": "01:13",
        "object_list": [
            "orange carrot",
            "white knife with blue handle"
        ],
        "actions": [
            "Pick up knife",
            "Slice carrot",
            "Terminate"
        ],
        "success_condition": "carrot is sliced."
    },
    "Task 9": {
        "task_idx": 9,
        "name": "cut carrot",
        "general_folder_name": "cutCarrot2",
        "gt_failure_reason": "The robot accidentally dropped one carrot slice to floor when slicing.",
        "gt_failure_step": "01:11",
        "object_list": [
            "orange carrot",
            "white knife with blue handle"
        ],
        "actions": [
            "Pick up knife",
            "Slice carrot",
            "Terminate"
        ],
        "success_condition": "carrot is sliced."
    },
    "Task 10": {
        "task_idx": 7,
        "name": "put pear in drawer",
        "general_folder_name": "putPearDrawer1",
        "gt_failure_reason": "The robot never opened the drawer before trying to put pear in drawer.",
        "gt_failure_step": "01:33",
        "object_list": [
            "yellow drawer",
            "green pear"
        ],
        "actions": [
            "Pick up pear",
            "Put pear in drawer"
        ],
        "success_condition": "pear is inside drawer and drawer is closed."
    },
    "Task 11": {
        "task_idx": 7,
        "name": "put pear in drawer",
        "general_folder_name": "putPearDrawer2",
        "gt_failure_reason": "The robot failed to close the drawer because the drawer is too full.",
        "gt_failure_step": "02:31",
        "object_list": [
            "yellow storage box",
            "green pear",
            "carrots",
            "purple beets",
            "jell-o"
        ],
        "actions": [
            "Open drawer",
            "Pick up pear",
            "Put pear in drawer",
            "Close drawer"
        ],
        "success_condition": "pear is inside drawer and drawer is closed."
    },
    "Task 12": {
        "task_idx": 7,
        "name": "put pear in drawer",
        "general_folder_name": "putPearDrawer3",
        "gt_failure_reason": "The robot gripper is occupied by the pear when it attempted to open drawer, as a result, the drawer cannot be opened. The robot should open drawer first before picking up the pear.",
        "gt_failure_step": ["00:44", "01:10"],
        "object_list": [
            "yellow storage box",
            "green pear"
        ],
        "actions": [
            "Pick up pear",
            "Open drawer",
            "Put pear in drawer",
            "Close drawer"
        ],
        "success_condition": "pear is inside drawer and drawer is closed."
    },
    "Task 13": {
        "task_idx": 5,
        "name": "cook carrot",
        "general_folder_name": "sauteeCarrot1",
        "gt_failure_reason": "The robot has mistakenly picked up a spoon instead of knife to slice carrot.",
        "gt_failure_step": "01:02",
        "object_list": ["carrot", "knife with blue handle", "spoon", "table", "steel saucepan", "stove burner"],
        "actions": [
            "Pick up knife",
            "Slice carrot",
            "Put knife on table",
            "Pick up carrot slice",
            "Put carrot slice in saucepan",
            "Toggle on stove burner",
            "Terminate"
        ],
        "success_condition": "a sliced carrot is inside a pan, the pan is on top of a stove burner that is turned on."
    },
    "Task 14": {
        "task_idx": 5,
        "name": "cook carrot",
        "general_folder_name": "sauteeCarrot2",
        "gt_failure_reason": "The carrot is put on top of the stove burner instead of on the saucepan.",
        "gt_failure_step": "04:22",
        "object_list": ["carrot", "knife with blue handle", "table", "steel saucepan", "stove burner"],
        "actions": [
            "Pick up knife",
            "Skip",
            "Skip",
            "Slice carrot",
            "Put knife on table",
            "Pick up carrot slice",
            "Put carrot slice in saucepan",
            "Toggle on stove burner",
            "Terminate"
        ],
        "success_condition": "a sliced carrot is inside a pan, the pan is on top of a stove burner that is turned on."
    },
    "Task 15": {
        "task_idx": 5,
        "name": "cook carrot",
        "general_folder_name": "sauteeCarrot3",
        "gt_failure_reason": "The robot accidentally dropped the knife when trying to slice the carrot.",
        "gt_failure_step": "00:55",
        "object_list": ["carrot", "knife with blue handle", "spoon", "table on the left of sink", "steel saucepan", "stove burner"],
        "actions": [
            "Pick up knife",
            "Slice carrot",
            "Put knife on table",
            "Pick up carrot slice",
            "Put carrot slice in saucepan",
            "Toggle on stove burner",
            "Terminate"
        ],
        "success_condition": "a sliced carrot is inside a pan, the pan is on top of a stove burner that is turned on."
    },
    "Task 16": {
        "task_idx": 5,
        "name": "cook carrot",
        "general_folder_name": "sauteeCarrot4",
        "gt_failure_reason": "The robot never toggled on the stove burner, as a result, the carrot cannot be cooked.",
        "gt_failure_step": "03:40",
        "object_list": ["carrot", "knife with blue handle", "table", "steel saucepan", "stove burner"],
        "actions": [
            "Pick up knife",
            "Skip",
            "Skip",
            "Slice carrot",
            "Put knife on table",
            "Pick up carrot slice",
            "Put carrot slice in saucepan",
            "Terminate"
        ],
        "success_condition": "a sliced carrot is inside a pan, the pan is on top of a stove burner that is turned on."
    },
    "Task 17": {
        "task_idx": 6,
        "name": "secure objects",
        "general_folder_name": "secureObjects1",
        "gt_failure_reason": "The knife should be placed inside the drawer and pear in fridge. The current placements are flipped.",
        "gt_failure_step": ["02:15", "05:00"],
        "object_list": [
            "white knife with green handle",
            "green pear",
            "black fridge with gray door and white interior",
            "yellow storage box",
            "faucet behind the sink"
        ],
        "distractor_list": [
            "faucet behind the sink"
        ],
        "actions": [
            "Pick up knife",
            "Put knife in fridge",
            "Close fridge",
            "Pick up pear",
            "Put pear near drawer",
            "Open drawer",
            "Pick up pear",
            "Put pear in drawer",
            "Close drawer"
        ],
        "success_condition": "pear and knife are in their most suitable container respectively."
    },
    "Task 18": {
        "task_idx": 10,
        "name": "put all visible fruits in bowl",
        "general_folder_name": "putFruitsBowl1",
        "gt_failure_reason": "The robot should not put carrot in the bowl since it is not a fruit.",
        "gt_failure_step": ["02:56", "03:17"],
        "object_list": [
            "carrot",
            "strawberry with red body and green top",
            "pear",
            "pear next to strawberry",
            "white bowl"
        ],
        "actions": [
            "Pick up strawberry",
            "Put strawberry in bowl",
            "Pick up pear",
            "Put pear in bowl",
            "Pick up carrot",
            "Put carrot in bowl"
        ],
        "success_condition": "all fruits are in the bowl."
    },
    "Task 19": {
        "task_idx": 10,
        "name": "put all fruits in bowl",
        "general_folder_name": "putFruitsBowl2",
        "gt_failure_reason": "The robot did not put strawberry in the bowl since it is a fruit.",
        "gt_failure_step": "02:01",
        "object_list": [
            "carrot",
            "strawberry",
            "green pear",
            "red apple",
            "bowl"
        ],
        "actions": [
            "Pick up apple",
            "Put apple in bowl",
            "Pick up pear",
            "Put pear in bowl"
        ],
        "success_condition": "all fruits are in the bowl."
    },
    "Task 20": {
        "task_idx": 3,
        "name": "make coffee",
        "general_folder_name": "makeCoffee1",
        "gt_failure_reason": "The robot failed to toggle on coffee machine.",
        "gt_failure_step": "01:57",
        "object_list": [
            "coffee machine",
            "purple cup",
            "table on the left of sink"
        ],
        "actions": [
            "Pick up cup",
            "Put cup in coffee machine",
            "Toggle on coffee machine",
            "Pick up cup",
            "Put cup on table"
        ],
        "success_condition": "a cup filled with coffee is on table."
    },
    "Task 21": {
        "task_idx": 3,
        "name": "make coffee",
        "general_folder_name": "makeCoffee2",
        "gt_failure_reason": "A mug is already inside the coffee machine, as a result, the cup cannot be put inside.",
        "gt_failure_step": "01:11",
        "object_list": [
            "coffee machine",
            "purple cup",
            "blue cup with handle",
            "table on the left of sink"
        ],
        "actions": [
            "Pick up cup",
            "Put cup in coffee machine",
            "Toggle on coffee machine",
            "Pick up cup",
            "Put cup on table"
        ],
        "success_condition": "a cup filled with coffee is on table."
    },
    "Task 22": {
        "task_idx": 12,
        "name": "make coffee",
        "general_folder_name": "makeCoffee3",
        "gt_failure_reason": "The robot mistakenly picked up the pink cup instead of the blue cup after filling blue cup with coffee.",
        "gt_failure_step": "02:04",
        "object_list": [
            "white coffee machine",
            "pink cup",
            "table on the left of sink",
            "blue cup"
        ],
        "actions": [
            "Toggle on coffee machine",
            "Pick up cup",
            "Put cup on table"
        ],
        "success_condition": "a blue cup filled with coffee is on table."
    },
    "Task 23": {
        "task_idx": 11,
        "name": "pre-heat pot",
        "general_folder_name": "heatPot1",
        "gt_failure_reason": "The robot cannot pick up the pot due to a cutting board blocking the pot.",
        "gt_failure_step": "00:23",
        "object_list": [
            "stove burner",
            "gray pot",
            "cutting board"
        ],
        "actions": [
            "Pick up pot",
            "Put pot on stove burner",
            "Toggle on stove burner"
        ],
        "success_condition": "a pot is on top of a stove burner that is toggled on."
    },
    "Task 24": {
        "task_idx": 11,
        "name": "pre-heat pot",
        "general_folder_name": "heatPot2",
        "gt_failure_reason": "The robot failed to toggle on stove burner.",
        "gt_failure_step": "01:31",
        "object_list": [
            "stove burner",
            "gray pot",
            "sink"
        ],
        "actions": [
            "Pick up pot",
            "Put pot on stove burner",
            "Toggle on stove burner"
        ],
        "success_condition": "a pot is on top of a stove burner that is toggled on."
    },
    "Task 25": {
        "task_idx": 2,
        "name": "store apple in a bowl and put in a fridge",
        "general_folder_name": "appleInFridge1",
        "gt_failure_reason": "The robot put the dark blue bowl (empty) instead of the white bowl with apple inside the fridge.",
        "gt_failure_step": "02:57",
        "object_list": ["white bowl", "dark blue bowl", "table", "red apple", "fridge"],
        "actions": [
            "Pick up apple",
            "Put apple in bowl",
            "Open fridge",
            "Pick up bowl",
            "Put bowl in fridge",
            "Close fridge",
            "Terminate"
        ],
        "success_condition": "a bowl with an apple inside is put in fridge."
    },
    "Task 26": {
        "task_idx": 2,
        "name": "store apple in a bowl and put in a fridge",
        "general_folder_name": "appleInFridge2",
        "gt_failure_reason": "The robot put the bowl with apple on top of the fridge instead of inside it.",
        "gt_failure_step": "04:13",
        "object_list": ["white bowl", "table", "red apple", "fridge"],
        "actions": [
            "Pick up apple",
            "Put apple in bowl",
            "Open fridge",
            "Pick up bowl",
            "Put bowl in fridge",
            "Close fridge",
            "Terminate"
        ],
        "success_condition": "a bowl with an apple inside is put in fridge."
    },
    "Task 27": {
        "task_idx": 2,
        "name": "store apple in a bowl and put in a fridge",
        "general_folder_name": "appleInFridge3",
        "gt_failure_reason": "The robot did not open the firdge before attempting to put the bowl inside.",
        "gt_failure_step": "02:41",
        "object_list": ["dark blue bowl", "table", "red apple", "black fridge with grey door"],
        "actions": [
            "Pick up apple",
            "Put apple in bowl",
            "Pick up bowl",
            "Put bowl in fridge",
            "Open fridge",
            "Close fridge",
            "Terminate"
        ],
        "success_condition": "a bowl with an apple inside is put in fridge."
    },
    "Task 28": {
        "task_idx": 2,
        "name": "store apple in a bowl and put in a fridge",
        "general_folder_name": "appleInFridge4",
        "gt_failure_reason": "The robot never closed the fridge door.",
        "gt_failure_step": "03:47",
        "object_list": ["dark blue bowl", "table", "red apple", "black fridge with grey door"],
        "actions": [
            "Pick up apple",
            "Put apple in bowl",
            "Open fridge",
            "Pick up bowl",
            "Put bowl in fridge",
            "Terminate"
        ],
        "success_condition": "a bowl with an apple inside is put in a closed fridge."
    },
    "Task 29": {
        "task_idx": 4,
        "name": "serve a bowl of potato on table that was heated using a microwave",
        "general_folder_name": "heatPotato1",
        "gt_failure_reason": "The robot failed because it attempted to toggle on the microwave while the microwave door was still open.",
        "gt_failure_step": ["04:24", "05:12"],
        "object_list": ["microwave with a white casing", "dark blue bowl", "orange table", "potato"],
        "actions": [
            "Pick up potato",
            "Put potato in bowl",
            "Skip",
            "Ignore",
            "Open microwave",
            "Pick up bowl",
            "Put bowl in microwave",
            "Toggle on microwave",
            "Close microwave",
            "Toggle off microwave",
            "Skip",
            "Ignore",
            "Open microwave",
            "Pick up bowl",
            "Put bowl on table",
            "Close microwave",
            "Terminate"
        ],
        "success_condition": "a bowl of a heated potato is on the table."
    },
    "Task 30": {
        "task_idx": 4,
        "name": "serve a bowl of potato on table that was heated using a microwave",
        "general_folder_name": "heatPotato2",
        "gt_failure_reason": "The robot failed because it attempted to open the microwave door while it was still toggled on.",
        "gt_failure_step": "01:48",
        "object_list": ["microwave with a white casing", "dark blue bowl", "orange table", "potato"],
        "actions": [
            "Pick up potato",
            "Put potato in bowl",
            "Skip",
            "Ignore",
            "Toggle on microwave",
            "Skip",
            "Ignore",
            "Open microwave",
            "Pick up bowl",
            "Put bowl in microwave",
            "Close microwave",
            "Skip",
            "Ignore",
            "Open microwave",
            "Pick up bowl",
            "Put bowl on table",
            "Terminate",
            "Close microwave",
            "Terminate"
        ],
        "success_condition": "a bowl of a heated potato is on the table."
    }
}


File: main/utils.py

import numpy as np
import open3d as o3d
import torch
from sentence_transformers import SentenceTransformer
from sentence_transformers import util as st_utils
import constants as cons
import scipy.spatial

translation_lm_id =  'stsb-roberta-large' # 'all-distilroberta-v1'
# device = torch.device(f"cuda:0" if torch.cuda.is_available() else "cpu")
device = torch.device("cpu")
translation_lm = SentenceTransformer(translation_lm_id).to(device)


def get_pcd_dist(pts_A, pts_B):
    pcd_A = o3d.geometry.PointCloud()
    pcd_A.points = o3d.utility.Vector3dVector(pts_A)
    pcd_B = o3d.geometry.PointCloud()
    pcd_B.points = o3d.utility.Vector3dVector(pts_B)

    dists = pcd_A.compute_point_cloud_distance(pcd_B)
    dist = np.min(np.array(dists))
    return dist

def is_merge(pts_A, pts_B):
    if len(np.array(pts_A)) == 0 or len(np.array(pts_B)) == 0:
        return True
    else:
        dist = get_pcd_dist(pts_A, pts_B)
        if dist < 0.01:
            return True, dist
        else:
            return False, dist

def get_label_from_object_id(object_id, events, task):
    try:
        if len(events) == 1:
            src_obj = next(obj for obj in events[0].metadata["objects"] if obj["objectId"] == object_id)
        else:
            found = False
            for event in events:
                for obj in event.metadata["objects"]:
                    if obj["objectId"] == object_id:
                        src_obj = obj
                        found = True
                        break
                if found:
                    break

        if src_obj['name'] in task['unity_name_map']:
            label = cons.NAME_MAP[task['unity_name_map'][src_obj['name']]]
        elif src_obj['name'] == "Bread_2_Slice_1":
            label = cons.NAME_MAP['BreadSliced']
        elif src_obj['objectType'] in cons.NAME_MAP:
            label = cons.NAME_MAP[src_obj['objectType']]
        else:
            label = src_obj['objectType'].lower()
        return label
    except:
        return None

def is_moving(object_id, event):
    for obj in event.metadata["objects"]:
        if object_id == obj["objectId"]:
            return (obj["moveable"] and obj["isMoving"])
    return False

def is_picked_up(object_id, event):
    for obj in event.metadata["objects"]:
        if object_id == obj["objectId"]:
            return (obj["pickupable"] and obj["isPickedUp"])
    return False

def is_receptacle(object_id, event):
    for obj in event.metadata["objects"]:
        if object_id == obj["objectId"]:
            return obj["receptacle"]
    return False

def transform_point3s(t, ps):
    """Transfrom 3D points from one space to another.
    Args:
        t (numpy.array [4, 4]): SE3 transform.
        ps (numpy.array [n, 3]): Array of n 3D points (x, y, z).
    Returns:
        numpy.array [n, 3]: Transformed 3D points.
    """
    # convert to homogeneous
    ps_homogeneous = np.hstack([ps, np.ones((len(ps), 1), dtype=np.float32)])
    ps_transformed = np.dot(t, ps_homogeneous.T).T

    return ps_transformed[:, :3]

def get_admissible_actions(object_list, last_event):
    for obj_name in cons.NAME_MAP:
        if obj_name.split("-")[0] in object_list and obj_name not in object_list:
            object_list.append(obj_name)
    available_actions = []
    # print("available objects:", object_list)
   
    pickable_classes = []
    for obj in last_event.metadata["objects"]:
        if obj['pickupable']:
            pickable_classes.append(obj['objectType'])

    recep_ids = cons.get_receptacle_ids()
    recep_classes = [cons.object_intid_to_string(recep_id) for recep_id in recep_ids]
    toggle_ids = cons.get_togglable_ids()
    toggle_classes = [cons.object_intid_to_string(toggle_id) for toggle_id in toggle_ids]
    openable_ids = cons.get_openable_ids()
    openable_classes = [cons.object_intid_to_string(openable_id) for openable_id in openable_ids]

    sliceable_classes = cons._SLICEABLES
    crackable_classes = cons._CRACKABLE
    flat_recep_classes = cons._FLAT_RECEPT
    fillable_classes = cons._FILLABLE
    
    for object_name in object_list:
        object_class = object_name.split("-")[0]
        if object_class in pickable_classes:
            available_actions.append(f"(pick_up, {object_name})")
        if object_class in recep_classes:
            for tmp in object_list:
                if tmp.split("-")[0] in pickable_classes and tmp != object_name:
                    if object_class in flat_recep_classes:
                        available_actions.append(f"(put_on, {tmp}, {object_name})")
                    else:
                        available_actions.append(f"(put_in, {tmp}, {object_name})")
        if object_class in toggle_classes:
            available_actions.append(f"(toggle_on, {object_name})")
            available_actions.append(f"(toggle_off, {object_name})")
        if object_class in openable_classes:
            available_actions.append(f"(open_obj, {object_name})")
            available_actions.append(f"(close_obj, {object_name})")
        if object_class in sliceable_classes:
            available_actions.append(f"(slice_obj, {object_name})")
        if object_class in crackable_classes:
            available_actions.append(f"(crack_obj, {object_name})")
        if object_class in fillable_classes:
            for tmp in object_list:
                if (object_class in fillable_classes) and (tmp.split("-")[0] in fillable_classes):
                    if object_class != tmp and "Sink" not in tmp:
                        available_actions.append(f"(pour, {tmp}, {object_class})")
    return available_actions

def get_initial_plan(actions):
    plan = ""
    idx = 0
    for action in actions:
        params = action[1:-1].split(", ")
        if params[0] != "navigate_to_obj":
        # if True:
            for i, obj_name in enumerate(params[1:]):
                if obj_name in cons.NAME_MAP.keys():
                    params[i+1] = cons.NAME_MAP[obj_name]
                else:
                    params[i+1] = obj_name.lower()
            plan += f"{idx+1}. ({', '.join(params)})\n"
            idx += 1
    return plan[:-1]

def get_robot_plan(folder_name, step=None, with_obs=False):
    with open('state_summary/{}/state_summary_L2.txt'.format(folder_name), 'r') as f:
        L2_captions = f.readlines()
    
    with open('state_summary/{}/state_summary_L1.txt'.format(folder_name), 'r') as f:
        L1_captions = f.readlines()

    if with_obs is False:
        captions = L2_captions
    else:
        captions = L1_captions

    robot_plan = ""
    for caption in captions:
        if step is not None and step in caption:
            break
        if with_obs:
            robot_plan += caption
        else:
            robot_plan += caption[:caption.find("Visual observation")-1] + "\n"
    return robot_plan

def get_replan_prefix():
    return f"""Provide a plan with the available actions for the robot to recover from the failure and finish the task.
Available actions: pick up, put in some container, put on some receptacle, open (e.g. fridge), close, toggle on (e.g. faucet), toggle off, slice object, crack object (e.g. egg), pour (liquid) from A to B.
The robot can only hold one object in its gripper, in other words, if there's object in the robot gripper, it can no longer pick up another object.
###
To clean a dirty object A, the plan is
1. (pick_up, A)
2. (put_in, A, sink)
3. (toggle_on, faucet)
4. (toggle_off, faucet)
5. (pour, A, sink)
6. (pick_up, A)
Object A should be clean after executing these actions.
###
The plan should 1) not contain any if statements 2) contain only the available actions 3) resemble the format of the initial plan.
"""

# helper function for finding similar sentence in a corpus given a query
def find_most_similar(query_str, corpus_embedding):
    query_embedding = translation_lm.encode(query_str, convert_to_tensor=True, device=device)
    # calculate cosine similarity against each candidate sentence in the corpus
    cos_scores = st_utils.pytorch_cos_sim(query_embedding, corpus_embedding)[0].detach().cpu().numpy()
    # retrieve high-ranked index and similarity score
    most_similar_idx, matching_score = np.argmax(cos_scores), np.max(cos_scores)
    return most_similar_idx, matching_score

def get_cos_sim(str1, str2):
    embedding_1 = translation_lm.encode(str1, convert_to_tensor=True, device=device)
    embedding_2 = translation_lm.encode(str2, convert_to_tensor=True, device=device)
    cos_scores = st_utils.pytorch_cos_sim(embedding_1, embedding_2)[0].detach().cpu().numpy()
    return cos_scores[0]

def translate_plan(plan, object_list, last_event):
    translated_plan = ""
    available_actions = get_admissible_actions(object_list, last_event)
    action_list_embedding = translation_lm.encode(available_actions, batch_size=32, convert_to_tensor=True, device=device)
    idx = 0
    for step_instruct in plan.split("\n"):
        parsed_instruct = "".join(step_instruct.split(".")[1:]).strip()
        if ")" in parsed_instruct:
            parsed_instruct = parsed_instruct[:parsed_instruct.find(")")+1]
        most_similar_idx, matching_score = find_most_similar(parsed_instruct, action_list_embedding)
        translated_action = available_actions[most_similar_idx]
        print(step_instruct, "=>", translated_action, "score:", matching_score)
        if matching_score < 0.6:
            continue
        translated_plan += f"{translated_action}\n"
        idx += 1
    return translated_plan

def in_hull(p, hull):
    """
    Test if points in `p` are in `hull`

    `p` should be a `NxK` coordinates of `N` points in `K` dimensions
    `hull` is either a scipy.spatial.Delaunay object or the `MxK` array of the 
    coordinates of `M` points in `K`dimensions for which Delaunay triangulation
    will be computed
    """
    if not isinstance(hull, scipy.spatial.Delaunay):
        hull = scipy.spatial.Delaunay(hull)

    return hull.find_simplex(p)>=0

def is_inside(src_pts, target_pts, thresh=0.5):
    try:
        hull = scipy.spatial.ConvexHull(target_pts)
    except:
        return False
    # print("vertices of hull: ", np.array(hull.vertices).shape)
    hull_vertices = np.array([[0,0,0]])
    for v in hull.vertices:
        hull_vertices = np.vstack((hull_vertices, np.array([target_pts[v,0], target_pts[v,1], target_pts[v,2]])))
    hull_vertices = hull_vertices[1:]

    num_src_pts = len(src_pts)
    # Don't want threshold to be too large (specially with more objects, like 4, 0.9*thresh becomes too large)
    thresh_obj_particles = thresh * num_src_pts
    src_points_in_hull = in_hull(src_pts, hull_vertices)
    # print("src pts in target, thresh: ", src_points_in_hull.sum(), thresh_obj_particles)
    if src_points_in_hull.sum() > thresh_obj_particles:
        return True
    else:
        return False
    
def convert_step_to_timestep(step, video_fps):
    seconds = step // video_fps
    formatted_time = '{:02d}:{:02d}'.format(int(seconds / 60), seconds % 60)
    return formatted_time

def convert_timestep_to_step(timestep, video_fps):
    minutes, seconds = timestep.split(":")
    step = int(minutes) * 60 * video_fps + int(seconds) * video_fps
    return step

def check_task_success(task_idx, last_event):
    if task_idx == 1: # boil water
        for obj in last_event.metadata["objects"]:
            if "Pot" == obj["objectType"]:
                return obj["isFilledWithLiquid"] and 'water' == obj["fillLiquid"] and obj['temperature'] == 'Hot'
    if task_idx == 2: # toast bread
        for obj in last_event.metadata["objects"]:
            if "BreadSliced" == obj["objectType"]:
                return obj["isCooked"]
    if task_idx == 3: # cook egg
        egg_cracked_is_cooked = False
        for obj in last_event.metadata["objects"]:
            if "EggCracked" == obj["objectType"]:
                egg_cracked_is_cooked = obj["isCooked"]
            if "Pan" == obj["objectType"]:
                pan_is_clean = not obj["isDirty"]
        return egg_cracked_is_cooked and pan_is_clean
    if task_idx == 4: # heat potato
        for obj in last_event.metadata["objects"]:
            if "Potato" == obj["objectType"]:
                potato_is_cooked = obj["isCooked"]
            if "Plate" == obj["objectType"]:
                plate_is_clean = not obj["isDirty"]
        return potato_is_cooked and plate_is_clean
    if task_idx == 5: # make coffee
        for obj in last_event.metadata["objects"]:
            if "Mug" == obj["objectType"]:
                return obj["isFilledWithLiquid"] and 'coffee' == obj["fillLiquid"] and not obj["isDirty"]
    if task_idx == 6: # water plant
        for obj in last_event.metadata["objects"]:
            if "HousePlant" == obj["objectType"]:
                return obj["isFilledWithLiquid"] and 'water' == obj["fillLiquid"]
    if task_idx == 7: # store egg
        for obj in last_event.metadata["objects"]:
            if "Egg" == obj["objectType"]:
                succ_1 = False
                for p in obj['parentReceptacles']:
                    if "Bowl" in p:
                        succ_1 = True
            if "Bowl" == obj["objectType"]:
                succ_2 = False
                for p in obj['parentReceptacles']:
                    if "Fridge" in p:
                        succ_2 = True
        return succ_1 and succ_2
    if task_idx == 8: # make salad
        succ_1 = False
        lettuceSliced, potatoSliced, tomatoSliced = False, False, False
        for obj in last_event.metadata["objects"]:
            if "Bowl" == obj["objectType"]:
                if obj['parentReceptacles'] is not None and "Fridge" in obj['parentReceptacles'][0].split('|'):
                    succ_1 = True
                for fruit_obj_id in obj['receptacleObjectIds']:
                    # Need the second condition cause sometimes the object id in thor looks like - 
                    # 'Tomato|+01.30|+00.96|-01.08|TomatoSliced_4' instead of 'TomatoSliced|+01.30|+00.96|-01.08|'
                    if ('LettuceSliced' in fruit_obj_id.split('|')) or ('LettuceSliced' in fruit_obj_id.split('|')[-1].split('_')):
                        lettuceSliced = True
                    elif ('TomatoSliced' in fruit_obj_id.split('|')) or ('TomatoSliced' in fruit_obj_id.split('|')[-1].split('_')):
                        tomatoSliced = True
                    elif ('PotatoSliced' in fruit_obj_id.split('|')) or ('PotatoSliced' in fruit_obj_id.split('|')[-1].split('_')):
                        potatoSliced = True
                return succ_1 and lettuceSliced and potatoSliced and tomatoSliced
    if task_idx == 9: # switch devices
        for obj in last_event.metadata["objects"]:
            if "Laptop" == obj["objectType"]:
                succ_1 = "TVStand" in obj['parentReceptacles'][0] and not obj['isOpen']
            if "Television" == obj["objectType"]:
                succ_2 = obj['isToggled']
        return succ_1 and succ_2
    if task_idx == 10: # warm water
        for obj in last_event.metadata["objects"]:
            if "Mug" == obj["objectType"]:
                succ_1 = obj["isFilledWithLiquid"] and 'water' == obj["fillLiquid"] and obj['temperature'] == 'Hot'
            if "Cup" == obj["objectType"]:
                succ_2 = obj["isFilledWithLiquid"] and 'water' == obj["fillLiquid"] and obj['temperature'] == 'Hot'
        return succ_1 or succ_2
    return False


